\documentclass[addpoints]{exam}
\usepackage[utf8]{inputenc}
\input{macros.tex}

% \printanswers

% Uses boxes instead of circles for multiple choice
\checkboxchar{$\Box$}
\checkedchar{$\blacksquare$}

% True/False
\newcommand{\tf}[1][{}]{%
  \fillin[#1][0.25in]%
}

\begin{document}
\pagestyle{foot}
\footer{}{Page \thepage\ of \numpages}{}

\begin{center}
  {\LARGE Foundations of Machine Learning: Mathematics Assessment}

\end{center}

\begin{questions}
  \question When you hear or see the following, what do you think?  (Not whether
  you already know what's written, but whether you're comfortable with the
  notation and/or language.)
  \begin {quote}Let $S$ be the subspace
    spanned by the orthonormal vectors $a$ and $b$. Let $p$ be the
    projection of the vector $v$ into $S$. Let $r=v-p$ be the residual
    vector. Then $r\perp S$ and $\left\{ r,a,b\right\} $ form an orthonormal
    set.
  \end{quote}
  \begin{checkboxes}
    \choice You're speaking my language - totally comfortable.
    \choice Familiar, but rusty.  I'll be ready to go by the start of class.
    \choice Never properly learned this.  I need to get up to speed.
    \choice Wait, this is what I'm signing up for? 
  \end{checkboxes}

  \question When you hear or see the following, what do you think?  (Not whether
  you already know what's written, but whether you're comfortable with the
  notation and/or language.)
  \begin {quote}Given some data $\left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\in\reals^{d}\times\reals$,
    the ridge regression solution for regularization parameter $\lambda>0$
    is given by
    \[
      \hat{w}=\argmin_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{2}^{2},
    \]
    where $\|w\|_{2}^{2}=w_{1}^{2}+\cdots+w_{d}^{2}$ is the square of
    the $\ell_{2}$-norm of $w$.
  \end{quote}
  \begin{checkboxes}
    \choice You're speaking my language - totally comfortable.
    \choice Familiar, but rusty.  I'll be ready to go by the start of class.
    \choice Never properly learned this.  I need to get up to speed.
    \choice Wait, this is what I'm signing up for? 
  \end{checkboxes}

  \question When you hear or see the following, what do you think?  (Not whether
  you already know what's written, but whether you're comfortable with the
  notation and/or language.):
  \begin {quote}For ``loss'' function $\ell:\cy\times\cy\to\reals$,
    define the ``risk'' of a function $f:\cx\to\cy$ by 
    \[
      R(f)=\ex\ell\left(f(x),y\right),
    \]
    where the expectation is over $(x,y)\sim P_{\cx\times\cy}$, a distribution
    over $\cx\times\cy$.
  \end{quote}
  \begin{checkboxes}
    \choice You're speaking my language - totally comfortable.
    \choice Familiar, but rusty.  I'll be ready to go by the start of class.
    \choice Never properly learned this.  I need to get up to speed.
    \choice Wait, this is what I'm signing up for? 
  \end{checkboxes}

  \newpage
  \question When you hear or see the following, what do you think?  (Not whether
  you already know what's written, but whether you're comfortable with the
  notation and/or language.):
  \begin {quote}If we fix a direction $u\in\reals^{d}$,
    we can compute the directional derivative $f'(x;u)$ as
    \[
      f'(x;u)=\lim_{h\to0}\frac{f(x+hu)-f(x)}{h}.
    \]\end{quote}
  \begin{checkboxes}
    \choice You're speaking my language - totally comfortable.
    \choice Familiar, but rusty.  I'll be ready to go by the start of class.
    \choice Never properly learned this.  I need to get up to speed.
    \choice Wait, this is what I'm signing up for? 
  \end{checkboxes}

  \question How comfortable are you answering the following question:
  \begin {quote}Verify, just by multiplying out the expressions on the RHS, that
    the following ``completing the square'' identity is true: For any vectors
    $x,b\in\reals^{d}$ and symmetric invertible matrix $M\in\reals^{d\times d}$,
    we have
    \begin{eqnarray}
      x^{T}Mx-2b^{T}x & = & \left(x-M^{-1}b\right)^{T}M(x-M^{-1}b)-b^{T}M^{-1}b\label{eq:second-quad-form}
    \end{eqnarray}
  \end{quote}
  \begin{checkboxes}
    \choice So easy. If I had a whiteboard here, I'd do it for you right now.
    \choice Yeah - easy.  I'll have the answer to you in 5 minutes -- I just have to check something on Google first.  
    \choice Hmmmm. This will be easy by the first day of class.
    \choice :( 
  \end{checkboxes}

  \question How comfortable are you answering the following question: 
  \begin {quote}Take the gradient of the following w.r.t. $w$:
    \[
      L(w,b,\xi,\alpha,\lambda)=\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}+\sum_{i=1}^{n}\alpha_{i}\left(1-y_{i}\left[w^{T}x_{i}+b\right]-\xi_{i}\right)-\sum_{i=1}^{n}\lambda_{i}\xi_{i}
    \]\end{quote}
  \begin{checkboxes}
    \choice So easy. If I had a whiteboard here, I'd do it for you right now.
    \choice Yeah - easy.  I'll have the answer to you in 5 minutes -- I just have to check something on Google first.  
    \choice Hmmmm. This will be easy by the first day of class.
    \choice :( 
  \end{checkboxes}




  \question How comfortable are you answering the following question: 
  \begin{quote}Consider $x_{1},\ldots,x_{n}$ sampled i.i.d.
    from a distribution $P$ on $\reals$. Write $\mu=\ex x$, for $x\sim P$.
    Show that the mean $\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}$ is an unbiased
    estimate of $\mu$ (i.e. show that $\ex\bar{x}=\mu$). Similarly, show that the sample variance
    $\frac{1}{n-1}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}$ is an
    unbiased estimate for $\var\left(x\right)$. \end{quote}
  \begin{checkboxes}
    \choice So easy. If I had a whiteboard here, I'd do it for you right now.
    \choice Yeah - easy.  I'll have the answer to you in 5 minutes -- I just have to check something on Google first.  
    \choice Hmmmm. This will be easy by the first day of class.
    \choice :( 
  \end{checkboxes}
  
\end{questions}
\end{document}
