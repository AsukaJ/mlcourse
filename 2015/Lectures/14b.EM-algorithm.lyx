#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_math eulervm
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Expectation Maximization Algorithm
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
David MacKay's book Information Theory, Inference, and Learning Algorithms
\end_layout

\end_inset

 
\end_layout

\begin_layout Section
Kullback-Leibler (KL) Divergence
\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Statistical Divergence Measures
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Given two probability distributions on 
\begin_inset Formula $\cx$
\end_inset

, 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

.
\end_layout

\begin_layout Itemize
How to measure how different they are?
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $S$
\end_inset

 be the space of all probability distributions on 
\begin_inset Formula $\cx$
\end_inset

.
\end_layout

\begin_layout Itemize
Divergence is typically written 
\begin_inset Formula $D(p\|q)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
For a divergence measure, we want
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $D(p\|q)\ge0$
\end_inset

 for all 
\begin_inset Formula $p,q\in S$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $D(p\|q)=0$
\end_inset

 if and only if 
\begin_inset Formula $p=q$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kullback-Leibler Divergence
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $p(x)$
\end_inset

 and 
\begin_inset Formula $q(x)$
\end_inset

 be PMFs on 
\begin_inset Formula $\cx$
\end_inset

.
 
\end_layout

\begin_layout Itemize
How can we measure how 
\begin_inset Quotes eld
\end_inset

different
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 are?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
Kullback-Leibler
\series default
 or 
\series bold

\begin_inset Quotes eld
\end_inset

KL
\begin_inset Quotes erd
\end_inset

 Diverence
\series default
 is defined by
\begin_inset Formula 
\begin{eqnarray*}
\kl(p\|q) & = & \sum_{x}p(x)\log\frac{p(x)}{q(x)}.
\end{eqnarray*}

\end_inset

(Assumes 
\begin_inset Formula $q(x)=0$
\end_inset

 implies 
\begin_inset Formula $p(x)=0$
\end_inset

.)
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can also write this as
\begin_inset Formula 
\begin{eqnarray*}
\kl(p\|q) & = & \ex_{p}\log\frac{p(X)}{q(X)},
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $X\sim p(x)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator
\begin_inset Note Note
status collapsed

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Statistical Divergence and Metrics
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Some divergences are proper distance metrics:
\end_layout

\begin_deeper
\begin_layout Itemize
e.g.
 Hellinger distance: 
\begin_inset Formula $h^{2}(p,q)=\frac{1}{2}\int\left(\sqrt{p(x)}-\sqrt{q(x)}\right)^{2}dx$
\end_inset


\end_layout

\begin_layout Itemize
where 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 are PMFs.
\end_layout

\end_deeper
\begin_layout Itemize
Kullback-Leibler divergence is an important divergence measure
\end_layout

\begin_deeper
\begin_layout Itemize
Not a proper distance metric.
\end_layout

\begin_layout Itemize
Not even symmetric.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gibbs Inequality (
\begin_inset Formula $\kl(p\|q)\ge0$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Gibbs Inequality]
\end_layout

\end_inset

Let 
\begin_inset Formula $p(x)$
\end_inset

 and 
\begin_inset Formula $q(x)$
\end_inset

 be PMFs on 
\begin_inset Formula $\cx$
\end_inset

.
 Then
\begin_inset Formula 
\[
\kl(p\|q)\ge0,
\]

\end_inset

with equality iff 
\begin_inset Formula $p(x)=q(x)$
\end_inset

 for all 
\begin_inset Formula $x\in\cx$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
KL divergence measures the 
\begin_inset Quotes eld
\end_inset

distance
\begin_inset Quotes erd
\end_inset

 between distributions.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Note:
\end_layout

\begin_deeper
\begin_layout Itemize
KL divergence 
\series bold
not a metric
\series default
.
\end_layout

\begin_layout Itemize
KL divergence is 
\series bold
not symmetric
\series default
.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Jensen's Inequality
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Jensen's Inequality]
\end_layout

\end_inset

If 
\begin_inset Formula $f:\cx\to\reals$
\end_inset

 is a 
\series bold
convex
\series default
 function, and 
\begin_inset Formula $X\in\cx$
\end_inset

 is a random variable, then
\begin_inset Formula 
\[
\ex f(X)\ge f(\ex X).\pause
\]

\end_inset

Moreover, if 
\begin_inset Formula $f$
\end_inset

 is 
\series bold
strictly convex
\series default
, then equality implies that 
\begin_inset Formula $X=\ex X$
\end_inset

 with probability 
\begin_inset Formula $1$
\end_inset

 (i.e.
 
\begin_inset Formula $X$
\end_inset

 is a constant).
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 
\begin_inset Formula $f(x)=x^{2}$
\end_inset

 is convex.
 So 
\begin_inset Formula $\ex X^{2}\ge\left(\ex X\right)^{2}$
\end_inset

.
 Thus 
\begin_inset Formula 
\[
\var X=\ex X^{2}-\left(\ex X\right)^{2}\ge0.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Jensen's inequality is used to prove Gibbs inequality 
\begin_inset Formula $(\log(x)$
\end_inset

 is strictly concave).
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
EM Algorithm for Latent Variable Models
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model (
\begin_inset Formula $k=3$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Enumerate
Choose 
\begin_inset Formula $Z\in\left\{ 1,2,3\right\} \sim\text{Multi}\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Choose 
\begin_inset Formula $X\mid Z=z\sim\cn\left(X\mid\mu_{z},\Sigma_{z}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/clustering/mixture-3-gaussians.png
	lyxscale 30
	width 70text%

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model (
\begin_inset Formula $k$
\end_inset

 Components)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
GMM Parameters 
\begin_inset Formula 
\begin{eqnarray*}
\text{Cluster probabilities}: &  & \pi=\left(\pi_{1},\ldots,\pi_{k}\right)\\
\text{Cluster means}: &  & \mu=\left(\mu_{1},\ldots,\mu_{k}\right)\\
\text{Cluster covariance matrices:} &  & \Sigma=\left(\Sigma_{1},\ldots\Sigma_{k}\right)
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\theta=\left(\pi,\mu,\Sigma\right)$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Marginal log-likelihood
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta) & = & \log\left\{ \sum_{z=1}^{k}\pi_{z}\cn\left(x\mid\mu_{z},\Sigma_{z}\right)\right\} 
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General Latent Variable Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Two sets of random variables: 
\begin_inset Formula $Z$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $Z$
\end_inset

 consists of unobserved 
\series bold
hidden variables
\series default
.
\end_layout

\begin_layout Itemize
\begin_inset Formula $X$
\end_inset

 consists of 
\series bold
observed variables
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Joint probability model parameterized by 
\begin_inset Formula $\theta\in\Theta$
\end_inset

:
\begin_inset Formula 
\[
p(x,z\mid\theta)
\]

\end_inset


\end_layout

\begin_layout AlertBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Notation abuse
\end_layout

\end_inset


\end_layout

\begin_layout AlertBlock
Notation 
\begin_inset Formula $p(x,z\mid\theta)$
\end_inset

 suggests a Bayesian setting, in which 
\begin_inset Formula $\theta$
\end_inset

 is a r.v.
 However we are 
\series bold
not
\series default
 assuming a Bayesian setting.
 
\begin_inset Formula $p(x,z\mid\theta)$
\end_inset

 is just easier to read than 
\begin_inset Formula $p_{\theta}(x,z)$
\end_inset

, once 
\begin_inset Formula $\theta$
\end_inset

 gets more complicated.
 
\begin_inset Formula 
\[
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Complete and Incomplete Data
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
An observation of 
\begin_inset Formula $X$
\end_inset

 is called an 
\series bold
incomplete data set
\series default
.
\end_layout

\begin_layout Itemize
An observation 
\begin_inset Formula $\left(X,Z\right)$
\end_inset

 is called a 
\series bold
complete data set
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
We never have a complete data set for latent variable models.
\end_layout

\begin_layout Itemize
But it's a useful construct.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose we have an incomplete data set 
\begin_inset Formula $\cd=\left(x_{1},\ldots,x_{n}\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To simplify notation, take 
\begin_inset Formula $X$
\end_inset

 to represent the entire dataset
\begin_inset Formula 
\[
X=\left(X_{1},\ldots,X_{n}\right),
\]

\end_inset

and 
\begin_inset Formula $Z$
\end_inset

 to represent the corresponding unobserved variables
\begin_inset Formula 
\[
Z=\left(Z_{1},\ldots,Z_{n}\right).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Log-Likelihood
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The log-likelihood of 
\begin_inset Formula $\theta$
\end_inset

 for observation 
\begin_inset Formula $X=x$
\end_inset

 is
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta) & = & \log\left\{ \sum_{z}p(x,z\mid\theta)\right\} .
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
(We write discrete case -- everything same for continuous case.)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For exponential families,
\end_layout

\begin_deeper
\begin_layout Itemize
Without the sum 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $\sum_{z}$
\end_inset


\begin_inset Quotes erd
\end_inset

, things simplify.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $\log$
\end_inset

 and the 
\begin_inset Formula $\exp$
\end_inset

 cancel out.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Assumption for the EM algorithm
\series default
:
\end_layout

\begin_deeper
\begin_layout Itemize
Optimization for complete data is relatively easy
\begin_inset Formula 
\[
\argmax_{\theta\in\Theta}\;\log p(x,z\mid\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
(We'll actually need slightly more than this.)
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The EM Algorithm 
\series bold
Key Idea
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Marginal log likelihood is hard to optimize:
\begin_inset Formula 
\[
\max_{\theta}\;\log\left\{ \sum_{z}p(x,z\mid\theta)\right\} 
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Full log-likelihood would be easy to optimize:
\begin_inset Formula 
\[
\max_{\theta}\;\log p(x,z\mid\theta)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we had a 
\series bold
distribution 
\series default

\begin_inset Formula $q(z)$
\end_inset

 for the latent variables 
\begin_inset Formula $Z$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 
\begin_inset Formula $q(z)=p(z\mid x,\theta)$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Could maximize the 
\series bold
expected complete data log-likelihood
\series default
:
\begin_inset Formula 
\[
\max_{\theta}\:\sum_{z}q(z)\log p(x,z\mid\theta)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A Lower Bound for Marginal Likelihood
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $q(z)$
\end_inset

 be any PMF on 
\begin_inset Formula $\cz$
\end_inset

, the support of 
\begin_inset Formula $Z$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta) & = & \log\sum_{z}p(x,z\mid\theta)\\
\pause & = & \log\sum_{z}q(z)\left[\frac{p(x,z\mid\theta)}{q(z)}\right]\\
\pause & \ge & \sum_{z}q(z)\log\left(\frac{p(x,z\mid\theta)}{q(z)}\right)\\
\pause & =: & \cl(q,\theta).
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The inequality is by Jensen's, by concavity of the 
\begin_inset Formula $\log$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Lower Bound and Expected Complete Log-Likelihood
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider maximizing the lower bound 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\pause\cl(q,\theta) & = & \sum_{z}q(z)\log\left(\frac{p(x,z\mid\theta)}{q(z)}\right)\\
\pause & = & \underbrace{\sum_{z}q(z)\log p(x,z\mid\theta)}_{\ex\left[\text{complete log-likelihood}\right]}-\underbrace{\sum_{z}q(z)\log q(z)}_{\text{no }\theta\text{ here}}\pause
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Maximizing 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

 equivalent to maximizing 
\begin_inset Formula $\ex\left[\text{complete data log-likelihood}\right]$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A Family of Lower Bounds
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Each 
\begin_inset Formula $q$
\end_inset

 gives a different lower bound: 
\begin_inset Formula $\log p(x\mid\theta)\ge\cl(q,\theta)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Two lower bounds, as functions of 
\begin_inset Formula $\theta$
\end_inset

:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/EM-algorithm/lowerBounds-Bishop9.14.png
	lyxscale 50
	height 55theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM: Coordinate Ascent on Lower Bound
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In EM algorithm, we maximize the lower bound 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

:
\begin_inset Formula 
\[
\log p(x\mid\theta)\ge\cl(q,\theta).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
EM Algorithm (high level):
\end_layout

\begin_deeper
\begin_layout Enumerate
Choose initial 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $q^{*}=\argmax_{q}\cl(q,\theta^{\text{old}})$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $\theta^{\text{new}}=\argmax_{\theta}\cl(q^{*},\theta)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Go to step 2, until converged.
\end_layout

\end_deeper
\begin_layout Itemize
Will show:
\series bold
 
\begin_inset Formula $p(x\mid\theta^{\text{new}})\ge p(x\mid\theta^{\text{old}})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Get sequence of 
\begin_inset Formula $\theta$
\end_inset

's with monotonically increasing likelihood.
\series default

\begin_inset Note Note
status open

\begin_layout Itemize
Why is this a good idea?
\end_layout

\begin_deeper
\begin_layout Itemize
In many situations, relatively easier to find 
\begin_inset Formula $q^{\text{new}}$
\end_inset

 and 
\begin_inset Formula $\theta^{\text{new}}$
\end_inset

.
\end_layout

\begin_layout Itemize
e.g.
 GMM
\end_layout

\end_deeper
\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM: Coordinate Ascent on Lower Bound
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Start at 
\begin_inset Formula $\theta^{\text{old}}.$
\end_inset

 Find best lower bound at 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

: 
\begin_inset Formula $\cl(q,\theta)$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\theta^{\text{new}}=\argmax_{\theta}\cl(q,\theta)$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/EM-algorithm/EM-twosteps-Bishop9.14.png
	lyxscale 50
	height 55theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Lower Bound
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let's investigate the lower bound:
\begin_inset Formula 
\begin{eqnarray*}
\cl(q,\theta) & = & \sum_{z}q(z)\log\left(\frac{p(x,z\mid\theta)}{q(z)}\right)\\
\pause & = & \sum_{z}q(z)\log\left(\frac{p(z\mid x,\theta)p(x\mid\theta)}{q(z)}\right)\\
\pause & = & \sum_{z}q(z)\log\left(\frac{p(z\mid x,\theta)}{q(z)}\right)+\sum_{z}q(z)\log p(x\mid\theta)\\
\pause & = & -\kl[q(z),p(z\mid x,\theta)]+\log p(x\mid\theta)\pause
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Itemize
Amazing! We get back an equality for the marginal likelihood:
\begin_inset Formula 
\[
\log p(x\mid\theta)=\cl(q,\theta)+\kl[q(z),p(z\mid x,\theta)]
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Best Lower Bound
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $q$
\end_inset

 maximizing
\begin_inset Formula 
\begin{eqnarray*}
\cl(q,\theta^{\text{old}}) & = & -\kl[q(z),p(z\mid x,\theta^{\text{old}})]+\underbrace{\log p(x\mid\theta^{\text{old}})}_{\text{no }q\text{ here}}?
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Recall 
\begin_inset Formula $\kl(p\|q)\ge0$
\end_inset

, and 
\begin_inset Formula $\kl(p\|p)=0$
\end_inset

.
\end_layout

\begin_layout Itemize
Best 
\begin_inset Formula $q$
\end_inset

 is 
\begin_inset Formula $q^{*}(z)=p(z\mid x,\theta^{\text{old}})$
\end_inset

:
\begin_inset Formula 
\[
\pause\cl(q^{*},\theta^{\text{old}})=-\underbrace{\kl[p(z\mid x,\theta^{\text{old}}),p(z\mid x,\theta^{\text{old}})]}_{=0}+\log p(x\mid\theta^{\text{old}})
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Summary:
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta^{\text{old}}) & = & \cl(q^{*},\theta^{\text{old}})\quad(\text{tangent at }\theta^{\text{old}}).\\
\pause\log p(x\mid\theta) & \ge & \cl(q^{*},\theta)\quad\forall\theta
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General EM Algorithm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Choose initial 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

.
\end_layout

\begin_layout Enumerate

\series bold
Expectation Step
\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $q^{*}(z)=p(z\mid x,\theta^{\text{old}})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let
\begin_inset Formula 
\[
J(\theta)=\cl(q^{*},\theta)=\sum_{z}q^{*}(z)\log\left(\frac{p(x,z\mid\theta)}{q^{*}(z)}\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that 
\begin_inset Formula $J(\theta)$
\end_inset

 is an 
\series bold
expectation
\series default
 w.r.t.
 
\begin_inset Formula $Z\sim q^{*}(z)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate

\series bold
Maximization Step
\series default
 
\begin_inset Formula 
\[
\theta^{\text{new}}=\argmax_{\theta}J(\theta).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Go to step 2, until converged.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
EM Monotonically Increases Likelihood
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM Gives Monotonically Increasing Likelihood: By Picture
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/EM-algorithm/EM-twosteps-Bishop9.14.png
	lyxscale 50
	height 55theight%

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM Gives Monotonically Increasing Likelihood: By Math
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Start at 
\begin_inset Formula $\theta^{\text{old}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Choose 
\begin_inset Formula $q^{*}(z)=\argmax_{q}\cl(q,\theta^{\text{old}})$
\end_inset

.
 We've shown
\begin_inset Formula 
\[
\log p(x\mid\theta^{\text{old}})=\cl(q^{*},\theta^{\text{old}})
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Choose 
\begin_inset Formula $\theta^{\text{new}}=\argmax_{\theta}\cl(q^{*},\theta^{\text{old}})$
\end_inset

.
 So
\begin_inset Formula 
\begin{eqnarray*}
\cl(q^{*},\theta^{\text{new}}) & \ge & \cl(q^{*},\theta^{\text{old}}).
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Putting it together, we get
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta^{\text{new}}) & \ge & \cl(q^{*},\theta^{\text{new}})\qquad\cl\text{ is a lower bound}\\
\pause & \ge & \cl(q^{*},\theta^{\text{old}})\qquad\mbox{By definition of }\theta^{\text{new}}\\
\pause & = & \log p(x\mid\theta^{\text{old}})\qquad\text{Bound is tight at }\theta^{\text{old}}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Bishop's 
\backslash
emph{Pattern recognition and machine learning}, Figure 9.14.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
EM Gives Monotonically Increasing Likelihood: And so?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\theta_{n}$
\end_inset

 be value of EM algorithm after 
\begin_inset Formula $n$
\end_inset

 steps.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Are there conditions for which
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{n}$
\end_inset

 converges to the maximum likelihood?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{n}$
\end_inset

 converges to a local maximum?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{n}$
\end_inset

 converges to a stationary point of likelihood?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\theta_{n}$
\end_inset

 converges?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
There are conditions for each of these (to happen and not to happen).
\end_layout

\begin_layout Itemize
See 
\begin_inset Quotes eld
\end_inset

On the Convergence Properties of the EM Algorithm
\begin_inset Quotes erd
\end_inset

 by C.
 F.
 Jeff Wu, 
\emph on
The Annals of Statistics
\emph default
, Mar.
 1983.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://web.stanford.edu/class/ee378b/papers/wu-em.pdf
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
In practice, can run EM multiple times with random starts.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
The Gaussian Mixture Model
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Homework: Derive EM for GMM from General EM Algorithm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Subsequent slides may help set things up.
\end_layout

\begin_layout Itemize
Key skills:
\end_layout

\begin_deeper
\begin_layout Itemize
MLE for multivariate Gaussian distributions.
\end_layout

\begin_layout Itemize
Lagrange multipliers 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model (
\begin_inset Formula $k=3$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Enumerate
Choose 
\begin_inset Formula $Z\in\left\{ 1,2,3\right\} \sim\text{Multi}\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Choose 
\begin_inset Formula $X\mid Z=z\sim\cn\left(X\mid\mu_{z},\Sigma_{z}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/clustering/mixture-3-gaussians.png
	lyxscale 30
	width 70text%

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gaussian Mixture Model (
\begin_inset Formula $k$
\end_inset

 Components)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
GMM Parameters 
\begin_inset Formula 
\begin{eqnarray*}
\text{Cluster probabilities}: &  & \pi=\left(\pi_{1},\ldots,\pi_{k}\right)\\
\text{Cluster means}: &  & \mu=\left(\mu_{1},\ldots,\mu_{k}\right)\\
\text{Cluster covariance matrices:} &  & \Sigma=\left(\Sigma_{1},\ldots\Sigma_{k}\right)
\end{eqnarray*}

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\theta=\left(\pi,\mu,\Sigma\right)$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Marginal log-likelihood
\begin_inset Formula 
\begin{eqnarray*}
\log p(x\mid\theta) & = & \log\left\{ \sum_{z=1}^{k}\pi_{z}\cn\left(x\mid\mu_{z},\Sigma_{z}\right)\right\} 
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $q^{*}(z)$
\end_inset

 = Soft Assignments
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
At each step, we take 
\begin_inset Formula 
\[
q^{*}(z)=p(z\mid x,\theta^{\text{old}})
\]

\end_inset

.
\end_layout

\begin_layout Itemize
This corresponds to 
\begin_inset Quotes eld
\end_inset

soft assignments
\begin_inset Quotes erd
\end_inset

 we had last time:
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{i}^{j} & = & \pr\left(Z=j\mid X=x_{i}\right)\\
\pause & = & \frac{\pi_{j}\cn\left(x_{i}\mid\mu_{j},\Sigma_{j}\right)}{\sum_{c=1}^{k}\pi_{c}\cn\left(x_{i}\mid\mu_{c},\Sigma_{c}\right)}
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Expectation Step
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
The complete log-likelihood is
\begin_inset Formula 
\begin{eqnarray*}
\log p(x,z\mid\theta) & = & \sum_{i=1}^{n}\log\left[\pi_{z}\cn\left(x_{i}\mid\mu_{z},\Sigma_{z}\right)\right]\\
 & = & \sum_{i=1}^{n}\left(\log\pi_{z}+\underbrace{\log\cn\left(x_{i}\mid\mu_{z},\Sigma_{z}\right)}_{\text{simplifies nicely}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Take the expected complete log-likelihood w.r.t.
 
\begin_inset Formula $q^{*}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
J(\theta) & = & \sum_{z}q^{*}(z)\log p(x,z\mid\theta)\\
 & = & \sum_{i=1}^{n}\sum_{j=1}^{k}\gamma_{i}^{j}\left[\log\pi_{j}+\log\cn\left(x_{i}\mid\mu_{j},\Sigma_{j}\right)\right]
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Maximization Step
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\theta^{*}$
\end_inset

 maximizing 
\begin_inset Formula $J(\theta)$
\end_inset

.
 Result is what we had last time:
\begin_inset Formula 
\begin{eqnarray*}
\mu_{c}^{\text{new}} & = & \frac{1}{n_{c}}\sum_{i=1}^{n}\gamma_{i}^{c}x_{i}\\
\Sigma_{c}^{\text{new}} & = & \frac{1}{n_{c}}\sum_{i=1}^{n}\gamma_{i}^{c}\left(x_{i}-\mu_{\text{MLE}}\right)\left(x_{i}-\mu_{\text{MLE}}\right)^{T}\\
\pi_{c}^{\text{new}} & = & \frac{n_{c}}{n},
\end{eqnarray*}

\end_inset

 for each 
\begin_inset Formula $c=1,\ldots,k$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Recommendations for Further Study
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Machine Learning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Look at other course notes at this level.
\end_layout

\begin_deeper
\begin_layout Itemize
Every course covers different subset of topics.
\end_layout

\begin_layout Itemize
Different perspectives.
 (e.g.
 Bayesian / Probabilistic)
\end_layout

\end_deeper
\begin_layout Enumerate
Read on some 
\begin_inset Quotes eld
\end_inset

second semester
\begin_inset Quotes erd
\end_inset

 topics
\end_layout

\begin_deeper
\begin_layout Itemize
LDA / Topic Models (DS-GA 1005?)
\end_layout

\begin_layout Itemize
Sequence models: Hidden Markov Models / MEMMs / CRFs (DS-GA 1005)
\end_layout

\begin_layout Itemize
Bayesian methods
\end_layout

\begin_layout Itemize
Collaborative Filtering / Recommendations
\end_layout

\begin_layout Itemize
Ranking
\end_layout

\begin_layout Itemize
Bandit problems (Thompson sampling / UCB methods)
\end_layout

\begin_layout Itemize
Gaussian processes
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Other Stuff To Learn
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Statistics
\end_layout

\begin_layout Itemize
Data Structures & Algorithms (Theoretical)
\end_layout

\begin_layout Itemize
Some production programming language (e.g.
 Java, C++)
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_body
\end_document
