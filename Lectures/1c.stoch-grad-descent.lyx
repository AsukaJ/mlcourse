#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_math eulervm
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Gradient and Stochastic Gradient Descent
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Give example of why gradient descent is bad when we have very different
 scales of different coordinates.
 Figure 7.4 of Bishop's Neural Networks for Pattern Recognition book.
 
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\emph on
Linear
\emph default
 Least Squares Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Setup
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Output space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Action space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Loss: 
\begin_inset Formula $\loss(\hat{y},y)=\frac{1}{2}\left(y-\hat{y}\right)^{2}$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Hypothesis space:
\series default
 
\begin_inset Formula $\cf=\left\{ f:\cx\to\cy\mid f(x)=w^{T}x\right\} $
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Given data set 
\begin_inset Formula $\cd_{n}=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
Let's find the ERM 
\begin_inset Formula $\hat{f}\in\cf$
\end_inset

.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\emph on
Linear
\emph default
 Least Squares Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Objective Function: Empirical Risk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The function we want to minimize is the empirical risk:
\begin_inset Formula 
\[
\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2},
\]

\end_inset

where 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

 parameterizes the hypothesis space 
\begin_inset Formula $\cf$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unconstrained Optimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Setting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Objective function 
\begin_inset Formula $f:\reals^{d}\to\reals$
\end_inset

 is 
\emph on
differentiable.
\end_layout

\end_deeper
\begin_layout Block
Want to find 
\begin_inset Formula 
\[
x^{*}=\arg\min_{x\in\reals^{d}}f(x)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
gradient 
\series default

\begin_inset Formula $\del_{x}f(x_{0})$
\end_inset

 of a differentiable function 
\begin_inset Formula $f(x)$
\end_inset

 at the point 
\begin_inset Formula $x_{0}$
\end_inset

 is the direction to move in for the 
\series bold
fastest increase
\series default
 in 
\begin_inset Formula $f(x)$
\end_inset

, when starting from 
\begin_inset Formula $x_{0}$
\end_inset

.
\end_layout

\begin_layout Definition
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename ../Figures/GradientDescent/two-dim-gradient.png
	height 40theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Figure A.111 from 
\emph on
Newtonian Dynamics
\emph default
, by Richard Fitzpatrick.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Initialize 
\begin_inset Formula $x=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x\gets x-\underbrace{\eta}_{\mbox{step size}}\del f(x)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
until stopping criterion satisfied
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent Path
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient Descent for a nice (convex) function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/GradientDescent/gradientDescentPath-convex.png
	lyxscale 50
	height 70theight%

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent Path
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient Descent Path for the Rosenbrock Function (not convex)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/GradientDescent/Banana-SteepDesc.gif
	lyxscale 50
	height 60theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
(Figure by P.A.
 Simionescu from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{http://en.wikipedia.org/wiki/Image:Banana-SteepDesc.gif}{Wikipedia page
 on gradient descent}
\end_layout

\end_inset

 )
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent - Details
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Step Size
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Empirically 
\begin_inset Formula $\eta=0.1$
\end_inset

 often works well
\end_layout

\begin_layout Itemize

\series bold
Better
\series default
: Optimize at every step (e.g.
 backtracking line search)
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Steepest descent
\begin_inset Quotes erd
\end_inset

 means going all the way to the local minimum in the direction 
\begin_inset Formula $-\del f(x)$
\end_inset

.
 Consecutive step directions are orthogonal.
\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Stopping Rule
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Could use a maximum number of steps (e.g.
 
\begin_inset Formula $100$
\end_inset

)
\end_layout

\begin_layout Itemize
Wait until 
\begin_inset Formula $\|\del f(x)\|\leq\eps$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Test performance on holdout data (in learning setting)
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent for Linear Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient of Objective Function:
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The gradient of the objective is
\begin_inset Formula 
\begin{align*}
\del_{w}\hat{R}_{n}(w)= & \del_{w}\left[\frac{1}{n}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2}\right]\\
= & \frac{2}{n}\sum_{i=1}^{n}\underbrace{\left(w^{T}x_{i}-y_{i}\right)}_{i\mbox{th residual}}x_{i}
\end{align*}

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: Does it scale?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
At every iteration, we compute the gradient at current 
\begin_inset Formula $w$
\end_inset

:
\begin_inset Formula 
\[
\del_{w}\hat{R}_{n}(w)=\frac{2}{n}\sum_{i=1}^{n}\underbrace{\left(w^{T}x_{i}-y_{i}\right)}_{i\mbox{th residual}}x_{i}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We have to touch all 
\begin_inset Formula $n$
\end_inset

 training points to take a single step.
 [
\begin_inset Formula $O(n)$
\end_inset

]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Called a 
\series bold
batch optimization
\series default
 method
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can we make progress without looking at all the data?
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Real goal is to minimize the risk (expected loss):
\begin_inset Formula 
\[
\argmin_{f\in\cf}\ex\left[\ell(f(X),Y)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For linear regression, that's
\begin_inset Formula 
\[
\argmin_{w}\ex\left(w^{T}X-Y\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent on this?
\begin_inset Formula 
\begin{align*}
\del_{w}\ex\left(w^{T}X-Y\right)^{2}= & \ex\left[2\left(w^{T}X-Y\right)X\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk [approximately]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Want to find gradient of the risk: 
\begin_inset Formula 
\[
\del R(w)=\ex\left[2\left(w^{T}X-Y\right)X\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can estimate expectation with a sample:
\begin_inset Formula 
\[
\widehat{\del R(w)}=\frac{1}{n}\sum_{i=1}^{n}\left[2\left(\underbrace{w^{T}x_{i}-y_{i}}_{\mbox{i'th residual}}\right)x_{i}\right]
\]

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's return to the general case...
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk: General Case
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient of Risk:
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Say hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 is parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Switching
\begin_inset Note Note
status open

\begin_layout Plain Layout
http://planetmath.org/differentiationundertheintegralsign
\end_layout

\end_inset

 
\begin_inset Formula $\del_{w}$
\end_inset

 and 
\begin_inset Formula $\ex$
\end_inset

 we can write the gradient of risk as
\begin_inset Formula 
\begin{align*}
\mbox{Gradient(Risk)}=\del_{w}\ex\left[\ell(f(X),Y)\right]= & \ex\left[\del_{w}\ell(f(X),Y)\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\series bold
\size large
\color blue
Unbiased 
\series default
\size default
\color inherit
estimator for Gradient(Risk):
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\left[\del_{w}\ell(f_{w}(x_{i}),y_{i})\right]\approx\underbrace{\ex\left[\del_{w}\ell(f(X),Y)\right]}_{\mbox{Gradient(Risk)}}
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk: General Case
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We want Gradient(Risk)
\end_layout

\begin_layout Itemize
Estimate it using sample of size 
\begin_inset Formula $n$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Bigger 
\begin_inset Formula $n$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Better estimate
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Bigger 
\begin_inset Formula $n$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Touching more data (slower!)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But how big an 
\begin_inset Formula $n$
\end_inset

 do we need?
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk [approximately]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descent takes a bunch of steps whether we use
\end_layout

\begin_deeper
\begin_layout Itemize
the perfect step direction 
\begin_inset Formula $\del R(w)$
\end_inset

,
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
an empirical estimate using all training data 
\begin_inset Formula $\del\hat{R}_{n}(w)$
\end_inset

, or
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
an empirical estimate using a random subset of data 
\begin_inset Formula $\del\hat{R}_{N}(w)$
\end_inset

 (
\begin_inset Formula $N\ll n$
\end_inset

)
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What about 
\begin_inset Formula $N=1$
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Even with a sample of size 
\begin_inset Formula $1$
\end_inset

, the estimate
\begin_inset Formula 
\[
\del_{w}\ell(f_{w}(x_{i}),y_{i})
\]

\end_inset

is still 
\series bold
unbiased for gradient(Risk).
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic Gradient Descent (SGD)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Stochastic Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
initialize 
\begin_inset Formula $w=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat
\end_layout

\begin_deeper
\begin_layout Itemize
randomly choose training point 
\begin_inset Formula $(x_{i},y_{i})\in\cd_{n}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w\gets w-\eta\underbrace{\del_{w}\ell(f_{w}(x_{i}),y_{i})}_{\mbox{Grad(Loss on i'th example)}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
until stopping criteria met
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SGD: Step Size 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\eta_{t}$
\end_inset

 be the step size at the 
\begin_inset Formula $t$
\end_inset

'th step.
 
\end_layout

\begin_layout Itemize
How should 
\begin_inset Formula $\eta_{t}$
\end_inset

's decrease with each step?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Robbins-Monro Conditions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Many classical convergence results depend on the following two conditions:
\begin_inset Formula 
\[
\sum_{t=1}^{\infty}\eta_{t}^{2}<\infty\qquad\sum_{t=1}^{\infty}\eta_{t}=\infty
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
As fast as 
\begin_inset Formula $\eta_{t}=O\left(\frac{1}{t}\right)$
\end_inset

 would satisfy this...
 but should be faster than 
\begin_inset Formula $O\left(\frac{1}{\sqrt{t}}\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
A useful reference for practical techniques: Leon Bottou's 
\begin_inset Quotes eld
\end_inset

Tricks
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://research.microsoft.com/pubs/192769/tricks-2012.pdf
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Many interesting examples involve a loss function Q(z, w) which is not different
iable on a subset of points with probability zero.
 Intuition suggests that this is a minor problems because the iterations
 of the online gradient descent have zero probability to reach one of these
 points.
 Even if we reach one of these points, we can just draw another example
 z.
\end_layout

\begin_layout Plain Layout
The analysis presented in this section addresses the convergence of the
 general online gradient algorithm (section 2.3) applied to the optimization
 of a differentiable cost function C(w) with the following properties: •
 The cost function C(w) has a single minimum w ∗ .
 • The cost function C(w) satisfies the following condition: ∀ε > 0, inf
 (w−w∗) 2>ε (w − w ∗ ) ∇wC(w) > 0 (4.1) Condition (4.1) simply states that
 the opposite of the gradient −∇wC(w) always points towards the minimum
 w ∗ .
 This particular formulation also rejects cost functions which have plateaus
 on which the gradient vanishes without making us closer to the minimum.
\end_layout

\end_inset


\end_layout

\begin_layout Separator
\begin_inset Note Note
status open

\begin_layout Plain Layout
[http://jmlr.csail.mit.edu/proceedings/papers/v5/sunehag09a/sunehag09a.pdf]
 References: Robbins and Monro (1951) proved a theorem that implies convergence
 for one-dimensional stochastic gradient descent; Blum (1954) generalized
 it to the multivariate case.
 Robbins and Siegmund (1971) achieved a stronger result of wider applicability
 in supermartingale theory.
 Here we extend the known convergence results (Bottou and LeCun, 2005) in
 two ways: a) We prove that updates that include scaling matrices with eigenvalu
es bounded by positive constants from above and below will converge almost
 surely; b) under slightly stronger assumptions we obtain a O
\end_layout

\end_inset


\end_layout

\end_body
\end_document
