#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_math eulervm
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Classification and Regression Trees
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Should re-read HTF section for big picture review
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Should say up front that we'll be focusing on splits that work with a single
 feature at time? We could have linear combination features -- why not do
 this? What about multiple features at a time [box constraints?]
\end_layout

\end_inset


\end_layout

\begin_layout Section
Regression Trees
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General Tree Structure
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/generalTreeStructure.png
	lyxscale 50
	width 70theight%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Criminisi et al.
 MSR-TR-2011-114, 28 October 2011.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Decision Tree
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/decisionTree.png
	lyxscale 50
	width 50theight%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Criminisi et al.
 MSR-TR-2011-114, 28 October 2011.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Binary Decision Tree on 
\begin_inset Formula $\reals^{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider a binary tree on 
\begin_inset Formula $\left\{ \left(X_{1},X_{2}\right)\mid X_{1},X_{2}\in\reals\right\} $
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/basicBinaryTree.png
	lyxscale 50
	width 50theight%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename ../Figures/trees/binaryRegions.png
	lyxscale 50
	width 50theight%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From 
\backslash
emph{An Introduction to Statistical Learning, with applications in R} (Springer,
 2013) with permission from the authors: G.
 James, D.
 Witten,  T.
 Hastie and R.
 Tibshirani.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Binary Regression Tree on 
\begin_inset Formula $\reals^{2}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider a binary tree on 
\begin_inset Formula $\left\{ \left(X_{1},X_{2}\right)\mid X_{1},X_{2}\in\reals\right\} $
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/binaryRegions.png
	lyxscale 50
	width 50theight%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename ../Figures/trees/binaryTree3D.png
	lyxscale 50
	width 50theight%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From 
\backslash
emph{An Introduction to Statistical Learning, with applications in R} (Springer,
 2013) with permission from the authors: G.
 James, D.
 Witten,  T.
 Hastie and R.
 Tibshirani.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Fitting a Regression Tree
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The decision tree gives the partition of 
\begin_inset Formula $\cx$
\end_inset

 into regions:
\begin_inset Formula 
\[
\left\{ R_{1},\ldots,R_{M}\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Recall that a partition is a 
\series bold
disjoint union
\series default
, that is:
\begin_inset Formula 
\[
\cx=R_{1}\cup R_{2}\cup\cdots\cup R_{M}
\]

\end_inset

and
\begin_inset Formula 
\[
R_{i}\cap R_{j}=\emptyset\quad\forall i\neq j
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Fitting a Regression Tree
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Given the partition 
\begin_inset Formula $\left\{ R_{1},\ldots,R_{M}\right\} $
\end_inset

, final prediction is
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}c_{m}\ind{x\in R_{m}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How to choose 
\begin_inset Formula $c_{1},\ldots,c_{M}$
\end_inset

? 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For loss function 
\begin_inset Formula $\ell(\hat{y},y)=\left(\hat{y}-y\right)^{2}$
\end_inset

, best is
\begin_inset Formula 
\[
\hat{c}_{m}=\text{ave}(y_{i}\mid x_{i}\in R_{m}).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Complexity of a Tree
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\left|T\right|=M$
\end_inset

 denote the number of terminal nodes in 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We will use 
\begin_inset Formula $\left|T\right|$
\end_inset

 to measure the complexity of a tree.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any given complexity, 
\end_layout

\begin_deeper
\begin_layout Itemize
we want the tree minimizing square error on training set.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Finding the optimal binary tree of a given complexity is computationally
 intractable.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Learning the simplest (smallest) decision tree is an NP-complete problem
 [Hyafil & Rivest ’76] 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We proceed with a 
\series bold
greedy algorithm
\end_layout

\begin_deeper
\begin_layout Itemize
Means build the tree one node at a time, without any planning ahead.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Root Node, Continuous Variables
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $x=\left(x_{1},\ldots,x_{d}\right)\in\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Splitting variable
\series default
 
\begin_inset Formula $j\in\left\{ 1,\ldots,d\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Split point
\series default
 
\begin_inset Formula $s\in\reals$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Partition based on 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $s$
\end_inset

:
\begin_inset Formula 
\begin{align*}
R_{1}(j,s) & =\left\{ x\mid x_{j}\le s\right\} \\
R_{2}(j,s) & =\left\{ x\mid x_{j}>s\right\} 
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Root Node, Continuous Variables
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For each splitting variable 
\begin_inset Formula $j$
\end_inset

 and split point 
\begin_inset Formula $s$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\hat{c}_{1}(j,s) & = & \text{ave}(y_{i}\mid x_{i}\in R_{1}(j,s))\\
\hat{c}_{2}(j,s) & = & \text{ave}(y_{i}\mid x_{i}\in R_{2}(j,s))
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $j,s$
\end_inset

 minimizing
\begin_inset Formula 
\[
\sum_{i:x_{i}\in R_{1}(j,s)}\left(y_{i}-\hat{c}_{1}(j,s)\right)^{2}+\sum_{i:x_{i}\in R_{2}(j,s)}\left(y_{i}-\hat{c}_{2}(j,s)\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
How?
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
HOW DO WE Do THIS SPLITTING EFFICIENTLY??? For each variable, try splitting
 at each data point (or unique value in the range of that feature)...
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Then Proceed Recursively
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
We have determined 
\begin_inset Formula $R_{1}$
\end_inset

 and 
\begin_inset Formula $R_{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Find best split for points in 
\begin_inset Formula $R_{1}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Find best split for points in 
\begin_inset Formula $R_{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Continue...
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When do we stop?
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Complexity Control Strategy
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If the tree is too big, we may overfit.
\end_layout

\begin_layout Itemize
If too small, we may miss patterns in the data (underfit).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Typical approach: 
\end_layout

\begin_deeper
\begin_layout Enumerate
Build a really big tree (e.g.
 until all regions have 
\begin_inset Formula $\le5$
\end_inset

 points).
\end_layout

\begin_layout Enumerate
Prune the tree.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Tree Terminology
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Each 
\series bold
internal node
\series default
 
\end_layout

\begin_deeper
\begin_layout Itemize
has a splitting variable and a split point
\end_layout

\begin_layout Itemize
corresponds to binary partition of the space
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A 
\series bold
terminal node 
\series default
or 
\series bold
leaf node
\end_layout

\begin_deeper
\begin_layout Itemize
corresponds to a region
\end_layout

\begin_layout Itemize
corresponds to a particular prediction
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A 
\series bold
subtree
\series default
 
\begin_inset Formula $T\subset T_{0}$
\end_inset

 is any tree obtained by 
\series bold
pruning 
\series default

\begin_inset Formula $T_{0}$
\end_inset

, which means collapsing any number of its internal nodes.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Tree Pruning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Full Tree 
\begin_inset Formula $T_{0}$
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/bigBaseballTree.png
	lyxscale 30
	height 60theight%

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From 
\backslash
emph{An Introduction to Statistical Learning, with applications in R} (Springer,
 2013) with permission from the authors: G.
 James, D.
 Witten,  T.
 Hastie and R.
 Tibshirani.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Tree Pruning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Subtree 
\begin_inset Formula $T\subset T_{0}$
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/smallBaseballTree.png
	lyxscale 30
	width 43col%

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From 
\backslash
emph{An Introduction to Statistical Learning, with applications in R} (Springer,
 2013) with permission from the authors: G.
 James, D.
 Witten,  T.
 Hastie and R.
 Tibshirani.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Emprical Risk and Tree Complexity
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we want to prune a big tree 
\begin_inset Formula $T_{0}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\hat{R}(T)$
\end_inset

 be the empirical risk of 
\begin_inset Formula $T$
\end_inset

 (i.e.
 square error on training)
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Clearly, for any 
\begin_inset Formula $T\subset T_{0}$
\end_inset

, 
\begin_inset Formula $\hat{R}(T)\ge\hat{R}(T_{0})$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\left|T\right|$
\end_inset

 be the number of terminal nodes in 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\left|T\right|$
\end_inset

 is our measure of complexity for a tree.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Cost Complexity (or Weakest Link) Pruning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definitions
The 
\series bold
cost complexity criterion
\series default
 with parameter 
\begin_inset Formula $\alpha$
\end_inset

 is
\begin_inset Formula 
\[
C_{\alpha}(T)=\hat{R}(T)+\alpha\left|T\right|
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Trades off between empirical risk and complexity of tree.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Cost complexity pruning:
\end_layout

\begin_deeper
\begin_layout Itemize
For each 
\begin_inset Formula $\alpha$
\end_inset

, find the tree 
\begin_inset Formula $T\subset T_{0}$
\end_inset

 minimizing 
\begin_inset Formula $C_{\alpha}(T)$
\end_inset

.
\end_layout

\begin_layout Itemize
Use cross validation to find the right choice of 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $C_{\alpha}(T)$
\end_inset

 has familiar regularized ERM form, but
\end_layout

\begin_deeper
\begin_layout Itemize
Cannot take the gradient w.r.t.
 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Greedy Pruning is Sufficient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Find subtree 
\begin_inset Formula $T_{1}\subset T_{0}$
\end_inset

 that minimizes 
\begin_inset Formula $\hat{R}(T_{1})-\hat{R}(T_{0})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then find 
\begin_inset Formula $T_{2}\subset T_{1}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Repeat until we have just a single node.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $N$
\end_inset

 is the number of nodes of 
\begin_inset Formula $T_{0}$
\end_inset

 (terminal and internal nodes), then we end up with a set of trees:
\begin_inset Formula 
\[
\ct=\left\{ T_{0}\supset T_{1}\supset T_{2}\supset\cdots\supset T_{\left|N\right|}\right\} 
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Breiman et al.
 (1984) proved that this is all you need.
 That is:
\begin_inset Formula 
\[
\left\{ \argmin_{T\subset T_{0}}C_{\alpha}(T)\mid\alpha\ge0\right\} \subset\ct
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Would be great to include proof? or at least a reference?
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Regularization Path for Trees
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/pruningTreesize.png
	lyxscale 70
	height 45theight%

\end_inset


\end_layout

\begin_layout Standard
SPAM dataset: Blue curve is cross-validation estimate of misclassification
 rate as a function of tree size.
 Orange curve is test error.
 The cross-validation is indexed by values of 
\begin_inset Formula $\alpha$
\end_inset

, shown above.
 The tree sizes shown below refer to 
\begin_inset Formula $\left|T_{\alpha}\right|$
\end_inset

, the size of the original tree indexed by 
\begin_inset Formula $\alpha$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Why doesn't this overfit? For each 
\begin_inset Formula $\alpha,$
\end_inset

 we might get a different tree depth in each fold...
 the tree size is the size of the tree when use 
\begin_inset Formula $\alpha$
\end_inset

 on the full dataset, after pruning.
  My best guess for why we're not seeing overfitting is that the tree size
 complexity just doesn't get that large.
 Note that in this figure, tree size is the number of terminal nodes, not
 the depth of the tree.
 Perhaps if we'd been working with larger trees, we'd start to see overfitting.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{HTF Figure 9.4}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Separator
\begin_inset Note Note
status open

\begin_layout Separator

\end_layout

\begin_layout Section
Classification Trees
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Trees
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider classification case: 
\begin_inset Formula $\cy=\left\{ 1,2,\ldots,K\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We need to modify
\end_layout

\begin_deeper
\begin_layout Itemize
criteria for splitting nodes
\end_layout

\begin_layout Itemize
method for pruning tree 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Trees
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let node 
\begin_inset Formula $m$
\end_inset

 represent region 
\begin_inset Formula $R_{m}$
\end_inset

, with 
\begin_inset Formula $N_{m}$
\end_inset

 observations
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Denote proportion of observations in 
\begin_inset Formula $R_{m}$
\end_inset

 with class 
\begin_inset Formula $k$
\end_inset

 by
\begin_inset Formula 
\[
\hat{p}_{mk}=\frac{1}{N_{m}}\sum_{\left\{ i:x_{i}\in R_{m}\right\} }\ind{y_{i}=k}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Predicted classification
\series default
 for node 
\begin_inset Formula $m$
\end_inset

 is
\begin_inset Formula 
\[
k(m)=\argmax_{k}\hat{p}_{mk}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Predicted class probability distribution
\series default
 is 
\begin_inset Formula $\left(\hat{p}_{m1},\ldots,\hat{p}_{mk}\right)$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Misclassification Error
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider node 
\begin_inset Formula $m$
\end_inset

 representing region 
\begin_inset Formula $R_{m}$
\end_inset

, with 
\begin_inset Formula $N_{m}$
\end_inset

 observations
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose we predict 
\begin_inset Formula 
\[
k(m)=\argmax_{k}\hat{p}_{mk}
\]

\end_inset

 as the class for all inputs in region 
\begin_inset Formula $R_{m}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What is the misclassification rate on the training data?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
It's just
\begin_inset Formula 
\[
1-\hat{p}_{mk(m)}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Trees: Node Impurity Measures
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider node 
\begin_inset Formula $m$
\end_inset

 representing region 
\begin_inset Formula $R_{m}$
\end_inset

, with 
\begin_inset Formula $N_{m}$
\end_inset

 observations
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How can we generalize from squared error to classification?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We will introduce some different measures of 
\series bold
node impurity
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
We want 
\series bold
pure
\series default
 leaf nodes (i.e.
 as close to a single class as possible)
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We'll find splitting variables and split point 
\series bold
minimizing node impurity
\series default
.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Two-Class Node Impurity Measures
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Consider binary classification
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $p$
\end_inset

 be the relative frequency of class 1.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Here are three node impurity measures as a function of 
\begin_inset Formula $p$
\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/trees/impurityMeasureTwoClass.png
	lyxscale 50
	width 83theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{HTF Figure 9.3}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Trees: Node Impurity Measures
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider leaf node 
\begin_inset Formula $m$
\end_inset

 representing region 
\begin_inset Formula $R_{m}$
\end_inset

, with 
\begin_inset Formula $N_{m}$
\end_inset

 observations
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Three measures 
\begin_inset Formula $Q_{m}(T)$
\end_inset

 of 
\series bold
node impurity
\series default
 for leaf node 
\begin_inset Formula $m$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Misclassification error:
\begin_inset Formula 
\[
1-\hat{p}_{mk(m)}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gini index:
\begin_inset Formula 
\[
\sum_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk})
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Entropy
\begin_inset Note Note
status collapsed

\begin_layout Itemize
\begin_inset Foot
status open

\begin_layout Plain Layout
In HTF's book this is called 
\begin_inset Quotes eld
\end_inset

cross-entropy
\begin_inset Quotes erd
\end_inset

, but I don't know why.
 Using entropy as the node impurity measure is equivalent to using 
\begin_inset Quotes eld
\end_inset

information gain
\begin_inset Quotes erd
\end_inset

, which you may read about elsewhere.
\end_layout

\end_inset


\end_layout

\end_inset

 or deviance:
\begin_inset Formula 
\[
-\sum_{k=1}^{K}\hat{p}_{mk}\log\hat{p}_{mk}.
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
Should give example of how misclassification error can be the same, but
 gini or entropy can have preference for more pure classes.
 examples in the HTF or the more elementary book.
 
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Class Distributions: Pre-split 
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/trees/treeEntropyPreSplit.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Criminisi et al.
 MSR-TR-2011-114, 28 October 2011.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Class Distributions: Split Search 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Figures/trees/treeEntropySplitSearch.png
	lyxscale 50
	width 70text%

\end_inset


\end_layout

\begin_layout Itemize
(Maximizing information gain is equivalent to minimizing entropy)
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
Not clear what is going on here -- is this weighted average information
 gain? 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Criminisi et al.
 MSR-TR-2011-114, 28 October 2011.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Trees: How exactly do we do this?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $R_{L}$
\end_inset

 and 
\begin_inset Formula $R_{R}$
\end_inset

 be regions corresponding to a potential node split.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Suppose we have 
\begin_inset Formula $N_{L}$
\end_inset

 points in 
\begin_inset Formula $R_{L}$
\end_inset

 and 
\begin_inset Formula $N_{R}$
\end_inset

 points in 
\begin_inset Formula $R_{R}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $Q(R_{L})$
\end_inset

 and 
\begin_inset Formula $Q(R_{R})$
\end_inset

 be the node impurity measures.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The we search for a split that minimizes
\begin_inset Formula 
\[
N_{L}Q(R_{L})+N_{R}Q(R_{R})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Trees: Node Impurity Measures
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For building the tree, Gini and Entropy are more effective.
\end_layout

\begin_deeper
\begin_layout Itemize
They push for more pure nodes, not just misclassification rate 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
For pruning the tree, use misclassification error -- closer to risk estimate.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\end_layout

\begin_layout Section
Trees in General
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Missing Features (or 
\begin_inset Quotes eld
\end_inset

Predictors
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Features are also called 
\series bold
covariates
\series default
 or 
\series bold
predictors
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What to do about missing features? 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Throw out inputs with missing features
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Impute missing values with feature means
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If a categorical feature, let 
\begin_inset Quotes eld
\end_inset

missing
\begin_inset Quotes erd
\end_inset

 be a new category.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
For trees, can use 
\series bold
surrogate splits
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
For every internal node, form a list of surrogate features and split points
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Goal is to approximate the original split as well as possible
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Surrogates ordered by how well they approximate the original split.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator
\begin_inset Note Note
status open

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Categorical Features
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have feature with 
\begin_inset Formula $q$
\end_inset

 possible values (unordered).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We want to find the best split into 2 groups
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
There are 
\begin_inset Formula $2^{q-1}$
\end_inset

 possible partitions.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Search time?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
For binary classification 
\begin_inset Formula $(K=2)$
\end_inset

, there is an efficient algorithm.
 (Breiman 1984)
\begin_inset Note Note
status open

\begin_layout Plain Layout
binary classification, or a feature with two values???
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Otherwise, can use approximations.
\end_layout

\end_deeper
\begin_layout Itemize
Statistical issue?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
If a category has a very large number of categories, we can overfit.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Extreme example: Row Number could lead to perfect classification with a
 single split.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Trees vs Linear Models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Trees have to work much harder to capture linear relations.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/trees/treeVsLinear.pdf
	lyxscale 30
	height 60theight%

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From 
\backslash
emph{An Introduction to Statistical Learning, with applications in R} (Springer,
 2013) with permission from the authors: G.
 James, D.
 Witten,  T.
 Hastie and R.
 Tibshirani.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Interpretability
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Trees are certainly easy to explain.
\end_layout

\begin_layout Itemize
You can show a tree on a slide.
\end_layout

\begin_layout Itemize
Small trees seem interpretable.
\end_layout

\begin_layout Itemize
For large trees, maybe not so easy.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Trees for Nonlinear Feature Discovery
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose tree 
\begin_inset Formula $T$
\end_inset

 gives partition 
\begin_inset Formula $R_{1},\ldots,R_{m}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Predictions are
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}c_{m}\ind{x\in R_{m}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If we make a feature for every region 
\begin_inset Formula $R$
\end_inset

:
\begin_inset Formula 
\[
\ind{x\in R},
\]

\end_inset

we can view this as a 
\series bold
linear model
\series default
.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Trees can be used to discover nonlinear features.
 
\end_layout

\end_deeper
\begin_layout Separator
\begin_inset Note Note
status open

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Instability / High Variance of Trees
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Trees are
\series bold
 high variance
\series default
: 
\end_layout

\begin_deeper
\begin_layout Itemize
If we randomly split the data, we may get quite different trees from each
 part 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
By contrast, linear models have low variance (at least when well-regularized)
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Later we investigate several ways to reduce this variance
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comments about Trees
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Trees make no use of 
\series bold
geometry
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
No inner products or distances
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
called a 
\begin_inset Quotes eld
\end_inset

nonmetric
\begin_inset Quotes erd
\end_inset

 method
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature scale irrelevant
\series default
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Predictions are not continuous
\end_layout

\begin_deeper
\begin_layout Itemize
not so bad for classification
\end_layout

\begin_layout Itemize
may not be desirable for regression
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The discussion above focuses on the CART (classification and regression
 tree) implementation of trees.
 The other popular methodology is ID3 and its later versions, C4.5 and C5.0
 (Quinlan, 1993).
 
\end_layout

\end_inset


\end_layout

\end_body
\end_document
