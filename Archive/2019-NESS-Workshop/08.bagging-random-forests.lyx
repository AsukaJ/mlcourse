#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
%\setbeamercolor{frametitle}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=NYUPurple,urlcolor=LightPurple"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Part
Bagging and Random Forests 
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Contents
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Notes from Zhi-Hua Zhou's book 
\emph on
Ensemble Methods: foundations and algorithms: 
\emph default
ensemble combines base
\emph on
 
\emph default
learners; base learning algorithms generate these learners; when ensemble
 method using a single base learning algorithm, we get a 
\series bold
homogeneous ensemble
\series default
 (e.g.
 boosting, bagging, random forests), but there are also methods that use
 multiple learning algorithms to produce 
\series bold
heterogeneous ensembles
\series default
.
 Learning algorithms may be called 
\series bold
individual learners
\series default
 or 
\series bold
component learners
\series default
.
 
\end_layout

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

There are three threads of early contributions .
 .
 [to] ensemble methods; that is, combining classifiers, ensembles of weak
 learners, and mixture of experts.
\begin_inset Quotes erd
\end_inset

 (p 16) 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Ensemble Methods: Introduction
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ensembles: Parallel vs Sequential
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Ensemble methods combine multiple models 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Parallel ensembles
\series default
: each model is built independently
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 bagging and random forests
\end_layout

\begin_layout Itemize
Main Idea: Combine many (high complexity, low bias) models to reduce variance
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Sequential ensembles
\series default
: 
\end_layout

\begin_deeper
\begin_layout Itemize
Models are generated sequentially
\end_layout

\begin_layout Itemize
Try to add new models that do well where previous models lack
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Tree-based Ensemble Methods
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
In the next two lectures, we'll discuss many methods for creating ensembles
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Many methods we'll discuss (bagging, Adaboost, and gradient boosting)
\end_layout

\begin_deeper
\begin_layout Itemize
can be used with a wide range of base model types.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
However,
\series bold
 in practice, they're almost always used with trees.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Note Note
status open

\begin_layout Plain Layout
not even sure this is true for bagging though...
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
The Benefits of Averaging
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
A Poor Estimator
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $z,z_{1},\ldots,z_{n}$
\end_inset

 i.i.d.
 
\begin_inset Formula $\ex z=\mu$
\end_inset

 and 
\begin_inset Formula $\var\left(z\right)=\sigma^{2}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We could use any single 
\begin_inset Formula $z_{i}$
\end_inset

 to estimate 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Performance?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Unbiased: 
\begin_inset Formula $\ex z_{i}=\mu$
\end_inset

.
\end_layout

\begin_layout Itemize
Standard error of estimator would be 
\begin_inset Formula $\sigma$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
The
\series bold
 standard error
\series default
 is the standard deviation of the sampling distribution of a statistic.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\text{SD}(z)=\sqrt{\var(z)}=\sqrt{\sigma^{2}}=\sigma$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Variance of a Mean 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $z,z_{1},\ldots,z_{n}$
\end_inset

 be i.i.d.
 with 
\begin_inset Formula $\ex z=\mu$
\end_inset

 and 
\begin_inset Formula $\var\left(z\right)=\sigma^{2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Let's consider the average of the 
\begin_inset Formula $z_{i}$
\end_inset

's.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Average has the same expected value but smaller standard error:
\begin_inset Formula 
\[
\ex\left[\frac{1}{n}\sum_{i=1}^{n}z_{i}\right]=\mu\qquad\var\left[\frac{1}{n}\sum_{i=1}^{n}z_{i}\right]=\frac{\sigma^{2}}{n}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Clearly the average is preferred to a single 
\begin_inset Formula $z_{i}$
\end_inset

 as estimator.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can we apply this to reduce variance of general prediction functions? 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Averaging Independent Prediction Functions 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have 
\begin_inset Formula $B$
\end_inset

 independent training sets from the same distribution.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Learning algorithm gives 
\begin_inset Formula $B$
\end_inset

 decision functions: 
\begin_inset Formula $\hat{f}_{1}(x),\hat{f}_{2}(x),\ldots,\hat{f}_{B}(x)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define the average prediction function as:
\begin_inset Formula 
\[
\hat{f}_{\text{avg}}=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What's random here?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\begin_inset Formula $B$
\end_inset

 independent training sets are random, which gives rise to variation among
 the 
\begin_inset Formula $\hat{f}_{b}$
\end_inset

's.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Averaging Independent Prediction Functions 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Fix some particular 
\begin_inset Formula $x_{0}\in\cx$
\end_inset

.
\end_layout

\begin_layout Itemize
Then average prediction on 
\begin_inset Formula $x_{0}$
\end_inset

 is
\begin_inset Formula 
\[
\hat{f}_{\text{avg}}(x_{0})=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}(x_{0}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $\hat{f}_{\text{avg}}(x_{0})$
\end_inset

 and 
\begin_inset Formula $\hat{f}_{1}(x_{0}),\ldots,\hat{f}_{B}(x_{0})$
\end_inset

 as random variables
\end_layout

\begin_deeper
\begin_layout Itemize
Since the training sets were random
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We have no idea about the distributions of 
\begin_inset Formula $\hat{f}_{1}(x_{0}),\ldots,\hat{f}_{B}(x_{0})$
\end_inset

 – they could be crazy...
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But we do know that 
\begin_inset Formula $\hat{f}_{1}(x_{0}),\ldots,\hat{f}_{B}(x_{0})$
\end_inset

 are i.i.d.
 And that's all we need here...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Averaging Independent Prediction Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The average prediction on 
\begin_inset Formula $x_{0}$
\end_inset

 is
\begin_inset Formula 
\[
\hat{f}_{\text{avg}}(x_{0})=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}(x_{0}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{f}_{\text{avg}}(x_{0})$
\end_inset

 and 
\begin_inset Formula $\hat{f}_{b}(x_{0})$
\end_inset

 have the same expected value, but
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{f}_{\text{avg}}(x_{0})$
\end_inset

 has smaller variance:
\begin_inset Formula 
\begin{eqnarray*}
\mbox{\var}(\hat{f}_{\mbox{avg}}(x_{0})) & = & \frac{1}{B^{2}}\var\left(\sum_{b=1}^{B}\hat{f}_{b}(x_{0})\right)\\
\pause & = & \frac{1}{B}\var\left(\hat{f}_{1}(x_{0})\right)
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Averaging Independent Prediction Functions 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Using
\begin_inset Formula 
\[
\hat{f}_{\text{avg}}=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}
\]

\end_inset

seems like a win.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But in practice we don't have 
\begin_inset Formula $B$
\end_inset

 independent training sets...
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Instead, we can use 
\series bold
the bootstrap
\series default
....
 
\end_layout

\end_deeper
\begin_layout Section
The Bootstrap
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Bootstrap Sample
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A 
\series bold
bootstrap sample
\series default
 from 
\begin_inset Formula $\cd_{n}$
\end_inset

 is a sample of size 
\begin_inset Formula $n$
\end_inset

 drawn 
\emph on
with replacement
\emph default
 from 
\begin_inset Formula $\cd_{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In a bootstrap sample, some elements of 
\begin_inset Formula $\cd_{n}$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
will show up multiple times, 
\end_layout

\begin_layout Itemize
some won't show up at all.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Each 
\begin_inset Formula $X_{i}$
\end_inset

 has a probability 
\begin_inset Formula $(1-1/n)^{n}$
\end_inset

 of not being selected.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Recall from analysis that for large 
\begin_inset Formula $n$
\end_inset

,
\begin_inset Formula 
\[
\left(1-\frac{1}{n}\right)^{n}\approx\frac{1}{e}\approx.368.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So we expect ~63.2% of elements of 
\begin_inset Formula $\cd$
\end_inset

 will show up at least once.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Bootstrap Method
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A 
\series bold
bootstrap method 
\series default
is when you 
\emph on
simulate 
\emph default
having 
\begin_inset Formula $B$
\end_inset

 independent samples from 
\begin_inset Formula $P$
\end_inset

 by taking 
\begin_inset Formula $B$
\end_inset

 bootstrap samples from the sample 
\begin_inset Formula $\cd_{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Given original data 
\begin_inset Formula $\cd_{n}$
\end_inset

, compute 
\begin_inset Formula $B$
\end_inset

 bootstrap samples 
\begin_inset Formula $D_{n}^{1},\ldots,D_{n}^{B}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For each bootstrap sample, compute some function
\begin_inset Formula 
\[
\phi(D_{n}^{1}),\ldots,\phi(D_{n}^{B})
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Work with these values as though 
\begin_inset Formula $D_{n}^{1},\ldots,D_{n}^{B}$
\end_inset

 were i.i.d.
 
\begin_inset Formula $P$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Amazing fact:
\series default
 Things often come out very close to what we'd get with independent samples
 from 
\begin_inset Formula $P$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Section
Bagging 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bagging
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Draw 
\begin_inset Formula $B$
\end_inset

 bootstrap samples 
\begin_inset Formula $D^{1},\ldots,D^{B}$
\end_inset

 from original data 
\begin_inset Formula $\cd$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\hat{f}_{1},\hat{f}_{2},\ldots,\hat{f}_{B}$
\end_inset

 be the prediction functions from training on 
\begin_inset Formula $D^{1},\ldots,D^{B}$
\end_inset

, respectively.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
bagged prediction function
\series default
 is a 
\series bold
combination 
\series default
of these:
\begin_inset Formula 
\[
\hat{f}_{\text{avg}}(x)=\mbox{Combine}\left(\hat{f}_{1}(x),\hat{f}_{2}(x),\ldots,\hat{f}_{B}(x)\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How might we combine 
\end_layout

\begin_deeper
\begin_layout Itemize
prediction functions for regression?
\end_layout

\begin_layout Itemize
binary class predictions? 
\end_layout

\begin_layout Itemize
binary probability predictions?
\end_layout

\begin_layout Itemize
multiclass predictions? 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Bagging proposed by Leo Breiman (1996).
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bagging for Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Draw 
\begin_inset Formula $B$
\end_inset

 bootstrap samples 
\begin_inset Formula $D^{1},\ldots,D^{B}$
\end_inset

 from original data 
\begin_inset Formula $\cd$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\hat{f}_{1},\hat{f}_{2},\ldots,\hat{f}_{B}:\cx\to\reals$
\end_inset

 be the real-valued prediction functions from 
\begin_inset Formula $D^{1},\ldots,D^{B}$
\end_inset

, respectively.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Bagged prediction function is given as
\begin_inset Formula 
\[
\hat{f}_{\text{bag}}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}(x).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Empirically
\series default
, 
\begin_inset Formula $\hat{f}_{\text{bag}}$
\end_inset

 often performs similarly to what we'd get from training on 
\begin_inset Formula $B$
\end_inset

 independent samples:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\hat{f}_{\mbox{bag}}(x)$
\end_inset

 has same expectation as 
\begin_inset Formula $\hat{f}_{1}(x)$
\end_inset

, but
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{f}_{\mbox{bag}}(x)$
\end_inset

 has smaller variance than 
\begin_inset Formula $\hat{f}_{1}(x)$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
See HTF 8.7 for an example where bagging makes things worse.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bagging Classification Trees
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{5}$
\end_inset

 and output space 
\begin_inset Formula $\cy=\left\{ -1,1\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout ColumnsTopAligned

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.5
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align left
\begin_inset Graphics
	filename ../Figures/bagging/baggedTrees.png
	lyxscale 40
	width 100col%

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.4
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Sample size 
\begin_inset Formula $n=30$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Each bootstrap tree is quite different
\end_layout

\begin_layout Itemize
Different splitting variable at the root
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This high degree of variability from small perturbations of the training
 data is why tree methods are described as 
\series bold
high variance
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From HTF Figure 8.9}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Classification Combination Methods
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Two ways to combine classifications: consensus class or average probabilities.
\end_layout

\begin_layout Standard
\align left
\begin_inset Graphics
	filename ../Figures/bagging/baggingPerformance.png
	lyxscale 50
	height 62theight%

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From HTF Figure 8.10}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Terms 
\begin_inset Quotes eld
\end_inset

Bias
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Variance
\begin_inset Quotes erd
\end_inset

 in Casual Usage 
\begin_inset Newline newline
\end_inset

(Warning! Confusion Zone!) 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Restricting the hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 
\begin_inset Quotes eld
\end_inset


\series bold
biases
\series default

\begin_inset Quotes erd
\end_inset

 the fit 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
away
\series default
 from the best possible fit of the training data, and
\end_layout

\begin_layout Itemize

\series bold
towards
\series default
 a [usually] simpler model.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Full, unpruned decision trees have very little bias.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Pruning decision trees introduces a bias.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Variance
\series default
 refers to how much the fit changes across different random training sets.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Stability
\series default
 is another term referring to this concept (and I think should be preferred).
 
\end_layout

\begin_deeper
\begin_layout Itemize
Low variance = High stability
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
If different random training sets give very similar fits, then algorithm
 has high 
\series bold
stability
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Decision trees are found to be high variance (i.e.
 not very stable).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Conventional Wisdom on When Bagging Helps
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hope is that bagging reduces variance without making bias worse.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
General sentiment is that bagging helps most when
\end_layout

\begin_deeper
\begin_layout Itemize
Relatively unbiased base prediction functions
\end_layout

\begin_layout Itemize
High variance / low stability
\end_layout

\begin_deeper
\begin_layout Itemize
i.e.
 small changes in training set can cause large changes in predictions
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hard to find clear and convincing theoretical results on this
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But following this intuition leads to improved ML methods, e.g.
 Random Forests
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Random Forests
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Recall the Motivating Principal of Bagging 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Averaging 
\begin_inset Formula $\hat{f}_{1},\ldots,\hat{f}_{B}$
\end_inset

 reduces variance if they're based on i.i.d.
 samples from 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Bootstrap samples are 
\end_layout

\begin_deeper
\begin_layout Itemize
independent samples from the training set, but
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
are 
\series bold
not
\series default
 independent samples from 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This dependence limits the amount of variance reduction we can get.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Would be nice to reduce the dependence between 
\begin_inset Formula $\hat{f}_{i}$
\end_inset

's...
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This seems a bit confusing — since bootstrap samples are iid, then don't
 we have a variance reduction? Well...
 they're iid conditional on the training set, yes.
 So the conditional variance of the average is certainly reduced by averaging
 multiple bootstrap samples, compared to a single one.
 But we really want the unconditional variance to be reduced.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Random Forest
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block

\series bold
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\series bold
Main idea of random forests
\end_layout

\end_inset


\end_layout

\begin_layout Block
Use 
\series bold
bagged decision trees
\series default
, but modify the tree-growing procedure to reduce the dependence between
 trees.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Key step
\series default
 in random forests:
\end_layout

\begin_deeper
\begin_layout Itemize
When constructing 
\series bold
each tree node
\series default
, restrict choice of splitting variable to a randomly chosen subset of features
 of size 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Typically choose 
\begin_inset Formula $m\approx\sqrt{p}$
\end_inset

, where 
\begin_inset Formula $p$
\end_inset

 is the number of features.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can choose 
\begin_inset Formula $m$
\end_inset

 using cross validation.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Random Forest
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Usual approach is to build very deep trees (low bias)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Diversity in individual tree prediction functions comes from
\end_layout

\begin_deeper
\begin_layout Itemize
bootstrap samples (somewhat different training data) and
\end_layout

\begin_layout Itemize
randomized tree building
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Bagging seems to work better when we are combining a diverse set of prediction
 functions.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
