#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
%\setbeamercolor{frametitle}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=NYUPurple,urlcolor=LightPurple"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\renewcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\renewcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Part
Stochastic Gradient Descent 
\end_layout

\begin_layout Section
Gradient Descent
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Unconstrained Optimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Setting
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Objective function 
\begin_inset Formula $f:\reals^{d}\to\reals$
\end_inset

 is 
\emph on
differentiable.
\end_layout

\end_deeper
\begin_layout Block
Want to find 
\begin_inset Formula 
\[
x^{*}=\arg\min_{x\in\reals^{d}}f(x)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $f:\reals^{d}\to\reals$
\end_inset

 be differentiable at 
\begin_inset Formula $x_{0}\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
gradient
\series default
 of 
\begin_inset Formula $f$
\end_inset

  at the point 
\begin_inset Formula $x_{0}$
\end_inset

, denoted 
\begin_inset Formula $\del_{x}f(x_{0})$
\end_inset

, is the direction to move in for the 
\series bold
fastest increase
\series default
 in 
\begin_inset Formula $f(x)$
\end_inset

, when starting from 
\begin_inset Formula $x_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/GradientDescent/two-dim-gradient.png
	height 40theight%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure A.111 from Newtonian Dynamics, by Richard Fitzpatrick.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Initialize 
\begin_inset Formula $x=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x\gets x-\underbrace{\eta}_{\mbox{step size}}\del f(x)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
until stopping criterion satisfied
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent Path
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/GradientDescent/vlad-GD-fixedAndBacktracking.png
	lyxscale 40
	height 80theight%

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: Step Size
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A fixed step size will work, eventually, as long as it's small enough (roughly
 - details to come)
\end_layout

\begin_deeper
\begin_layout Itemize
Too fast, may diverge
\end_layout

\begin_layout Itemize
In practice, try several fixed step sizes
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Intuition on when to take big steps and when to take small steps?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example
\series bold
\emph on
: 
\series default
\emph default
Linear Least Squares Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Setup
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Output space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Itemize
Action space 
\begin_inset Formula $\cy=\reals$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Loss: 
\begin_inset Formula $\loss(\hat{y},y)=\left(y-\hat{y}\right)^{2}$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Hypothesis space:
\series default
 
\begin_inset Formula $\cf=\left\{ f:\reals^{d}\to\reals\mid f(x)=w^{T}x\,,\,w\in\reals^{d}\right\} $
\end_inset


\end_layout

\begin_layout Pause
'
\end_layout

\end_deeper
\begin_layout Itemize
Given data set 
\begin_inset Formula $\cd_{n}=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
Let's find the ERM 
\begin_inset Formula $\hat{f}\in\cf$
\end_inset

.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Example
\series bold
\emph on
: 
\series default
\emph default
Linear Least Squares Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Objective Function: Empirical Risk
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The function we want to minimize is the empirical risk:
\begin_inset Formula 
\[
\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2},
\]

\end_inset

where 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

 parameterizes the hypothesis space 
\begin_inset Formula $\cf$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent on 
\begin_inset Formula $\hat{R}_{n}(w)$
\end_inset

 will solve linear least squares regression.
\end_layout

\end_deeper
\begin_layout Section
Gradient Descent for Empirical Risk - Scaling Issues
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent for Empirical Risk and Averages
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have a hypothesis space of functions 
\begin_inset Formula $\cf=\left\{ f_{w}:\cx\to\ca\mid w\in\reals^{d}\right\} $
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
ERM is to find 
\begin_inset Formula $w$
\end_inset

 minimizing
\begin_inset Formula 
\[
\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\loss(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose 
\begin_inset Formula $\loss(f_{w}(x_{i}),y_{i})$
\end_inset

 is differentiable as a function of 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Then we can do gradient descent on 
\begin_inset Formula $\hat{R}_{n}(w)$
\end_inset

...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: How does it scale with 
\begin_inset Formula $n$
\end_inset

?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
At every iteration, we compute the gradient at current 
\begin_inset Formula $w$
\end_inset

:
\begin_inset Formula 
\[
\del\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We have to touch all 
\begin_inset Formula $n$
\end_inset

 training points to take a single step.
 [
\begin_inset Formula $O(n)$
\end_inset

]
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
A method that looks at all training points before each step is called a
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
batch optimization
\series default
 method.
 
\end_layout

\begin_layout Itemize
So far we've presented 
\series bold

\begin_inset Quotes eld
\end_inset

batch gradient descent
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Will this scale to 
\begin_inset Quotes eld
\end_inset

big data
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Can we make progress without looking at all the data?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Stochastic Gradient Descent
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Noisy
\begin_inset Quotes erd
\end_inset

 Gradient Descent 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We know gradient descent works.
\end_layout

\begin_layout Itemize
But the gradient may be slow to compute.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What if we just use an estimate of the gradient?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Turns out that can work fine.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Intuition
\series default
: 
\end_layout

\begin_deeper
\begin_layout Itemize
Gradient descent is an iterative procedure anyway.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
At every step, we have a chance to recover from previous missteps.
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
Turns out, even terrible estimates will work, so long as they are 
\series bold
unbiased
\series default
.
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Real goal is to minimize the risk (expected loss):
\begin_inset Formula 
\[
\argmin_{f\in\cf}\ex\left[\ell(f(X),Y)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For linear regression, that's
\begin_inset Formula 
\[
\argmin_{w}\ex\left(w^{T}X-Y\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent on this?
\begin_inset Formula 
\begin{align*}
\del_{w}\ex\left(w^{T}X-Y\right)^{2}= & \ex\left[2\left(w^{T}X-Y\right)X\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk [approximately]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Want to find gradient of the risk: 
\begin_inset Formula 
\[
\del R(w)=\ex\left[2\left(w^{T}X-Y\right)X\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can estimate expectation with a sample:
\begin_inset Formula 
\[
\widehat{\del R(w)}=\frac{1}{n}\sum_{i=1}^{n}\left[2\left(\underbrace{w^{T}x_{i}-y_{i}}_{\mbox{i'th residual}}\right)x_{i}\right]
\]

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's return to the general case...
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk: General Case
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient of Risk:
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Say hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 is parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Switching
\begin_inset Note Note
status open

\begin_layout Plain Layout
http://planetmath.org/differentiationundertheintegralsign
\end_layout

\end_inset

 
\begin_inset Formula $\del_{w}$
\end_inset

 and 
\begin_inset Formula $\ex$
\end_inset

 we can write the gradient of risk as
\begin_inset Formula 
\begin{align*}
\mbox{Gradient(Risk)}=\del_{w}\ex\left[\ell(f(X),Y)\right]= & \ex\left[\del_{w}\ell(f(X),Y)\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\series bold
\size large
\color blue
Unbiased 
\series default
\size default
\color inherit
estimator for Gradient(Risk):
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\left[\del_{w}\ell(f_{w}(x_{i}),y_{i})\right]\approx\underbrace{\ex\left[\del_{w}\ell(f(X),Y)\right]}_{\mbox{Gradient(Risk)}}
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Real goal is to minimize the risk (expected loss):
\begin_inset Formula 
\[
\argmin_{f\in\cf}\ex\left[\ell(f(X),Y)\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For linear regression, that's
\begin_inset Formula 
\[
\argmin_{w}\ex\left(w^{T}X-Y\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gradient descent on this?
\begin_inset Formula 
\begin{align*}
\del_{w}\ex\left(w^{T}X-Y\right)^{2}= & \ex\left[2\left(w^{T}X-Y\right)X\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk [approximately]
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Want to find gradient of the risk: 
\begin_inset Formula 
\[
\del R(w)=\ex\left[2\left(w^{T}X-Y\right)X\right]
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can estimate expectation with a sample:
\begin_inset Formula 
\[
\widehat{\del R(w)}=\frac{1}{n}\sum_{i=1}^{n}\left[2\left(\underbrace{w^{T}x_{i}-y_{i}}_{\mbox{i'th residual}}\right)x_{i}\right]
\]

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's return to the general case...
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent on the Risk: General Case
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient of Risk:
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Say hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 is parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Switching
\begin_inset Note Note
status open

\begin_layout Plain Layout
http://planetmath.org/differentiationundertheintegralsign
\end_layout

\end_inset

 
\begin_inset Formula $\del_{w}$
\end_inset

 and 
\begin_inset Formula $\ex$
\end_inset

 we can write the gradient of risk as
\begin_inset Formula 
\begin{align*}
\mbox{Gradient(Risk)}=\del_{w}\ex\left[\ell(f(X),Y)\right]= & \ex\left[\del_{w}\ell(f(X),Y)\right]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\series bold
\size large
\color blue
Unbiased 
\series default
\size default
\color inherit
estimator for Gradient(Risk):
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}\left[\del_{w}\ell(f_{w}(x_{i}),y_{i})\right]\approx\underbrace{\ex\left[\del_{w}\ell(f(X),Y)\right]}_{\mbox{Gradient(Risk)}}
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\end_inset

 
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\series bold
full gradient
\series default
 is
\begin_inset Formula 
\[
\del\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize
It's an average over the 
\series bold
full batch
\series default
 of data 
\begin_inset Formula $\cd_{n}=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's take a random subsample of size 
\begin_inset Formula $N$
\end_inset

 (called a 
\series bold
minibatch
\series default
):
\begin_inset Formula 
\[
(x_{m_{1}},y_{m_{1}}),\ldots,(x_{m_{N}},y_{m_{N}})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
minibatch gradient is
\begin_inset Formula 
\[
\del\hat{R}_{N}(w)=\frac{1}{N}\sum_{i=1}^{N}\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What can we say about the minibatch gradient? 
\begin_inset Formula $\pause$
\end_inset

It's random.
 What's its expectation?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
What's the expected value of the 
\series bold
minibatch gradient
\series default
?
\begin_inset Note Note
status open

\begin_layout Plain Layout
This only uses the fact that the gradients of the loss for each point have
 the same expectation as gradient for a randomly chosen point – in other
 words, sampling without replacement is fine.
 sampling one point randomly and reusing it 
\begin_inset Formula $N$
\end_inset

 times is also fine.
 ...
 Expectation is linear without qualifications.
\end_layout

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\pause\ex\left[\del\hat{R}_{N}(w)\right] & = & \frac{1}{N}\sum_{i=1}^{N}\ex\left[\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})\right]\\
\pause & = & \ex\left[\del_{w}\ell(f_{w}(x_{m_{1}}),y_{m_{1}})\right]\\
\pause & = & \sum_{i=1}^{n}\pr\left(m_{1}=i\right)\del_{w}\ell(f_{w}(x_{i}),y_{i})\\
\pause & = & \frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{i}),y_{i})\\
\pause & = & \del\hat{R}_{n}(w)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\emph on
Technical note:
\emph default
 We only assumed that each point in the minibatch is equally likely to be
 any of the 
\begin_inset Formula $n$
\end_inset

 points in the batch – no independence needed.
 So still true if we're sampling without replacement.
 Still true if we sample one point randomly and reuse it 
\begin_inset Formula $N$
\end_inset

 times.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient Properties
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Minibatch gradient is an 
\series bold
unbiased estimator
\series default
 for the [full] batch gradient: 
\begin_inset Formula 
\[
\ex\left[\del\hat{R}_{N}(w)\right]=\del\hat{R}_{n}(w)
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The bigger the minibatch, the better the estimate.
 
\begin_inset Note Note
status open

\begin_layout Pause

\end_layout

\begin_layout Itemize
In fact, by Strong Law of Large Numbers, 
\begin_inset Formula $\lim_{N\to\infty}\del\hat{R}_{N}(w)=\del\hat{R}_{n}(w)$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\pause\lim_{N\to\infty}\del\hat{R}_{N}(w) & = & \lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^{N}\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})\\
\pause & = & \ex\left[\del_{w}\ell(f_{w}(x_{m_{1}}),y_{m_{1}})\right]\\
 & = & \del\hat{R}_{n}(w)
\end{eqnarray*}

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient – In Practice
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Tradeoffs of minibatch size:
\end_layout

\begin_deeper
\begin_layout Itemize
Bigger 
\begin_inset Formula $N$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Better estimate of gradient, but slower (more data to touch)
\end_layout

\begin_layout Itemize
Smaller 
\begin_inset Formula $N$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Worse estimate of gradient, but can be quite fast 
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Even 
\begin_inset Formula $N=1$
\end_inset

 works, it's traditionally called 
\series bold
stochastic gradient descent
\series default
 (SGD).
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
These days, people use SGD to refer to minibatch SGD as well.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
If someone says 
\begin_inset Quotes eld
\end_inset

SGD
\begin_inset Quotes erd
\end_inset

, you ask – 
\begin_inset Quotes eld
\end_inset

What's your [mini]batch size?
\begin_inset Quotes erd
\end_inset

, to avoid ambiguity.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Terminology Review (Rough)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Gradient descent
\series default
 or 
\series bold

\begin_inset Quotes eld
\end_inset

full-batch
\begin_inset Quotes erd
\end_inset

 gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Use full data set of size 
\begin_inset Formula $n$
\end_inset

 to determine step direction
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Minibatch gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Use a random subset of size 
\begin_inset Formula $N$
\end_inset

 to determine step direction
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Yoshua Bengio says
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Yoshua Bengio's 
\begin_inset Quotes eld
\end_inset

Practical recommendations for gradient-based training of deep architectures
\begin_inset Quotes erd
\end_inset

 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://arxiv.org/abs/1206.5533
\end_layout

\end_inset

.
\end_layout

\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $N$
\end_inset

 is typically between 
\begin_inset Formula $1$
\end_inset

 and few hundred
\end_layout

\begin_layout Itemize
\begin_inset Formula $N=32$
\end_inset

 is a good default value
\end_layout

\begin_layout Itemize
With 
\begin_inset Formula $N\ge10$
\end_inset

 we get computational speedup (per datum touched)
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Stochastic gradient descent
\end_layout

\begin_deeper
\begin_layout Itemize
Minibatch with 
\begin_inset Formula $m=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Use a single randomly chosen point to determine step direction.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Standard
But these days terminology isn't used so consistently, so always clarify
 the [mini]batch size.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Minibatch Gradient Descent (minibatch size 
\begin_inset Formula $N$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
initialize 
\begin_inset Formula $w=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat
\end_layout

\begin_deeper
\begin_layout Itemize
randomly choose 
\begin_inset Formula $N$
\end_inset

 points 
\begin_inset Formula $\left\{ (x_{i},y_{i})\right\} _{i=1}^{N}\subset\cd_{n}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w\gets w-\eta\left[\frac{1}{N}\sum_{i=1}^{N}\del_{w}\ell(f_{w}(x_{i}),y_{i})\right]$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
until stopping criteria met
\end_layout

\end_inset

 
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Step Size: In practice 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For SGD, fixed step size can work well in practice.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\emph on
Typical approach: 
\emph default
 Fixed step size reduced by constant factor whenever validation performance
 stops improving.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
But no theorem for this giving performance guarantees (to my knowledge).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Robbins-Monro conditions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For convergence guarantee, use decreasing step sizes (dampens noise in step
 direction).
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\eta_{t}$
\end_inset

 be the step size at the 
\begin_inset Formula $t$
\end_inset

'th step.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Robbins-Monro Conditions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Many classical convergence results depend on the following two conditions:
\begin_inset Formula 
\[
\sum_{t=1}^{\infty}\eta_{t}^{2}<\infty\qquad\sum_{t=1}^{\infty}\eta_{t}=\infty
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
As fast as 
\begin_inset Formula $\eta_{t}=O\left(\frac{1}{t}\right)$
\end_inset

 would satisfy this...
 but should be faster than 
\begin_inset Formula $O\left(\frac{1}{\sqrt{t}}\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
A useful reference for practical techniques: Leon Bottou's 
\begin_inset Quotes eld
\end_inset

Tricks
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://research.microsoft.com/pubs/192769/tricks-2012.pdf
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Practical Comparison of GD vs SGD
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Practical Comparison of GD vs SGD
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For huge data, GD isn't practical.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
In a theoretical sense, GD is much faster than SGD...
 (i.e.
 better convergence rates)
\end_layout

\begin_deeper
\begin_layout Itemize
but most of that benefit happens once you're already pretty close to the
 solution
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
much faster to add an extra decimal place of accuracy on the minimum
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Does SGD Catch Up to GD?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Ridge regression objective function value for GD and SGD with various stepsizes
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ../Figures/optimization-rates/batch-vs-sgd-on-ridge-regression.png
	lyxscale 60
	height 60theight%

\end_inset


\end_layout

\begin_layout Itemize
Why doesn't SGD catch up to batch GD? 
\begin_inset Formula $\pause$
\end_inset

 It does, just takes a 
\series bold
very
\series default
 long time.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Is it worth the wait? 
\begin_inset Formula $\pause$
\end_inset

 As we discuss in next module, probably not...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SGD vs GD on Log Scale
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Plain Layout
\align center
\begin_inset Note Note
status open

\begin_layout Plain Layout
NOTE: Can we verify that iteration refers to something reasonable to make
 SGD and GD comparable?
\end_layout

\end_inset


\begin_inset Graphics
	filename ../Figures/optimization-rates/sgd-vs-gd-ridge-logistic.png
	height 65theight%

\end_inset

 
\end_layout

\begin_layout Itemize
Shows 
\begin_inset Quotes eld
\end_inset


\series bold
linear convergence
\begin_inset Quotes erd
\end_inset


\series default
 for 
\begin_inset Quotes eld
\end_inset

Full
\begin_inset Quotes erd
\end_inset

 GD; 
\series bold
sublinear
\series default
 for others.
 
\end_layout

\begin_layout Itemize
Note: logarithmic y-axis
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Figure from 
\backslash
url{http://www.stat.cmu.edu/~ryantibs/convexopt/lectures/25-fast-stochastic.pdf}}}
\end_layout

\end_inset


\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Many interesting examples involve a loss function Q(z, w) which is not different
iable on a subset of points with probability zero.
 Intuition suggests that this is a minor problems because the iterations
 of the online gradient descent have zero probability to reach one of these
 points.
 Even if we reach one of these points, we can just draw another example
 z.
\end_layout

\begin_layout Plain Layout
The analysis presented in this section addresses the convergence of the
 general online gradient algorithm (section 2.3) applied to the optimization
 of a differentiable cost function C(w) with the following properties: •
 The cost function C(w) has a single minimum w ∗ .
 • The cost function C(w) satisfies the following condition: ∀ε > 0, inf
 (w−w∗) 2>ε (w − w ∗ ) ∇wC(w) > 0 (4.1) Condition (4.1) simply states that
 the opposite of the gradient −∇wC(w) always points towards the minimum
 w ∗ .
 This particular formulation also rejects cost functions which have plateaus
 on which the gradient vanishes without making us closer to the minimum.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
[http://jmlr.csail.mit.edu/proceedings/papers/v5/sunehag09a/sunehag09a.pdf]
 References: Robbins and Monro (1951) proved a theorem that implies convergence
 for one-dimensional stochastic gradient descent; Blum (1954) generalized
 it to the multivariate case.
 Robbins and Siegmund (1971) achieved a stronger result of wider applicability
 in supermartingale theory.
 Here we extend the known convergence results (Bottou and LeCun, 2005) in
 two ways: a) We prove that updates that include scaling matrices with eigenvalu
es bounded by positive constants from above and below will converge almost
 surely; b) under slightly stronger assumptions we obtain a O
\end_layout

\end_inset


\end_layout

\end_body
\end_document
