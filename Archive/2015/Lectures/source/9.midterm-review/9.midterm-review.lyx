#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_math eulervm
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Title
Midterm Review
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Statistical Learning Theory Overview
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Typical Sequence of Events at Deployment Time
\end_layout

\end_inset


\end_layout

\begin_layout Frame
Many problem domains can be formalized as follows:
\end_layout

\begin_deeper
\begin_layout Enumerate
Observe input 
\begin_inset Formula $x$
\end_inset

 in input space 
\begin_inset Formula $\cx$
\end_inset

.
\end_layout

\begin_layout Enumerate
Take action 
\begin_inset Formula $a$
\end_inset

 in action space 
\begin_inset Formula $\ca$
\end_inset

.
\end_layout

\begin_layout Enumerate
Observe outcome 
\begin_inset Formula $y$
\end_inset

 in output space 
\begin_inset Formula $\cy$
\end_inset

.
\end_layout

\begin_layout Enumerate
Evaluate action in relation to the outcome: 
\begin_inset Formula $\ell(a,y)$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Formalization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
The Spaces
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout ColumnsTopAligned

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.3
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\cx$
\end_inset

: input space
\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.3
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\cy$
\end_inset

: output space 
\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.3
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\ca$
\end_inset

: action space
\end_layout

\end_deeper
\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Decision Function
\end_layout

\end_inset


\end_layout

\begin_layout Block
A 
\series bold
decision function 
\series default
produces an action 
\begin_inset Formula $a\in\ca$
\end_inset

 for any input 
\begin_inset Formula $x\in\cx$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\begin{matrix}f: & \cx & \rightarrow & \ca\\
 & x & \mapsto & f(x)
\end{matrix}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Loss Function
\end_layout

\end_inset


\end_layout

\begin_layout Block
A 
\series bold
loss function
\series default
 evaluates an action in the context of the output 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\begin{matrix}\loss: & \ca\times\cy & \rightarrow & \reals^{\ge0}\\
 & (a,y) & \mapsto & \loss(a,y)
\end{matrix}
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Not sure we want loss to be nonnegative -- what about log loss?
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Action Spaces
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\ca=\left\{ -1,1\right\} $
\end_inset

 [hard classification, as used in AdaBoost]
\end_layout

\begin_layout Itemize
\begin_inset Formula $\ca=\reals$
\end_inset

 [regression or soft classification]
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $\mbox{\ca=\left\{  \text{Probability distributions a space }\cy\right\}  }$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Setup for Statistical Learning Theory
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Data Generating Assumption
\end_layout

\end_inset


\end_layout

\begin_layout Block
All pairs 
\begin_inset Formula $(X,Y)\in\cx\times\cy$
\end_inset

 are drawn i.i.d.
 from some 
\series bold
unknown
\series default
 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
The 
\series bold
expected loss 
\series default
or 
\series bold

\begin_inset Quotes eld
\end_inset

risk
\begin_inset Quotes erd
\end_inset


\series default
\emph on
 
\emph default
of a decision function 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

 is
\begin_inset Formula 
\[
R(f)=\ex\loss(f(X),Y),
\]

\end_inset

where the expectation taken is over 
\begin_inset Formula $(X,Y)\sim P_{\cx\times\cy}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Definition
A 
\series bold
Bayes decision function
\series default
 
\begin_inset Formula $\minimizer f:\cx\to\ca$
\end_inset

 is a function that achieves the 
\emph on
minimal risk
\emph default
 (called the 
\series bold
Bayes risk) 
\series default
among all possible functions: 
\begin_inset Formula 
\[
R(\minimizer f)=\inf_{f}R(f).
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Empirical Risk Functional
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Can we estimate 
\begin_inset Formula $R(f)$
\end_inset

 without knowing 
\begin_inset Formula $\cp_{\cx\times\cy}$
\end_inset

?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Assume we have sample data
\end_layout

\end_inset


\end_layout

\begin_layout Block
Let 
\begin_inset Formula $\cd_{n}=\left\{ (X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\right\} $
\end_inset

 be drawn i.i.d.
 from 
\begin_inset Formula $\cp_{\cx\times\cy}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
empirical risk
\series default
\emph on
 
\emph default
of 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

 with respect to 
\begin_inset Formula $\cd_{n}$
\end_inset

 is
\begin_inset Formula 
\[
\hat{R}_{n}(f)=\frac{1}{n}\sum_{i=1}^{n}\loss(f(X_{i}),Y_{i}).
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
A function 
\begin_inset Formula $\hat{f}$
\end_inset

 is an 
\series bold
empirical risk minimizer
\series default
 if
\begin_inset Formula 
\[
\hat{R}_{n}(\hat{f})=\inf_{f}\hat{R}_{n}(f),
\]

\end_inset

where the minimum is taken over all functions.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $P_{\cx}=\mbox{Uniform}[0,1]$
\end_inset

, 
\begin_inset Formula $Y\equiv1$
\end_inset

 (i.e.
 
\begin_inset Formula $Y$
\end_inset

 is always 
\begin_inset Formula $1$
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename fitAlmostSurely0.pdf

\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Standard
Under square loss or 0/1 loss: Empirical Risk = 0.
 Risk = 1.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Standard
So unconstrained ERM doesn't work here.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Constrained Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 is a set of functions mapping 
\begin_inset Formula $\cx\to\ca$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Empirical risk minimizer 
\series default
(ERM) in 
\begin_inset Formula $\cf$
\end_inset

 is 
\begin_inset Formula $\hat{f}\in\cf$
\end_inset

, where 
\begin_inset Formula 
\begin{eqnarray*}
\emprisk(\hat{f}) & = & \inf_{f\in\cf}\emprisk(f)=\inf_{f\in\cf}\frac{1}{n}\sum_{i=1}^{n}\loss(f(X_{i}),Y_{i}).
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Risk minimizer 
\series default
in 
\begin_inset Formula $\cf$
\end_inset

 is 
\begin_inset Formula $\minimizer{f_{\cf}}\in\cf$
\end_inset

 , where 
\begin_inset Formula 
\[
R(\minimizer{f_{\cf}})=\inf_{f\in\cf}R(f)=\inf_{f\in\cf}\ex\loss(f(X),Y)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Error Decomposition
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block

\end_layout

\begin_deeper
\begin_layout ColumnsCenterAligned

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.4
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename alexApproxEstErrorPic.png
	width 100col%

\end_inset


\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.4
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
f^{*}= & \argmin_{f}\ex\ell(f(X),Y)\\
f_{\cf}= & \argmin_{f\in\cf}\ex\ell(f(X),Y))\\
\hat{f}_{n}= & \argmin_{f\in\cf}\frac{1}{n}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Approximation Error 
\series default
(of 
\begin_inset Formula $\cf$
\end_inset

)
\series bold
 
\begin_inset Formula $=\ R(f_{\cf})-R(\minimizer f)$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Estimation error
\series default
 (of 
\begin_inset Formula $\hat{f}_{n}$
\end_inset

 in 
\begin_inset Formula $\cf$
\end_inset

) 
\begin_inset Formula $=\ R(\hat{f}_{n})-R(f_{\cf})$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximation Error
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Approximation error is a property of the class 
\begin_inset Formula $\cf$
\end_inset


\end_layout

\begin_layout Itemize
It's our penalty for restricting to 
\begin_inset Formula $\cf$
\end_inset

 rather than considering all measurable functions
\end_layout

\begin_deeper
\begin_layout Itemize
Approximation error is the minimum risk possible with 
\begin_inset Formula $\cf$
\end_inset

 (even with infinite training data)
\end_layout

\end_deeper
\begin_layout Itemize

\emph on
Bigger
\emph default
 
\begin_inset Formula $\cf$
\end_inset

 mean 
\emph on
smaller 
\emph default
approximation error.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Estimation Error
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
Estimation error
\emph default
: The performance hit for choosing 
\begin_inset Formula $f$
\end_inset

 using finite training data
\end_layout

\begin_deeper
\begin_layout Itemize

\emph on
Equivalently
\emph default
: It's the hit for not knowing the true risk, but only the empirical risk.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\emph on
Smaller 
\emph default

\begin_inset Formula $\cf$
\end_inset

 means 
\emph on
smaller 
\emph default
estimation error.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\emph on
Under typical conditions: 
\begin_inset Quotes eld
\end_inset


\emph default
With infinite training data, estimation error goes to zero.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Infinite training data solves the 
\emph on
statistical 
\emph default
problem, which is not knowing the true risk.]
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Optimization Error
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Does unlimited data solve our problems?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
There's still the 
\emph on
algorithmic 
\emph default
problem of 
\emph on
finding
\emph default
 
\begin_inset Formula $\hat{f}_{n}\in\cf$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For nice choices of loss functions and classes 
\begin_inset Formula $\cf$
\end_inset

, the algorithmic problem can be solved (to any desired accuracy).
\end_layout

\begin_deeper
\begin_layout Itemize
Takes time! Is it worth it?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
For trees, can't optimize exactly.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Optimization error: 
\series default
If 
\begin_inset Formula $\tilde{f}_{n}$
\end_inset

 is the function our optimization method returns, and 
\begin_inset Formula $\hat{f}_{n}$
\end_inset

 is the empirical risk minimizer, then the optimization error is 
\begin_inset Formula $R(\tilde{f}_{n})-R(\hat{f}_{n})$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
NOTE: May have  
\begin_inset Formula $R(\tilde{f}_{n})<R(\hat{f}_{n})$
\end_inset

, since 
\begin_inset Formula $\hat{f}_{n}$
\end_inset

 may overfit more than 
\begin_inset Formula $\tilde{f}_{n}$
\end_inset

!
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Error Decomposition
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
excess risk
\series default
 of 
\begin_inset Formula $f$
\end_inset

 is the amount by which the risk of 
\begin_inset Formula $f$
\end_inset

 exceeds the Bayes risk.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mbox{\textbf{Excess Risk}}(\tilde{f}_{n})\, & =\,\risk(\tilde{f}_{n})-\risk(\minimizer f)\\
 & =\underbrace{\risk(\tilde{f}_{n})-R(\hat{f}_{n})}_{\text{optimization error}}+\underbrace{\risk(\hat{f}_{n})-\risk(\minimizer{f_{\cf}})}_{\text{estimation error}}+\underbrace{\risk(\minimizer{f_{\cf}})-\risk(\minimizer f)}_{\text{approximation error}}
\end{align*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Hypothesis Spaces and Complexity Control
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Complexity Measures for Decision Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Depth of a decision tree
\end_layout

\begin_layout Itemize
Degree of a polynomial 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How about for 
\series bold
linear
\series default
 models? 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\ell_{0}$
\end_inset

 complexity: number of non-zero coefficients 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{1}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

lasso
\begin_inset Quotes erd
\end_inset

 complexity: 
\begin_inset Formula $\sum_{i=1}^{d}\left|w_{i}\right|$
\end_inset

, for coefficients 
\begin_inset Formula $w_{1},\ldots,w_{d}$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{2}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

ridge
\begin_inset Quotes erd
\end_inset

 complexity: 
\begin_inset Formula $\sum_{i=1}^{d}w_{i}^{2}$
\end_inset

 for coefficients 
\begin_inset Formula $w_{1},\ldots,w_{d}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Nested Hypothesis Spaces from Complexity Measure
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis space: 
\begin_inset Formula $\cf$
\end_inset


\end_layout

\begin_layout Itemize
Complexity measure 
\begin_inset Formula $\Omega:\cf\to\reals^{\ge0}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Consider all functions in 
\begin_inset Formula $\cf$
\end_inset

 with
\emph on
 
\emph default
complexity 
\series bold
at most 
\begin_inset Formula $r$
\end_inset

:
\series default

\begin_inset Formula 
\[
\cf_{r}=\left\{ f\in\cf\mid\Omega(f)\le r\right\} 
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $\Omega$
\end_inset

 is a norm on 
\begin_inset Formula $\cf$
\end_inset

, this is a 
\series bold
ball of radius 
\begin_inset Formula $r$
\end_inset

 
\series default
in 
\begin_inset Formula $\cf$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Increasing complexities: 
\begin_inset Formula $r=0,1.2,2.6,5.4,\ldots$
\end_inset

 gives nested spaces:
\begin_inset Formula 
\[
\cf_{0}\subset\cf_{1.2}\subset\cf_{2.6}\subset\cf_{5.4}\subset\cdots\subset\cf
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Constrained Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Constrained ERM (Ivanov regularization)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
For complexity measure 
\begin_inset Formula $\Omega:\cf\to\reals^{\ge0}$
\end_inset

 and fixed 
\begin_inset Formula $r\ge0$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\min_{f\in\cf}\; & \sum_{i=1}^{n}\ell(f(x_{i}),y_{i})\\
\mbox{s.t.}\; & \Omega(f)\le r
\end{align*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Choose 
\begin_inset Formula $r$
\end_inset

 using validation data or cross-validation.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Each 
\begin_inset Formula $r$
\end_inset

 corresponds to a different hypothesis spaces.
 Could also write:
\begin_inset Formula 
\[
\min_{f\in\cf_{r}}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Penalized Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Penalized ERM (Tikhonov regularization)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
For complexity measure 
\begin_inset Formula $\Omega:\cf\to\reals^{\ge0}$
\end_inset

 and fixed 
\begin_inset Formula $\lambda\ge0$
\end_inset

,
\begin_inset Formula 
\begin{align*}
\min_{f\in\cf} & \sum_{i=1}^{n}\ell(f(x_{i}),y_{i})+\lambda\Omega(f)
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Choose 
\begin_inset Formula $\lambda$
\end_inset

 using validation data or cross-validation.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Ridge and Lasso Regression
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ridge Regression: Workhorse of Modern Data Science
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Ridge Regression (Tikhonov Form)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The ridge regression solution for regularization parameter 
\begin_inset Formula $\lambda\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{w\in\reals^{d}}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{2}^{2},
\]

\end_inset

where 
\begin_inset Formula $\|w\|_{2}^{2}=w_{1}^{2}+\cdots+w_{d}^{2}$
\end_inset

 is the square of the 
\begin_inset Formula $\ell_{2}$
\end_inset

-norm.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Ridge Regression (Ivanov Form)
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
The ridge regression solution for complexity parameter 
\begin_inset Formula $r\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{\|w\|_{2}^{2}\le r}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}.
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Lasso Regression: Workhorse (2) of Modern Data Science
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Lasso Regression (Tikhonov Form)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The lasso regression solution for regularization parameter 
\begin_inset Formula $\lambda\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{w\in\reals^{d}}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{1},
\]

\end_inset

where 
\begin_inset Formula $\|w\|_{1}=\left|w_{1}\right|+\cdots+\left|w_{d}\right|$
\end_inset

 is the 
\begin_inset Formula $\ell_{1}$
\end_inset

-norm.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Lasso Regression (Ivanov Form)
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
The lasso regression solution for complexity parameter 
\begin_inset Formula $r\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{\|w\|_{1}\le r}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}.
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Lasso Gives Feature Sparsity: So What?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Time/expense to compute/buy features
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Memory to store features (e.g.
 real-time deployment)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Identifies the important features
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Better prediction? sometimes
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
As a feature-selection step for training a slower non-linear model
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Loss Functions
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Loss Functions for Regression
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Regression losses usually only depend on the 
\series bold
residual:
\series default
 
\begin_inset Formula 
\[
r=y-\hat{y}
\]

\end_inset


\begin_inset Formula 
\[
\left(\hat{y},y\right)\mapsto\ell(r)=\ell(y-\hat{y})
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Losses for Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 7.6}}
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Square
\series default
 or 
\begin_inset Formula $\ell_{2}$
\end_inset

 Loss: 
\begin_inset Formula $\ell(r)=r^{2}$
\end_inset

 (not robust)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Absolute
\series default
 or 
\series bold
Laplace
\series default
 or 
\begin_inset Formula $\ell_{1}$
\end_inset

 Loss: 
\begin_inset Formula $\ell(r)=\left|r\right|$
\end_inset

 (not differentiable)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
gives 
\series bold
median regression
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Huber
\series default
 Loss: Quadratic for 
\begin_inset Formula $\left|r\right|\le\delta$
\end_inset

 and linear for 
\begin_inset Formula $\left|r\right|>\delta$
\end_inset

 (robust and differentiable)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig7.6a.pdf
	width 40text%

\end_inset


\begin_inset Graphics
	filename fig7.6b.pdf
	width 40text%

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Classification Problem: Real-Valued Predictions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Action space 
\begin_inset Formula $\ca=\reals\qquad$
\end_inset

Output space 
\begin_inset Formula $\cy=\left\{ -1,1\right\} $
\end_inset


\end_layout

\begin_layout Itemize
Prediction function 
\begin_inset Formula $f:\cx\to\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
The value 
\begin_inset Formula $f(x)$
\end_inset

 is called the 
\series bold
score
\series default
 for the input 
\begin_inset Formula $x$
\end_inset

.
 Generally, the magnitude of the score represents the 
\series bold
confidence of our prediction
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Definition
The 
\series bold
margin
\series default
 on an example 
\begin_inset Formula $(x,y)$
\end_inset

 is 
\begin_inset Formula $yf(x)$
\end_inset

.
 The margin is a measure of how 
\series bold
correct
\series default
 we are.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We want to 
\series bold
maximize the margin
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Most classification losses depend only on the margin.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Losses
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Logistic/Log loss: 
\begin_inset Formula $\loss_{\text{Logistic}}=\log\left(1+e^{-m}\right)$
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename loss.Zero_One.Hinge.Logistic.png
	lyxscale 30
	width 70text%

\end_inset


\end_layout

\begin_layout Standard
Logistic loss is differentiable.
 Never enough margin for logistic loss.
\begin_inset Newline newline
\end_inset

How many support vectors?
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Standard

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
(Soft Margin) Linear Support Vector Machine 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis space 
\begin_inset Formula $\cf=\left\{ f(x)=w^{T}x\mid w\in\reals^{d}\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Loss 
\begin_inset Formula $\ell(m)=\left(1-m\right)_{+}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization
\begin_inset Formula 
\[
\min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)_{+}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
unconstrained optimization
\end_layout

\begin_layout Itemize
not differentiable
\end_layout

\begin_layout Itemize
Can we reformulate into a differentiable problem? 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM as a Quadratic Program
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The SVM optimization problem is equivalent to
\begin_inset Formula 
\begin{eqnarray*}
\textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & \xi_{i}\ge0\;\mbox{for }i=1,\ldots,n\\
 &  & \xi_{i}\ge\left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)\;\mbox{for }i=1,\ldots,n
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Differentiable objective function 
\end_layout

\begin_layout Itemize
A quadratic program that can be solved by any off-the-shelf QP solver.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM Dual Problem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Can eliminate the 
\begin_inset Formula $\lambda$
\end_inset

 variables:
\begin_inset Formula 
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset

 Constraints are 
\series bold
box constraints
\series default
.
 (Simpler than primal constraints.) 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $\alpha^{*}$
\end_inset

 is a solution to the dual problem, then 
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Since 
\begin_inset Formula $\alpha_{i}\in[0,\frac{c}{n}]$
\end_inset

, we see that 
\begin_inset Formula $c$
\end_inset

 controls the amount of weight we can put on any single example 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Margin
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For notational convenience, define 
\begin_inset Formula $f^{*}(x)=x_{i}^{T}w^{*}+b^{*}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Margin 
\begin_inset Formula $yf^{*}(x)$
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename loss.Zero_One.Hinge.png
	lyxscale 30
	height 40theight%

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Incorrect classification: 
\begin_inset Formula $yf^{*}(x)\le0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Margin error: 
\begin_inset Formula $yf^{*}(x)<1$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

On the margin
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Formula $yf^{*}(x)=1$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Good side of the margin
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Formula $yf^{*}(x)>1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Complementary Slackness Results: Summary
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\alpha_{i}^{*}=0 & \implies & y_{i}f^{*}(x_{i})\ge1\\
\alpha_{i}^{*}\in\left(0,\frac{c}{n}\right) & \implies & y_{i}f^{*}(x_{i})=1\\
\alpha_{i}^{*}=\frac{c}{n} & \implies & y_{i}f^{*}(x_{i})\le1
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
y_{i}f^{*}(x_{i})<1 & \implies & \alpha_{i}^{*}=\frac{c}{n}\\
y_{i}f^{*}(x_{i})=1 & \implies & \alpha_{i}^{*}\in\left[0,\frac{c}{n}\right]\\
y_{i}f^{*}(x_{i})>1 & \implies & \alpha_{i}^{*}=0
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Kernelization
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Input Space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Our general learning theory setup: no assumptions about 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 for the specific methods we've developed: 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Ridge regression
\end_layout

\begin_layout Itemize
Lasso regression
\end_layout

\begin_layout Itemize
Linear SVM
\end_layout

\end_deeper
\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Extraction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
Mapping an input from 
\begin_inset Formula $\cx$
\end_inset

 to a vector in 
\begin_inset Formula $\reals^{d}$
\end_inset

 is called 
\series bold
feature extraction
\series default
 or 
\series bold
featurization
\series default
.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename feature-extraction.png
	lyxscale 60
	width 90text%

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename feature-extraction.png
	width 80text%

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 Quadratic feature map: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
High-Dimensional Features Good but Expensive
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To get 
\series bold
expressive
\series default
 hypothesis spaces using linear models, 
\end_layout

\begin_deeper
\begin_layout Itemize
need high-dimensional feature spaces
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
But more costly in terms of computation and memory.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Methods Can Be 
\begin_inset Quotes eld
\end_inset

Kernelized
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A method is 
\series bold
kernelized 
\series default
if inputs only appear inside inner products: 
\begin_inset Formula $\left\langle \phi(x),\phi(y)\right\rangle $
\end_inset

 for 
\begin_inset Formula $x,y\in\cx$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The function 
\begin_inset Formula 
\[
k(x,y)=\left\langle \phi(x),\phi(y)\right\rangle 
\]

\end_inset

is called the 
\series bold
kernel 
\series default
function.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Evaluation Can Be Fast
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Example
Quadratic feature map
\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset

has dimension 
\begin_inset Formula $O(d^{2})$
\end_inset

, but
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle =\left\langle w,x\right\rangle +\left\langle w,x\right\rangle ^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Naively explicit computation of 
\begin_inset Formula $k(w,x)$
\end_inset

: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Implicit computation of 
\begin_inset Formula $k(w,x)$
\end_inset

: 
\begin_inset Formula $O(d)$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Recap
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Given a kernelized ML algorithm.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Can swap out the inner product for a new kernel function.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
New kernel may correspond to a high dimensional feature space.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Computational cost is independent of feature dimension.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Enumerate
However, now has a quadratic dependence on the size of the data set.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Recall the ridge regression objective:
\begin_inset Formula 
\[
J(w)=||Xw-y||^{2}+\lambda||w||^{2}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Differentiating and setting equal to zero ,we get
\begin_inset Formula 
\begin{eqnarray*}
\left(X^{T}X+\lambda I\right)w & = & X^{T}y
\end{eqnarray*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelizing Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So we have, for 
\begin_inset Formula $\lambda>0$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
(X^{T}X+\lambda I)w & = & X^{T}y\\
\pause w & = & \frac{1}{\lambda}X^{T}(y-Xw)\\
\pause w & = & X^{T}\alpha
\end{eqnarray*}

\end_inset

 for 
\begin_inset Formula $\alpha=\lambda^{-1}(y-Xw)\in\reals^{n}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So 
\begin_inset Formula $w$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset


\series bold
in the span of the data
\series default

\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{align*}
w= & \begin{pmatrix}| & \cdots & |\\
x_{1} & \cdots & x_{n}\\
| & \cdots & |
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}=\alpha_{1}x_{1}+\cdots\alpha_{n}x_{n}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelizing Ridge Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So plugging in 
\begin_inset Formula $w=X^{T}\alpha$
\end_inset

 to
\begin_inset Formula 
\begin{eqnarray*}
\alpha & = & \lambda^{-1}(y-Xw)\\
\pause\lambda\alpha & = & y-XX^{T}\alpha\\
\pause XX^{T}\alpha+\lambda\alpha & = & y\\
\pause\left(XX^{T}+\lambda I\right)\alpha & = & y\\
\pause\alpha & = & (\lambda I+XX^{T})^{-1}y
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When can we swap in a new kernel matrix for 
\begin_inset Formula $XX^{T}$
\end_inset

?
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mercer's Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
A symmetric function 
\begin_inset Formula $k(w,x)$
\end_inset

 can be expressed an inner product
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle 
\]

\end_inset

for some 
\begin_inset Formula $\phi$
\end_inset

 if and only if 
\begin_inset Formula $k(w,x)$
\end_inset

 is 
\series bold
positive semidefinite.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If we start with a psd kernel, can we generate more?
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix (or the Gram Matrix)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
For a set of 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} $
\end_inset

 and an inner product 
\begin_inset Formula $\left\langle \cdot,\cdot\right\rangle $
\end_inset

on the set, the 
\series bold
kernel matrix
\series default
 or the 
\series bold
Gram matrix
\series default
 is defined as
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle x_{i},x_{j}\right\rangle \end{pmatrix}_{i,j}=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
Then for the standard Euclidean inner product 
\begin_inset Formula $\left\langle x_{i},x_{j}\right\rangle =x_{i}^{T}x_{j}$
\end_inset

, we have
\begin_inset Formula 
\[
K=XX^{T}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Trees
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Trees vs Linear Models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Trees have to work much harder to capture linear relations.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename treeVsLinear.pdf
	lyxscale 30
	height 60theight%

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From 
\backslash
emph{An Introduction to Statistical Learning, with applications in R} (Springer,
 2013) with permission from the authors: G.
 James, D.
 Witten,  T.
 Hastie and R.
 Tibshirani.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comments about Trees
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Trees make no use of 
\series bold
geometry
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
No inner products or distances
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
called a 
\begin_inset Quotes eld
\end_inset

nonmetric
\begin_inset Quotes erd
\end_inset

 method
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Feature scale irrelevant
\series default
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Predictions are not continuous
\end_layout

\begin_deeper
\begin_layout Itemize
not so bad for classification
\end_layout

\begin_layout Itemize
may not be desirable for regression
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Ensemble Methods
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ensembles: Parallel vs Sequential
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Ensemble methods combine multiple models 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Parallel ensembles
\series default
: each model is built independently
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
e.g.
 bagging and random forests
\end_layout

\begin_layout Itemize
Main Idea: Combine many (high complexity, low bias) models to reduce variance
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Sequential ensembles
\series default
: 
\end_layout

\begin_deeper
\begin_layout Itemize
Models are generated sequentially
\end_layout

\begin_layout Itemize
Try to add new models that do well where previous models lack
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Averaging Independent Prediction Functions 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $Z_{1},\ldots,Z_{n}$
\end_inset

 be independent r.v's with mean 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Average has the same expected value but smaller variance:
\begin_inset Formula 
\[
\ex\left[\frac{1}{n}\sum_{i=1}^{n}Z_{i}\right]=\mu\qquad\var\left[\frac{1}{n}\sum_{i=1}^{n}Z_{i}\right]=\frac{\sigma^{2}}{n}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Prediction functions? Suppose we have 
\begin_inset Formula $B$
\end_inset

 independent training sets.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\hat{f}_{1}(x),\hat{f}_{2}(x),\ldots,\hat{f}_{B}(x)$
\end_inset

 be the prediction models for each set.
\end_layout

\begin_layout Itemize
Define the average prediction function as:
\begin_inset Formula 
\[
\hat{f}_{\text{avg}}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}(x).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Variance of average?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In practice we don't have 
\begin_inset Formula $B$
\end_inset

 independent training sets...
\end_layout

\begin_deeper
\begin_layout Itemize
Instead, we can use 
\series bold
the bootstrap
\series default
....
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Bootstrap Sample
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A 
\series bold
bootstrap sample
\series default
 from 
\begin_inset Formula $\cd=\left\{ X_{1},\ldots,X_{n}\right\} $
\end_inset

 is a sample of size 
\begin_inset Formula $n$
\end_inset

 drawn 
\emph on
with replacement
\emph default
 from 
\begin_inset Formula $\cd$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
In a bootstrap sample, some elements of 
\begin_inset Formula $\cd$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
will show up multiple times, 
\end_layout

\begin_layout Itemize
some won't show up at all.
 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So we expect ~63.2% of elements of 
\begin_inset Formula $\cd$
\end_inset

 will show up at least once.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bagging
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we had 
\begin_inset Formula $B$
\end_inset

 bootstrap samples from a training set.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Bagging estimator
\series default
 given as
\begin_inset Formula 
\[
\hat{f}_{\text{bag}}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}^{*}(x),
\]

\end_inset

 where 
\begin_inset Formula $\hat{f}_{b}^{*}$
\end_inset

 is trained on the 
\begin_inset Formula $b$
\end_inset

'th bootstrap sample.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Random Forest
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block

\series bold
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\series bold
Main idea of random forests
\end_layout

\end_inset


\end_layout

\begin_layout Block
Use 
\series bold
bagged decision trees
\series default
, but modify the tree-growing procedure to reduce the correlation between
 trees.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Key step
\series default
 in random forests:
\end_layout

\begin_deeper
\begin_layout Itemize
When constructing each tree node, restrict choice of splitting variable
 to a randomly chosen subset of features of size 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Typically choose 
\begin_inset Formula $m\approx\sqrt{p}$
\end_inset

, where 
\begin_inset Formula $p$
\end_inset

 is the number of features.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can choose 
\begin_inset Formula $m$
\end_inset

 using cross validation.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
AdaBoost - Rough Sketch
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Start with equal weight on all training points 
\begin_inset Formula $w_{1}=\cdots=w_{n}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Repeat for 
\begin_inset Formula $m=1,\ldots,M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Itemize
Fit weak classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 to weighted training points
\end_layout

\begin_layout Itemize
Increase weight on points 
\begin_inset Formula $G_{m}(x)$
\end_inset

 misclassifies
\end_layout

\end_deeper
\begin_layout Itemize
Final prediction 
\begin_inset Formula $G(x)=\sign\left[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\begin_inset Formula $\alpha_{m}$
\end_inset

's are nonnegative,
\end_layout

\begin_deeper
\begin_layout Itemize
larger when 
\begin_inset Formula $G_{m}$
\end_inset

 fits its weighted 
\begin_inset Formula $\cd$
\end_inset

 well
\end_layout

\begin_layout Itemize
smaller when 
\begin_inset Formula $G_{m}$
\end_inset

 fits weighted 
\begin_inset Formula $\cd$
\end_inset

 less well
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Important: Each 
\begin_inset Formula $G_{m}(x)$
\end_inset

 produces a class 
\begin_inset Formula $\left\{ -1,1\right\} $
\end_inset

, not a score.
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptive Basis Function Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis space 
\begin_inset Formula $\cf$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Can be classifiers or regression functions
\end_layout

\begin_layout Itemize
These would be the 
\begin_inset Quotes eld
\end_inset

weak classifiers
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

base classifiers
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
An 
\series bold
adaptive basis function expansion 
\series default
over 
\begin_inset Formula $\cf$
\end_inset

 is
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}\nu_{m}h_{m}(x),
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Each 
\begin_inset Formula $h_{m}\in\cf$
\end_inset

 is chosen in a learning process, and
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nu_{m}$
\end_inset

 are 
\series bold
expansion coefficients.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
For example, 
\begin_inset Formula $\cf$
\end_inset

 could be all decision trees of depth at most 4.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We now discuss one approach to fitting such a model.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Forward Stagewise Additive Modeling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Initialize 
\begin_inset Formula $f_{0}(x)=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $m=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Enumerate
Compute:
\begin_inset Formula 
\[
\left(\nu_{m},h_{m}\right)=\argmin_{\nu\in\reals,h\in\cf}\sum_{i=1}^{n}\ell\left\{ y_{i},f_{m-1}(x_{i})\underbrace{+\nu h(x_{i})}_{\text{new piece}}\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Enumerate
Set 
\begin_inset Formula $f_{m}(x)=f_{m-1}(x)+\nu_{m}h(x)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Return: 
\begin_inset Formula $f_{M}(x)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Exponential Loss and AdaBoost
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Take loss function to be 
\begin_inset Formula 
\[
\ell(y,f(x))=\exp\left(-yf(x)\right).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\cf=\left\{ h(x):\cx\to\left\{ -1,1\right\} \right\} $
\end_inset

 be a hypothesis space of weak classifiers.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then Forward Stagewise Additive Modeling (FSAM) reduces to AdaBoost.
\end_layout

\begin_deeper
\begin_layout Itemize
(See HTF Section 10.4 for proof.) 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Neural Networks
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename neural-network-percy.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
Score is just
\begin_inset Formula 
\begin{eqnarray*}
\text{score} & = & w_{1}h_{1}+w_{2}h_{2}\\
\pause & = & w_{1}\sigma(v_{1}^{T}\phi(x))+w_{2}\sigma\left(v_{2}^{T}\phi(x)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
This is the basic recipe.
\end_layout

\begin_deeper
\begin_layout Itemize
We can add more hidden nodes.
\end_layout

\begin_layout Itemize
We can add more hidden layers.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: Hidden Nodes as Learned Features
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename neural-network-percy.png
	lyxscale 40
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
Can interpret 
\begin_inset Formula $h_{1}$
\end_inset

 and 
\begin_inset Formula $h_{2}$
\end_inset

 as nonlinear features learned from data.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn
 2014.
 }}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: The Hypothesis Space
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
What hyperparameters describe a neural network?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Number of layers
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Number of nodes in each hidden layer
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Activation function (but so many to choose from)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Example neural network hypothesis space:
\begin_inset Formula 
\[
\cf=\left\{ f:\reals^{d}\to\reals\mid f\text{ is a NN with 2 hidden layers, 500 nodes in each}\right\} 
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Functions in 
\begin_inset Formula $\cf$
\end_inset

 
\series bold
parameterized by the weights between nodes
\series default
.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: Loss Functions and Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Neural networks give a 
\series bold
new hypothesis space
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
But we can use all the 
\series bold
same loss functions
\series default
 we've used before.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Optimization method of choice: 
\series bold
stochastic gradient descent
\series default
.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Network: Objective Function
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
In our simple network, the output score is given by 
\begin_inset Formula 
\begin{eqnarray*}
f(x) & = & w_{1}\sigma(v_{1}^{T}\phi(x))+w_{2}\sigma\left(v_{2}^{T}\phi(x)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective with square loss is then
\begin_inset Formula 
\[
J(w,v)=\sum_{i=1}^{n}\left(y_{i}-f_{w,v}(x_{i})\right)^{2}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note: 
\begin_inset Formula $J(w,v)$
\end_inset

 is 
\series bold
not convex
\series default
.
\end_layout

\begin_deeper
\begin_layout Itemize
makes optimization much more difficult
\end_layout

\begin_layout Itemize
accounts for many of the 
\begin_inset Quotes eld
\end_inset

tricks of the trade
\begin_inset Quotes erd
\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Learning with Back-Propagation
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Back-propagation is an 
\series bold
algorithm
\series default
 for computing the SGD gradient
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Mathematically, it's not necessary.
\end_layout

\begin_layout Itemize
With lots of chain rule, you can work out the gradient by hand.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Back-propagation is 
\end_layout

\begin_deeper
\begin_layout Itemize
a clean way to organize the computation of the gradient
\end_layout

\begin_layout Itemize
an efficient way to compute the gradient 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Predicting Distributions
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Likelihood of a Predicted Distribution
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have
\begin_inset Formula 
\[
\cd=\left\{ y_{1},\ldots,y_{n}\right\} \text{ sampled i.i.d. from }p(y).
\]

\end_inset


\end_layout

\begin_layout Itemize
Then the 
\series bold
likelihood 
\series default
of 
\begin_inset Formula $\hat{p}$
\end_inset

 for the data 
\begin_inset Formula $\cd$
\end_inset

 is defined to be
\begin_inset Formula 
\[
\hat{p}(\cd)=\prod_{i=1}^{n}\hat{p}(y_{i}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We'll write this as 
\begin_inset Formula 
\[
L_{\cd}(\hat{p}):=\hat{p}(\cd)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Special case: If 
\begin_inset Formula $\hat{p}$
\end_inset

 is a probability mass function, then
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $L_{\cd}(\hat{p})$
\end_inset

 is the probability of 
\begin_inset Formula $\cd$
\end_inset

 under 
\begin_inset Formula $\hat{p}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probability Estimation as Statistical Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Output space 
\begin_inset Formula $\cy$
\end_inset

 (containing observations from distribution 
\begin_inset Formula $P$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Action space
\series default
 
\begin_inset Formula $\ca=\left\{ p(y)\mid p\text{ is a probability density or mass function on }\cy\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
How to encode our objective of 
\begin_inset Quotes eld
\end_inset

high likelihood
\begin_inset Quotes erd
\end_inset

 as a loss function?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Define loss function as the negative log-likelihood of 
\begin_inset Formula $y$
\end_inset

 under 
\begin_inset Formula $p(\cdot)$
\end_inset

: 
\begin_inset Formula 
\[
\begin{matrix}\loss: & \ca\times\cy & \rightarrow & \reals\\
 & (p,y) & \mapsto & -\log p(y)
\end{matrix}
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Regression as Statistical Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx$
\end_inset


\end_layout

\begin_layout Itemize
Output space 
\begin_inset Formula $\cy$
\end_inset


\end_layout

\begin_layout Itemize
All pairs 
\begin_inset Formula $(X,Y)$
\end_inset

 are independent with distribution 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Action space
\series default
 
\begin_inset Formula $\ca=\left\{ p(y)\mid p\text{ is a probability density or mass function on }\cy\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis spaces comprise decision functions 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Given an 
\begin_inset Formula $x\in\cx$
\end_inset

, predict a probability distribution 
\begin_inset Formula $p(y)$
\end_inset

 on 
\begin_inset Formula $\cy$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Loss function as before: 
\begin_inset Formula 
\[
\begin{matrix}\loss: & \ca\times\cy & \rightarrow & \reals\\
 & (p,y) & \mapsto & -\log p(y)
\end{matrix}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
ERM gives MLE.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Regression as Statistical Learning
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
The risk of decision function 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

 
\begin_inset Formula 
\[
R(f)=-\ex_{X,Y}\log\left[f(X)\right](Y),
\]

\end_inset

where 
\begin_inset Formula $f(X)$
\end_inset

 is a PDF or PMF on 
\begin_inset Formula $\cy$
\end_inset

, and we're evaluating it on 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The empirical risk of 
\begin_inset Formula $f$
\end_inset

 for a sample 
\begin_inset Formula $\cd=\left\{ y_{1},\ldots,y_{n}\right\} \in\cy$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{R}(f)=-\sum_{i=1}^{n}\log\left[f(x_{i})\right](y_{i}).
\]

\end_inset

This is called the negative 
\series bold
conditional log-likelihood
\series default
.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Probabilistic Classifiers
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Setting: 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

, 
\begin_inset Formula $\cy=\left\{ 0,1\right\} $
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For each 
\begin_inset Formula $X=x$
\end_inset

, 
\begin_inset Formula $p(Y=1\mid x)=\theta$
\end_inset

.
 (i.e.
 
\begin_inset Formula $Y$
\end_inset

 has a Bernoulli
\begin_inset Formula $(\theta)$
\end_inset

 distribution)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\theta$
\end_inset

 may vary with 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Itemize
For each 
\begin_inset Formula $x\in\reals^{d}$
\end_inset

, just want to predict 
\begin_inset Formula $\theta\in[0,1]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Two steps:
\begin_inset Formula 
\[
\underbrace{x}_{\in\reals^{D}}\mapsto\underbrace{w^{T}x}_{\in\reals}\mapsto\underbrace{f(w^{T}x)}_{\in[0,1]},
\]

\end_inset

where 
\begin_inset Formula $f:\reals\to[0,1]$
\end_inset

 is called the 
\series bold
transfer 
\series default
or 
\series bold
inverse link 
\series default
function.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Probability model is then
\begin_inset Formula 
\[
p(Y=1\mid x)=f(w^{T}x)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Inverse Link Functions
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Two commonly used 
\begin_inset Quotes eld
\end_inset

inverse link
\begin_inset Quotes erd
\end_inset

 functions to map from 
\begin_inset Formula $w^{T}x$
\end_inset

 to 
\begin_inset Formula $\theta$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename bernoulliInverseLinkFunctions.pdf
	width 60text%

\end_inset


\end_layout

\begin_layout Itemize
Logistic function 
\begin_inset Formula $\implies$
\end_inset

 Logistic Regression
\end_layout

\begin_layout Itemize
Normal CDF 
\begin_inset Formula $\implies$
\end_inset

 Probit Regression
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Generalized Linear Models
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Specifying a Natural Exponential Family
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
The family is a 
\series bold
natural exponential family
\series default
 with parameter 
\begin_inset Formula $\theta$
\end_inset

 if
\begin_inset Formula 
\[
p_{\theta}(y)=\frac{1}{Z(\theta)}h(y)\exp\left[\theta^{T}y\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
To specify a natural exponential family, we need to choose 
\begin_inset Formula $h(y)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Everything else is determined.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Implicit in choosing 
\begin_inset Formula $h(y)$
\end_inset

 is the choice of the support of the distribution.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Natural Exponential Families: Examples
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Standard
The following are univariate natural exponential families:
\end_layout

\begin_layout Enumerate
Normal distribution with known variance.
\end_layout

\begin_layout Enumerate
Poisson distribution
\end_layout

\begin_layout Enumerate
Gamma distribution (with known 
\begin_inset Formula $k$
\end_inset

 parameter)
\end_layout

\begin_layout Enumerate
Bernoulli distribution (and Binomial with known number of trials)
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Enumerate
Negative binomial distribution 
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Linear Models [with Canonical Link]
\end_layout

\end_inset

 
\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
In GLMs, we first choose a natural exponential family.
\end_layout

\begin_deeper
\begin_layout Itemize
(This amounts to choosing 
\begin_inset Formula $h(y)$
\end_inset

.)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The idea is to plug in 
\begin_inset Formula $w^{T}x$
\end_inset

 for the natural parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This gives models of the following form:
\begin_inset Formula 
\[
p_{\theta}(y\mid x)=\frac{1}{Z(w^{T}x)}h(y)\exp\left[(w^{T}x)y\right].
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
This is the form we had for Poisson regression.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Generalized Linear Models [with General Link]
\end_layout

\end_inset

 
\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
More generally, choose a function 
\begin_inset Formula $\psi$
\end_inset

 so that
\begin_inset Formula 
\[
x\mapsto w^{T}x\mapsto\psi(w^{T}x),
\]

\end_inset

where 
\begin_inset Formula $\theta=\psi(w^{T}x)$
\end_inset

 is the natural parameter for the family.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So our final prediction (for one-parameter families) is:
\begin_inset Formula 
\[
p_{\theta}(y\mid x)=\frac{1}{Z(\psi(w^{T}x))}h(y)\exp\left[\psi(w^{T}x)y\right].
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Optimization
\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Initialize 
\begin_inset Formula $x=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x\gets x-\underbrace{\eta}_{\mbox{step size}}\del f(x)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
until stopping criterion satisfied
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: Does it scale?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
At every iteration, we compute the gradient at current 
\begin_inset Formula $w$
\end_inset

:
\begin_inset Formula 
\[
\del_{w}\hat{R}_{n}(w)=\frac{2}{n}\sum_{i=1}^{n}\underbrace{\left(w^{T}x_{i}-y_{i}\right)}_{i\mbox{th residual}}x_{i}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We have to touch all 
\begin_inset Formula $n$
\end_inset

 training points to take a single step.
 [
\begin_inset Formula $O(n)$
\end_inset

]
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Called a 
\series bold
batch optimization
\series default
 method
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Can we make progress without looking at all the data?
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic Gradient Descent (SGD)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Stochastic Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
initialize 
\begin_inset Formula $w=0$
\end_inset


\end_layout

\begin_layout Itemize
repeat
\end_layout

\begin_deeper
\begin_layout Itemize
randomly choose training point 
\begin_inset Formula $(x_{i},y_{i})\in\cd_{n}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w\gets w-\eta\underbrace{\del_{w}\ell(f_{w}(x_{i}),y_{i})}_{\mbox{Grad(Loss on i'th example)}}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
until stopping criteria met
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to find the Lasso solution?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
How to solve the Lasso?
\begin_inset Formula 
\[
\min_{w\in\reals^{d}}\sum_{i=1}^{n}\left(w^{T}x_{i}-y_{i}\right)^{2}+\lambda\left|w\right|_{1}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\left|w\right|_{1}$
\end_inset

 is not differentiable!
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Lasso as a Quadratic Program
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Substituting 
\begin_inset Formula $w=w^{+}-w^{-}$
\end_inset

 and 
\begin_inset Formula $\left|w\right|=w^{+}+w^{-}$
\end_inset

, Lasso problem is:
\begin_inset Formula 
\begin{align*}
\min_{w^{+},w^{-}\in\reals^{d}} & \sum_{i=1}^{n}\left(\left(w^{+}-w^{-}\right)^{T}x_{i}-y_{i}\right)^{2}+\lambda\left(w^{+}+w^{-}\right)\\
\mbox{subject to } & w_{i}^{+}\ge0\mbox{ for all }i\\
 & w_{i}^{-}\ge0\mbox{ for all }i
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective is 
\series bold
differentiable
\series default
 (in fact, 
\series bold
convex and quadratic
\series default
)
\end_layout

\begin_layout Itemize
\begin_inset Formula $2d$
\end_inset

 variables vs 
\begin_inset Formula $d$
\end_inset

 variables
\end_layout

\begin_layout Itemize
\begin_inset Formula $2d$
\end_inset

 constraints vs no constraints
\end_layout

\begin_layout Itemize
A 
\series bold

\begin_inset Quotes eld
\end_inset

quadratic program
\series default

\begin_inset Quotes erd
\end_inset

: a convex quadratic objective with linear constraints.
\end_layout

\begin_deeper
\begin_layout Itemize
Could plug this into a generic QP solver.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Projected SGD
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\min_{w^{+},w^{-}\in\reals^{d}} & \sum_{i=1}^{n}\left(\left(w^{+}-w^{-}\right)^{T}x_{i}-y_{i}\right)^{2}+\lambda\left(w^{+}+w^{-}\right)\\
\mbox{subject to } & w_{i}^{+}\ge0\mbox{ for all }i\\
 & w_{i}^{-}\ge0\mbox{ for all }i
\end{align*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Solution:
\end_layout

\begin_deeper
\begin_layout Itemize
Take a stochastic gradient step
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Project
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $w^{+}$
\end_inset

 and 
\begin_inset Formula $w^{-}$
\end_inset

 into the constraint set
\end_layout

\begin_deeper
\begin_layout Itemize
In other words, any component of 
\begin_inset Formula $w^{+}$
\end_inset

 or 
\begin_inset Formula $w^{-}$
\end_inset

 is negative, make it 0 .
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
Note: Sparsity pattern may change frequently as we iterate
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Coordinate Descent Method
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Coordinate Descent Method
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Goal: 
\series default
Minimize 
\begin_inset Formula $L(w)=L(w_{1},\ldots w_{d})$
\end_inset

 over 
\begin_inset Formula $w=\left(w_{1},\ldots,w_{d}\right)\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Initialize
\series default
 
\begin_inset Formula $w^{(0)}=0$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
while
\series default
 not converged:
\end_layout

\begin_deeper
\begin_layout Itemize
Choose a coordinate 
\begin_inset Formula $j\in\left\{ 1,\ldots,d\right\} $
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w_{j}^{\mbox{new}}\gets\argmin_{w_{j}}L(w_{1}^{(t)},\ldots,w_{j-1}^{(t)},\mathbf{w_{j}},w_{j+1}^{(t)},\ldots,w_{d}^{(t)})$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w^{(t+1)}\gets w^{(t)}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $w_{j}^{(t+1)}\gets w_{j}^{\mbox{new}}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $t\gets t+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For when it's easier to minimize w.r.t.
 one coordinate at a time
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Random coordinate choice 
\begin_inset Formula $\implies$
\end_inset


\series bold
stochastic coordinate descent
\end_layout

\begin_layout Itemize
Cyclic coordinate choice 
\begin_inset Formula $\implies$
\end_inset

 
\series bold
cyclic coordinate descent
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Coordinate Descent Method for Lasso
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Why mention coordinate descent for Lasso?
\end_layout

\begin_layout Itemize
In Lasso, the coordinate minimization has a 
\series bold
closed form solution
\series default
!
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Subsection
Lagrangian Methods
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Lagrangian
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Recall the general optimization problem:
\begin_inset Formula 
\begin{eqnarray*}
\textrm{minimize} &  & f_{0}(x)\\
\textrm{subject to} &  & f_{i}(x)\le0,\;\;i=1,\ldots,m\\
 &  & h_{i}(x)=0,\;\;i=1,\ldots p,
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Definition

\end_layout

\begin_layout Definition
The 
\series bold
Lagrangian
\series default
 for the general optimization problem is
\begin_inset Formula 
\[
L(x,\lambda,\nu)=f_{0}(x)+\sum_{I=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i}(x),
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\lambda_{i}$
\end_inset

's and 
\begin_inset Formula $\nu$
\end_inset

's are called 
\series bold
Lagrange multipliers
\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\nu$
\end_inset

 also called the 
\series bold
dual variables
\series default
 .
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Primal and the Dual
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Original optimization problem in  
\series bold
primal form:
\series default

\begin_inset Formula 
\[
p^{*}=\inf_{x}\sup_{\lambda\succeq0,\nu}L(x,\lambda,\nu)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
Lagrangian dual problem
\series default
:
\begin_inset Formula 
\[
d^{*}=\sup_{\lambda\succeq0,\nu}\inf_{x}L(x,\lambda,\nu)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We showed 
\series bold
weak duality
\series default
: 
\begin_inset Formula $p^{*}\ge d^{*}$
\end_inset

 for any optimization problem
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Convex and Concave Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A function 
\begin_inset Formula $f:\reals^{n}\to\reals$
\end_inset

 is 
\series bold
convex
\series default
 if 
\begin_inset Formula $\dom f$
\end_inset

 is a convex set and if for all 
\begin_inset Formula $x,y\in\dom f$
\end_inset

, and 
\begin_inset Formula $0\le\theta\le1$
\end_inset

, we have
\begin_inset Formula 
\[
f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y).
\]

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig7.5a.pdf
	width 40text%

\end_inset


\begin_inset Graphics
	filename fig7.5b.pdf
	width 40text%

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Fig.
 7.5}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Convex Optimization Problem: Standard Form
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Convex Optimization Problem: Standard Form
\end_layout

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\textrm{minimize} &  & f_{0}(x)\\
\textrm{subject to} &  & f_{i}(x)\le0,\;\;i=1,\ldots,m\\
 &  & a_{i}^{T}x=b_{i},\;\;i=1,\ldots p
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $f_{0},\ldots,f_{m}$
\end_inset

 are convex functions.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Block
Note: Equality constraints are now linear.
 Why? [otherwise feasible set won't be convex]
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Slater's Constraint Qualifications for Strong Duality
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Sufficient conditions for strong duality in a 
\series bold
convex
\series default
 problem.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Roughly: the problem must be 
\series bold
strictly 
\series default
feasible.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The domain 
\begin_inset Formula $\cd\subset\reals^{n}$
\end_inset

 of an optimization problem is the set on which all the functions are defined.
\end_layout

\begin_deeper
\begin_layout Itemize
i.e.
 
\begin_inset Formula $f_{0},f_{1},\ldots,f_{m}$
\end_inset

 are all defined.
\end_layout

\begin_layout Itemize
the domain 
\begin_inset Formula $\cd$
\end_inset

 is NOT the feasible set.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Qualifications when problem domain 
\begin_inset Formula $\cd\subset\reals^{n}$
\end_inset

 is an open set:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\exists x$
\end_inset

 such that 
\begin_inset Formula $Ax=b$
\end_inset

 and 
\begin_inset Formula $f_{i}(x)<0$
\end_inset

 for 
\begin_inset Formula $i=1,\ldots,m$
\end_inset


\end_layout

\begin_layout Itemize
For any affine inequality constraints, 
\begin_inset Formula $f_{i}(x)\le0$
\end_inset

 is sufficient 
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Complementary Slackness
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider a general optimization problem (i.e.
 not necessarily convex).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If we have 
\series bold
strong duality
\series default
, we get an interesting relationship between
\end_layout

\begin_deeper
\begin_layout Itemize
the optimal Lagrange multiplier 
\begin_inset Formula $\lambda_{i}$
\end_inset

 and
\end_layout

\begin_layout Itemize
the 
\begin_inset Formula $i$
\end_inset

th constraint at the optimum: 
\begin_inset Formula $f_{i}(x^{*})$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Relationship is called 
\begin_inset Quotes eld
\end_inset


\series bold
complementary slackness
\series default

\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\[
\lambda_{i}^{*}f_{i}(x^{*})=0
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Lagrange multiplier is zero unless the constraint is active at the optimum.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_body
\end_document
