#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty
\usepackage{tikz}
\end_preamble
\options handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans default
\font_typewriter default
\font_math eulervm
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Kernel Methods
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 
\begin_inset Note Note
status open

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David Rosenberg 
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status collapsed

\begin_layout Plain Layout
From percy notes 
\begin_inset Quotes eld
\end_inset

Aside: sometimes, we can compute dot products eﬃciently in high (even inﬁnite)
 dimensions without using the kernel trick.
 If φ(x) is sparse (as is often the case in natural language processing),
 then hφ(x), φ(x 0 )i can be computed in O(s) time rather than O(d) time,
 where s is the number of nonzero entries in φ(x).
 
\begin_inset Quotes eld
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Extraction
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Focus on effectively representing 
\begin_inset Formula $x\in\cx$
\end_inset

 as a vector 
\begin_inset Formula $\phi(x)\in\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
e.g.
 Bag of words:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ventureBeat.png
	lyxscale 50
	width 35col%

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Methods
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Primary focus is on comparing two inputs 
\begin_inset Formula $w,x\in\cx$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
A 
\series bold
kernel 
\series default
is a function that takes a pair of inputs 
\begin_inset Formula $w,x\in\cx$
\end_inset

 and returns a real value.
 That is, 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

.
\end_layout

\begin_layout Itemize
Can interpret 
\begin_inset Formula $k(w,x)$
\end_inset

 as a 
\series bold
similarity score
\series default
, but this is not precise.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We will deal with symmetric kernels: 
\begin_inset Formula $k(w,x)=k(x,w)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Kernel Examples
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename comparingDocs1.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Bag of Words
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename comparingDocs2.png
	lyxscale 60
	width 80col%
	groupId fullPage

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Bag of Words
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename comparingDocs3.png
	lyxscale 60
	width 80col%
	groupId fullPage

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Cosine Similarity
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename cosineSimilarity1.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Enumerate
Normalize each feature vector to have 
\begin_inset Formula $\|x\|_{2}=1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename cosineSimilarity2.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Enumerate
Normalize each feature vector to have 
\begin_inset Formula $\|x\|_{2}=1$
\end_inset

.
\end_layout

\begin_layout Enumerate
Take inner product
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparing Documents: Cosine Similarity
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename cosineSimilarity3.png
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Enumerate
Normalize each feature vector to have 
\begin_inset Formula $\|x\|_{2}=1$
\end_inset

.
\end_layout

\begin_layout Enumerate
Take inner product
\end_layout

\begin_layout Enumerate
Then define
\begin_inset Formula 
\[
k(\text{VentureBeat},\text{Twitter Tweet})=0.85
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Cosine Similarity Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Why the name? Recall
\begin_inset Formula 
\[
\left\langle w,x\right\rangle =\|w\|\|x\|\cos\theta,
\]

\end_inset

where 
\begin_inset Formula $\theta$
\end_inset

 is the angle between 
\begin_inset Formula $w,x\in\reals^{d}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So
\begin_inset Formula 
\[
k(w,x)=\cos\theta=\left\langle \frac{w}{\|w\|},\frac{x}{\|x\|}\right\rangle 
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linear Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(w,x)=w^{T}x
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
When we 
\begin_inset Quotes eld
\end_inset

kernelize
\begin_inset Quotes erd
\end_inset

 an algorithm, we write it in terms of the linear kernel.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then we can swap it out a replace it with a more sophisticated kernel
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{2}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\phi:(x_{1},x_{2})\mapsto\left(x_{1},x_{2},x_{1}^{2},x_{2}^{2},\sqrt{2}x_{1}x_{2}\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Gives us ability to represent conic section boundaries.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Define kernel as inner product in feature space:
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \langle\phi(w),\phi(x)\rangle\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+w_{1}^{2}x_{1}^{2}+w_{2}^{2}x_{2}^{2}+2w_{1}w_{2}x_{1}x_{2}\\
\pause & = & w_{1}x_{1}+w_{2}x_{2}+(w_{1}x_{1})^{2}+(w_{2}x_{2})^{2}+2(w_{1}x_{1})(w_{2}x_{2})\\
\pause & = & \langle w,x\rangle+\langle w,x\rangle^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Quadratic Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Feature map:
\begin_inset Formula 
\[
\ensuremath{\phi(x)=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2},\sqrt{2}x_{1}x_{2},\ldots,\sqrt{2}x_{i}x_{j},\ldots\sqrt{2}x_{d-1}x_{d})^{T}}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Number of terms = 
\begin_inset Formula $d+d(d+1)/2\approx d^{2}/2.$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Still have
\begin_inset Formula 
\begin{eqnarray*}
k(w,x) & = & \left\langle \phi(w),\phi(x)\right\rangle \\
 & = & \left\langle x,y\right\rangle +\left\langle x,y\right\rangle ^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Computation for inner product with explicit mapping: 
\begin_inset Formula $O(d^{2})$
\end_inset


\end_layout

\begin_layout Itemize
Computation for implicit kernel calculation: 
\begin_inset Formula $O(d)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Based on Guillaume Obozinski's Statistical Machine Learning course
 at Louvain, Feb 2014.}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Polynomial Kernel in 
\begin_inset Formula $\reals^{d}$
\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\end_layout

\begin_layout Itemize
Kernel function:
\begin_inset Formula 
\[
k(w,x)=\left(1+\left\langle w,x\right\rangle \right)^{M}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponds to a feature map with all terms up to degree 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
For any 
\begin_inset Formula $M$
\end_inset

, computing the kernel has same computational cost
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Cost of explicit inner product computation grows rapidly in 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Radial Basis Function (RBF) Kernel
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset


\begin_inset Formula 
\[
k(w,x)=\exp\left(-\frac{\|w-x\|^{2}}{2\sigma^{2}}\right),
\]

\end_inset

where 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is known as the bandwidth parameter.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Does it act like a similarity score?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Why 
\begin_inset Quotes eld
\end_inset

radial
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Have we departed from our 
\begin_inset Quotes eld
\end_inset

inner product of feature vector
\begin_inset Quotes erd
\end_inset

 recipe?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Yes and no: corresponds to an infinte dimensional feature vector
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Probably the most common nonlinear kernel.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Kernel Machines
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feature Vectors from a Kernel
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So what can we do with a kernel?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can generate feature vectors:
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Idea: Characterize input 
\begin_inset Formula $x$
\end_inset

 by its similarity to 
\begin_inset Formula $r$
\end_inset

 fixed prototypes in 
\begin_inset Formula $\cx$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
A 
\series bold
kernelized feature vector 
\series default
for an input 
\begin_inset Formula $x\in\cx$
\end_inset

 with respect to a kernel 
\begin_inset Formula $k$
\end_inset

 and prototype points 
\begin_inset Formula $\mu_{1},\ldots,\mu_{r}\in\cx$
\end_inset

 is given by
\begin_inset Formula 
\[
\Phi(x)=\left[k(x,\mu_{1}),\ldots,k(x,\mu_{r})\right]\in\reals^{r}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Machines
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
A 
\series bold
kernel machine 
\series default
is a linear model with kernelized feature vectors.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
This corresponds to a prediction functions of the form
\begin_inset Formula 
\begin{eqnarray*}
f(x) & = & \alpha^{T}\Phi(x)\\
\pause & = & \sum_{i=1}^{r}\alpha_{i}k(x,\mu_{i}),
\end{eqnarray*}

\end_inset

for 
\begin_inset Formula $\alpha\in\reals^{r}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
An Interpretation
\end_layout

\end_inset


\end_layout

\begin_layout Block
For each 
\begin_inset Formula $\mu_{i}$
\end_inset

, we get a function on 
\begin_inset Formula $\cx$
\end_inset

: 
\begin_inset Formula 
\[
x\mapsto k(x,\mu_{i})
\]

\end_inset


\begin_inset Formula $f(x)$
\end_inset

 is a linear combination of these functions.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ventureBeatArticle.png
	lyxscale 40
	width 40col%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ventreBeatBagOfWords.png
	lyxscale 40
	width 40col%

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Machine Basis Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Input space 
\begin_inset Formula $\cx=\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
RBF kernel 
\begin_inset Formula $k(w,x)=\exp\left(-\left(w-x\right)^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Prototypes at 
\begin_inset Formula $\left\{ -6,-4,-3,0,2,4\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Corresponding basis functions:
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernel Machine Prediction Functions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Basis functions
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-1.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Predictions of the form
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{r}\alpha_{i}k(x,\mu_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
RBF Network
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
An 
\series bold
RBF network 
\series default
is a linear model with an RBF kernel.
\end_layout

\begin_deeper
\begin_layout Itemize
First described in 1988 by Broomhead and Lowe (neural network literature)
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename kernel-fns-2.jpg
	lyxscale 60
	width 80col%
	groupId halfSlide

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Characteristics:
\end_layout

\begin_deeper
\begin_layout Itemize
Nonlinear
\end_layout

\begin_layout Itemize
Smoothness depends on RBF kernel bandwidth
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How to Choose Prototypes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Uniform grid on space?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
only feasible in low dimensions
\end_layout

\begin_layout Itemize
where to focus the grid?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Cluster centers of training data?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Possible, but clustering is difficult in high dimensions
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize

\series bold
Use all (or a subset of) the training points
\end_layout

\begin_deeper
\begin_layout Itemize
Most common approach for kernel methods
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
All Training Points as Prototypes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider training inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}\in\cx$
\end_inset


\end_layout

\begin_layout Itemize
Then
\begin_inset Formula 
\[
f(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Requires all training examples for prediction?
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Not quite: Only need 
\begin_inset Formula $x_{i}$
\end_inset

 for 
\begin_inset Formula $\alpha_{i}\neq0$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Want 
\begin_inset Formula $\alpha_{i}$
\end_inset

's to be sparse.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Train with 
\begin_inset Formula $\ell_{1}$
\end_inset

 regularization: 
\series bold

\begin_inset Formula $\ell_{1}$
\end_inset

-regularized vector machine
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
[Will show SVM also gives sparse functions of this form.]
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $\ell_{1}$
\end_inset

-Regularized Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
RBF Kernel with bandwidth 
\begin_inset Formula $\sigma=0.3$
\end_inset

.
\end_layout

\begin_layout Itemize
Linear hypothesis space: 
\begin_inset Formula $\cf=\left\{ f(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})\mid\alpha\in\reals^{n}\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Logistic loss function: 
\begin_inset Formula $\ell(y,\hat{y})=\log\left(1+e^{-y\hat{y}}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{1}$
\end_inset

-regularization, 
\begin_inset Formula $n=200$
\end_inset

 training points
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig14.4b.pdf
	width 45col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 14.4b}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $\ell_{2}$
\end_inset

-Regularized Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
RBF Kernel with bandwidth 
\begin_inset Formula $\sigma=0.3$
\end_inset

.
\end_layout

\begin_layout Itemize
Linear hypothesis space: 
\begin_inset Formula $\cf=\left\{ f(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})\mid\alpha\in\reals^{n}\right\} $
\end_inset

.
\end_layout

\begin_layout Itemize
Logistic loss function: 
\begin_inset Formula $\ell(y,\hat{y})=\log\left(1+e^{-y\hat{y}}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{2}$
\end_inset

-regularization, 
\begin_inset Formula $n=200$
\end_inset

 training points
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig14.4a.pdf
	width 45col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{KPM Figure 14.4a}}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Example: Vector Machine for Ridge Regression
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
\begin_inset Formula $\ell_{2}$
\end_inset

-Regularized Vector Machine for Regression
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernel function 
\begin_inset Formula $k:\cx\times\cx\to\reals$
\end_inset

 is symmetric (but nothing else).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Hypothesis space (linear functions on kernelized feature vector)
\begin_inset Formula 
\[
\cf=\left\{ f_{\alpha}(x)=\sum_{i=1}^{n}\alpha_{i}k(x,x_{i})\mid\alpha\in\reals^{n}\right\} .
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective function (square loss with 
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization): 
\begin_inset Formula 
\[
J(\alpha)=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-f_{\alpha}(x_{i})\right)^{2}+\lambda\alpha^{T}\alpha,
\]

\end_inset

where 
\begin_inset Formula 
\[
f_{\alpha}(x_{i})=\sum_{j=1}^{n}\alpha_{j}k(x_{i},x_{j}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Note: All dependence on 
\begin_inset Formula $x$
\end_inset

's is via the kernel function
\series default
.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Matrix
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Note that
\begin_inset Formula 
\[
f(x_{i})=\sum_{j=1}^{n}\alpha_{j}k(x_{i},x_{j})
\]

\end_inset

only depends on the kernel function on all pairs of 
\begin_inset Formula $n$
\end_inset

 training points.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
The 
\series bold
kernel matrix
\series default
 for a kernel 
\begin_inset Formula $k$
\end_inset

 on a set 
\begin_inset Formula $\left\{ x_{1},\ldots,x_{n}\right\} $
\end_inset

 as
\begin_inset Formula 
\[
K=\begin{pmatrix}k(x_{i},x_{j})\end{pmatrix}_{i,j}=\begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\in\reals^{n\times n}.
\]

\end_inset


\end_layout

\begin_layout Definition

\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Vectorizing the Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Claim: 
\begin_inset Formula $K\alpha$
\end_inset

 gives the prediction vector 
\begin_inset Formula $\left(f_{\alpha}(x_{1}),\ldots,f_{\alpha}(x_{n})\right)^{T}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\pause K\alpha & = & \begin{pmatrix}k(x_{1},x_{1}) & \cdots & k(x_{1},x_{n})\\
\vdots & \ddots & \cdots\\
k(x_{n},x_{1}) & \cdots & k(x_{n},x_{n})
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}\\
\pause & = & \begin{pmatrix}\alpha_{1}k(x_{1},x_{1})+\cdots+\alpha_{n}k(x_{1},x_{n})\\
\vdots\\
\alpha_{1}k(x_{n},x_{1})+\cdots+\alpha_{n}k(x_{1,}x_{n})
\end{pmatrix}\\
\pause & = & \begin{pmatrix}f_{\alpha}(x_{1})\\
\vdots\\
f_{\alpha}(x_{n})
\end{pmatrix}.
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Vectorizing the Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\begin_inset Formula $i$
\end_inset

th residual is 
\begin_inset Formula $y_{i}-f_{\alpha}(x_{i})$
\end_inset

.
 We can vectorize as:
\begin_inset Formula 
\begin{eqnarray*}
y-K\alpha & = & \begin{pmatrix}y_{1}-f_{\alpha}(x_{1})\\
\vdots\\
y_{n}-f_{\alpha}(x_{n})
\end{pmatrix}
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Sum of square residuals is
\begin_inset Formula 
\[
\left(y-K\alpha\right)^{T}\left(y-K\alpha\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Objective function:
\begin_inset Formula 
\[
J(\alpha)=\frac{1}{n}\|y-K\alpha\|^{2}+\lambda\alpha^{T}\alpha
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Vectorizing the Vector Machine
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consider 
\begin_inset Formula $\cx=\reals^{d}$
\end_inset

 and 
\begin_inset Formula $k(w,x)=w^{T}x$
\end_inset

 (linear kernel)
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $X\in\reals^{n\times d}$
\end_inset

 be the 
\series bold
design matrix
\series default
, which has each input vector as a row: 
\begin_inset Formula 
\[
X=\begin{pmatrix}-x_{1}-\\
\vdots\\
-x_{n}-
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Then the kernel matrix is
\begin_inset Formula 
\[
K=XX^{T}=\begin{pmatrix}-x_{1}-\\
\vdots\\
-x_{n}-
\end{pmatrix}\begin{pmatrix}| & \cdots & |\\
x_{1} & \cdots & x_{n}\\
| & \cdots & |
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
And the objective function is
\begin_inset Formula 
\[
J(\alpha)=\frac{1}{n}\|y-XX^{T}\alpha\|^{2}+\lambda\alpha^{T}\alpha
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Section
Features vs Kernels
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Features vs Kernels
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
Suppose a kernel can be written as an inner product: 
\begin_inset Formula 
\[
k(w,x)=\left\langle \phi(w),\phi(x)\right\rangle .
\]

\end_inset

Then the kernel machine is
\series bold
 
\series default
a
\series bold
 linear classifier 
\series default
with feature map 
\begin_inset Formula $\phi(x)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Mercer's Theorem characterizes kernels with these properties.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Features vs Kernels
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Proof
For prototype points 
\begin_inset Formula $x_{1},\ldots,x_{r}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
f(x) & = & \sum_{i=1}^{r}\alpha_{i}k(x,x_{i})\\
\pause & = & \sum_{i=1}^{r}\alpha_{i}\left\langle \phi(x),\phi(x_{i})\right\rangle \\
\pause & = & \left\langle \sum_{i=1}^{r}\alpha_{i}\phi(x_{i}),\phi(x)\right\rangle \\
\pause & = & w^{T}\phi(x)
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $w=\sum_{i=1}^{r}\alpha_{i}\phi(x_{i})$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_body
\end_document
