\section{EM: Concept Check}
%TODO:
%\begin{itemize}
%\item Make sure to include $\lambda$ or $\lambda_z$ in every probability
%  expression.  Follow my notation from lecture and write $p(x\mid
%  \lambda_z)$ rather than $p(x;\lambda_z)$, for consistency.
%  
%\item In poisson mixture model setup, make the second step depend on z by
%  writing $lambda_z$.
% 
%\item For the actual EM algorithm, let's put things more in terms of terminology
%  and perspective of slide 37 on general em.  The solutions need to be more
% complete -- if it's too hard, maybe it's not a good example.  Might be easier
% to start with a training set of size 1.  
% \item If we want to have a training set of size $n$, we should explicitly
%  introduce it. 
% \end{itemize}

\subsubsection{EM/Mixture Model Objectives}
\begin{itemize}
\item Write down the probability model corresponding to the GMM problem
  (multinomial distribution on mixture component $z$, multivariate Gaussian
  conditionals $x|z$).
\item Write down the joint density $p(x,z)$ for the GMM model.
\item Give an expression for the marginal log-likelihood for the observed data
  $x$ for the GMM model, and explain why it doesn't simplify as nicely as the
  log-likelihood of a multivariate Gaussian model.
\item Give pseudocode for the EM Algorithm for GMM (as in slide 29).
%\item Be able to state the relationship of EM for GMM to k-means.
\item Give an expression for the probability model for a generic latent variable model,
  where $x$ is observed, $z$ is latent (i.e. unobserved), and the parameters are represented by $\theta$.
%\item Be able to state Jensen's inequality, and define KL divergence (prereqs for ELBO).
\item Give EM algorithm pseudocode (as in slide 27).
%\item Explain how we can compute the two arg maxes needed in this algorithm (i.e. using bound on KL divergence).
%\item Be able to show that EM gives monotonically increasing likelihood.
%\item Be able to summarize variations on EM, including generalied EM (address optimization in ``M" step), and restriction to a set Q of distributions (addressing optimization in ``E" step).
\end{itemize}

\subsubsection{EM Question}

\textbf{Poisson Mixture Model Setup}: Consider the poisson mixture model, where each data instance is generated by
\begin{enumerate}
\item Drawing an (unobserved cluster) $z$ from a multinomial distribution $\pi = (\pi_1,\cdots, \pi_k)$ on $k$ clusters.
\item Drawing a count from a Poisson distribution with PMF:
\[p(x \mid \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}\]
\end{enumerate}
In our exposition, we will let $\lambda = (\lambda_1 ,\ldots,\lambda_k)$. \\

\textbf{Problems}:
\begin{enumerate}
\item To start, let $x, z$ be the count and cluster assignment for a single instance. Give an expression for $p(x,z)$ in terms of $p(z)$ and $p(x | z)$.
\begin{solution}
\item[]\Sol 
\[p(x,z) = p(z) p(x|z)  =  p(z) p(x|\lambda_z) = \pi_z \frac{\lambda_z^x e^{-\lambda_z}}{x!}\]
\end{solution}

\item Give an expression for the marginal distribution for a single observed $x$, $p(x)$ (marginalizing out $z$), in terms of probability expressions $\pi_k$ and $p(x| \lambda_k)$.
\begin{solution}
\item[]\Sol 
\[p(x) = \sum_{z=1}^k p(x,z) = \sum_{z=1}^k \pi_z p(x| \lambda_z)\]
\end{solution}

\item For a single data instance, we observe $x$, and want to know its cluster assignment $z$. Basic probability review: give an expression for the conditional probability $p(z|x)$ for a single instance $(x,z)$ (just in terms of probability expressions $p(\cdot)$.
\begin{solution}
\item[]\Sol 
\[p(z|x) =\frac{p(x,z)}{p(x)}\]
\end{solution}

\item Now assume we have some training set of size $n$. We observe $x = (x_1, \cdots, x_n)$, but don't observe $z = (z_1, \cdots, z_n)$. We'll work through the EM algorithm for this problem. First, let's tackle the ``E step", in which we evaluate the responsibilities $\gamma_i^j = p(z_i = j | x_i)$ for each $j \in \{1,\cdots,k\}$ . Give an expression for this responsibility for cluster $j$ and instance $i$.
\begin{solution}
\item[]\Sol 
\[
\gamma_i^j = p(z_i = j | x_i) =  \frac{p(x_i | \lambda_j)}{ \sum_{z=1}^k \pi_z p(x_i | \lambda_z)}= \frac{\pi_z \frac{\lambda_z^{x_i} e^{-\lambda_z}}{x_i!}}{\sum_{z=1}^k \pi_z \frac{\lambda_z^{x_i} e^{-\lambda_z}}{x_i!}}
\]
\end{solution}

\item Before we move on to the ``M step", let's apply this ``E step" result a toy problem. Imagine $k=3$, and we have $\lambda_1 = 1$, $\lambda_2 = 2$, and $\lambda_3 = 3$. Find $p(z = 2 | x = 1)$ in terms of $\pi_i$ for $i$ in $\{1,2,3\}$. Hint: Note $p(x)$ is constant for all $k$, so its straightforward to give proportional expressions for each of $p(z = k | x = 1)$ then normalize.
\begin{solution}
\item[]\Sol 
\begin{align*}
p(z = 1 | x = 1) &\propto p(x = 1 | z = 1)p(z = 1) = \pi_1 e^{-1}\\
P(z = 2 | x = 1) &\propto p(x = 1 | z = 2)p(z = 2) = \pi_2 2 e^{-2} \\
P(z = 3 | x = 1) &\propto p(x = 1 | z = 3)p(z = 3) = \pi_3 3 e^{-3} \\
&\\
P(z = 2 | X = 1) &= \frac{\pi_2 2e^{-2}}{\pi_1 e^{-1}  + \pi_2 2 e^{-2} +  \pi_3 3 e^{-3}}
\end{align*}
\end{solution}

\item Now we will tackle the ``M step" of the EM algorithm, during which we will update $\pi_z$ and $\lambda_z$. To start, find our objective (the expectation of the complete log likelihood).
\begin{solution}
\item[]\Sol 
The complete log-likelihood is
\begin{align*}
p(x, z | \lambda) &= \sum_{i=1}^n \log\left[\pi_z \frac{\lambda_z^{x_i} e^{-\lambda_z}}{x_i!}\right] \\
	&= \sum_{i=1}^n \left[ \log\pi_z + x_i\log(\lambda_z) -\lambda_z - \log x_i! \right] \\
\end{align*}

Taking the expected complete log likelihood with respect to $q^*$ yields

\begin{align*}
J(\lambda, \pi) &= \sum_{z = 1}^k q^*(z)\log p(x, z | \lambda) \\
	&= \sum_{i=1}^n \sum_{i=1}^k \gamma_i^j \left[ \log\pi_z + x_i\log(\lambda_z) -\lambda_z - \log x_i! \right] \\
\end{align*}

\end{solution}

\item Finally give the expression for $\lambda_z^{new}$ (that maximizes the objective you found above).
\begin{solution}
\item[]\Sol 
\[\frac{\partial J(\lambda, \pi)}{\partial \lambda_z} = \sum_{i=1}^n \gamma_{i}^z \left[-1 + \frac{x_i}{\lambda_z}\right]\]
Setting this equal to 0 and solving yields


\begin{align*}
\sum_{i=1}^n \gamma_{i}^z \left[-1 + \frac{x_i}{\lambda_z}\right] &= 0\\
\sum_{i=1}^n \gamma_{i}^z \frac{x_i}{\lambda_z} &= \sum_{i=1}^n \gamma_{i}^z\\
\frac{\sum_{i=1}^n \gamma_{i}^z x_i}{\sum_{i=1}^n \gamma_{i}^z} &= \lambda_z^{new}
\end{align*}
\end{solution}

\end{enumerate}
