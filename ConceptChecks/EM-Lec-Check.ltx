\section{EM: Concept Check}
TODO:
\begin{itemize}
\item Define $\lambda = (\lambda_1 ,\ldots,\lambda_k)$ at some point
\item Make sure to include $\lambda$ or $\lambda_z$ in every probability
  expression.  Follow my notation from lecture and write $p(x\mid
  \lambda_z)$ rather than $p(x;\lambda_z)$, for consistency.
\item In poisson mixture model setup, make the second step depend on z by
  writing $lambda_z$.
\item Instead of ``factorize'' $p(x,z)$, how about ``given an expression for ...
  in terms of p(z) and p(x|z)''  
\item For the actual EM algorithm, let's put things more in terms of terminology
  and perspective of slide 37 on general em.  The solutions need to be more
  complete -- if it's too hard, maybe it's not a good example.  Might be easier
  to start with a training set of size 1.  
\item If we want to have a training set of size $n$, we should explicitly
  introduce it. 

\end{itemize}

\subsubsection{EM/Mixture Model Objectives}
\begin{itemize}
\item Write down the probability model corresponding to the GMM problem
  (multinomial distribution on mixture component $z$, multivariate Gaussian
  conditionals $x|z$).
\item Write down the joint density $p(x,z)$ for the GMM model.
\item Give an expression for the marginal log-likelihood for the observed data
  $x$ for the GMM model, and explain why it doesn't simplify as nicely as the
  log-likelihood of a multivariate Gaussian model.
\item Give pseudocode for the EM Algorithm for GMM (as in slide 29).
%\item Be able to state the relationship of EM for GMM to k-means.
\item Give an expression for the probability model for a generic latent variable model,
  where $x$ is observed, $z$ is latent (i.e. unobserved), and the parameters are represented by $\theta$.
%\item Be able to state Jensen's inequality, and define KL divergence (prereqs for ELBO).
\item Give EM algorithm pseudocode (as in slide 27).
%\item Explain how we can compute the two arg maxes needed in this algorithm (i.e. using bound on KL divergence).
%\item Be able to show that EM gives monotonically increasing likelihood.
%\item Be able to summarize variations on EM, including generalied EM (address optimization in ``M" step), and restriction to a set Q of distributions (addressing optimization in ``E" step).
\end{itemize}

\subsubsection{EM Question}

\textbf{Poisson Mixture Model Setup}: Consider the poisson mixture model, where each data instance is generated by
\begin{enumerate}
\item Drawing an (unobserved cluster) $z$ from a multinomial distribution $(\pi_1,\cdots, \pi_k)$ on $k$ clusters.
\item Drawing a count from a Poisson distribution with PMF:
\[p(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}\]
\end{enumerate}
\textbf{Problems}:
\begin{enumerate}
\item Let $x, z$ be the count and cluster assignment for a single instance. Factorize $p(x,z)$.
\begin{solution}
\item[]\Sol 
\[p(x,z) = p(z) p(x|z) = \pi_z \frac{\lambda_z^x e^{-\lambda_z}}{x!}\]
\end{solution}

\item For a single data instance, we observe $x$, and want to know its cluster assignment $z$. Basic probability review: give an expression for the conditional probability $p(z|x)$ for a single instance $(x,z)$ (just in terms of probability expressions $p(\cdot)$.
\begin{solution}
\item[]\Sol 
\[p(z|x) =\frac{p(x,z)}{p(x)}\]
\end{solution}

\item Give an expression for the marginal distribution for a single observed $x$, $p(x)$ (marginalizing out $z$), in terms of probability expressions $\pi_k$ and $p(x; \lambda_k)$.
\begin{solution}
\item[]\Sol 
\[p(x) = \sum_{z=1}^k p(x,z) = \sum_{z=1}^k \pi_z p(x; \lambda_z)\]
\end{solution}

\item Now recall the EM algorithm. In the ``E step", we evaluate the responsibilities $\gamma_i^j = p(z = j | x_i)$ for each $j \in \{1,\cdots,k\}$. Give an expression for this responsibility for cluster $j$ and instance $i$.
\begin{solution}
\item[]\Sol 
\[
\gamma_i^j = p(z = j | x_i) =  \frac{p(x_i; \lambda_j)}{ \sum_{z=1}^k \pi_z p(x_i; \lambda_z)}= \frac{\pi_z \frac{\lambda_z^{x_i} e^{-\lambda_z}}{x_i!}}{\sum_{z=1}^k \pi_z \frac{\lambda_z^{x_i} e^{-\lambda_z}}{x_i!}}
\]
\end{solution}

\item In the ``M step", we will update our MLE estimates for $\pi_z$ and $\lambda_z$. Give an expression for $\pi_z^{new}$
\begin{solution}
\item[]\Sol 
\[\pi_z^{new} = \frac{n_z}{n} = \frac{\sum_{i=1}^n \gamma_i^z}{n}
\]
where $z_i$ is the hard cluster assignment.
\end{solution}


\item Give an expression for $\lambda_z^{new}$. Recall the MLE for a Poisson $\hat{\lambda}_{MLE} = \bar{x}$.
\begin{solution}
\item[]\Sol 
\[\lambda_z^{new} = \frac{1}{n_z}\sum_{i=1}^n \gamma_i^z x_i
\]
\end{solution}

\item Let's apply the distributions we just described for the ``E step" of a toy problem. Imagine $k=3$, and we have $\lambda_1 = 1$, $\lambda_2 = 2$, and $\lambda_3 = 3$. Find $p(z = 2 | x = 1)$ in terms of $\pi_i$ for $i$ in $\{1,2,3\}$. Hint: Note $p(x)$ is constant for all $k$, so its straightforward to give proportional expressions for each of $p(z = k | x = 1)$ then normalize.
\begin{solution}
\item[]\Sol 
\begin{align*}
p(z = 1 | x = 1) &\propto p(x = 1 | z = 1)p(z = 1) = \pi_1 e^{-1}\\
P(z = 2 | x = 1) &\propto p(x = 1 | z = 2)p(z = 2) = \pi_2 2 e^{-2} \\
P(z = 3 | x = 1) &\propto p(x = 1 | z = 3)p(z = 3) = \pi_3 3 e^{-3} \\
&\\
P(z = 2 | X = 1) &= \frac{\pi_2 2e^{-2}}{\pi_1 e^{-1}  + \pi_2 2 e^{-2} +  \pi_3 3 e^{-3}}
\end{align*}
\end{solution}
\end{enumerate}
