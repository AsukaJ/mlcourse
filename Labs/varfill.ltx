\section{Recitation 2}
\subsection{Intro Question}
\begin{enumerate}
\item You have been given a data set $(x_i,y_i)$ for $i=1,\ldots,n$ where
  $x_i\in\RR^d$ and $y_i\in\{-1,1\}$.  Assume $w\in\RR^n$ and
  $a\in\RR$.
  \begin{enumerate}
  \item Suppose $y_i(w^Tx_i+a)>0$ for all $i$.  Use a picture to
    explain what this means when $d=2$.
  \item Fix $M>0$.
    Suppose $y_i(w^Tx_i+a)\geq M$ for all $i$.  Use a picture to
    explain what this means when $d=2$.
  \end{enumerate}
\begin{figure}[H]
\centering
\begin{asy}
  include "Recitation2/Data.asy";
\end{asy}
\caption{Data set with $x_i\in\RR^2$ and $y_i\in\{+1,-1\}$}
\end{figure}
\begin{solution}
\item[]\Sol 
  \begin{enumerate}
  \item The data is linearly separable.
  \item The data is separable with margin at least $M/\|w\|_2$.
  \end{enumerate}
  Both of these answers will be fleshed out in the upcoming sections.
\end{solution}
\end{enumerate}
\subsection{Support Vector Machines}
\subsubsection{Review of Geometry}
If $v,w\in\RR^d$ then the component of $v$ in the direction $w$ is
given by $\displaystyle{\frac{w^Tv}{\|w\|_2}}$.  This can also be thought of as the
signed length of $v$ when orthogonally projected onto the line through
the vector $w$.
\begin{figure}[H]
\centering
\begin{asy}
  include "Recitation2/Component.asy";
\end{asy}
\caption{Component of $v_1,v_2$ in the direction $w$.}
\end{figure}
Assuming $w\neq 0$ we can use this to interpret the set
$$S=\{x\in\RR^d\mid w^Tx=b\}.$$
Noting that $w^Tx=b\iff \frac{w^Tx}{\|w\|_2} = \frac{b}{\|w\|_2}$ we
see that $S$ contains all vectors whose component in the direction $w$
is $\frac{b}{\|w\|_2}$.  
Using linear algebra we can see this is the hyperplane orthogonal
to the vector $w$ that passes through the point
$\frac{bw}{\|w\|_2^2}$.  Note also that there are infinitely many
pairs $(w,b)$ that give the same hyperplane.  If $c\neq0$ then
$$\{x\in\RR^d\mid w^Tx=b\}\quad\text{and}\quad\{x\in\RR^d\mid
(cw)^Tx=(cb)\}$$
result in the same hyperplanes.
\begin{figure}[H]
\centering
\begin{asy}
  include "Recitation2/Level.asy";
\end{asy}
\caption{Level Surfaces of $f(v)=w^Tv$ with $\|w\|_2=1$}
\end{figure}
Given a hyperplane $\{v\mid w^Tv=b\}$, we can distinguish points
$x\in\RR^d$ depending on whether $w^Tx-b$ is zero, positive, or
negative, or in other words, whether $x$ is on the hyperplane, on the
side $w$ is pointing at, or on the side $-w$ is pointing at. 
\begin{figure}[H]
\centering
\begin{asy}
  include "Recitation2/Sides.asy";
\end{asy}
\caption{Sides of the Hyperplane $w^Tv=15$}
\end{figure}
If we have a vector $x\in\RR^d$ and a hyperplane $H=\{v\mid w^Tv=b\}$ we
can measure the distance from $x$ to $H$ by
$$d(x,H) = \left|\frac{w^Tx-b}{\|w\|_2}\right|.$$
Without the absolute values we get the \textit{signed distance}: a
positive distance if $w^Tx>b$ and a negative distance if $w^Tx<b$.
\begin{figure}[H]
\centering
\begin{asy}
  include "Recitation2/Distance.asy";
\end{asy}
\caption{Signed Distance from $x_1,x_2$ to Hyperplane $w^Tv=20$}
\end{figure}
\subsection{Hard Margin SVM}
Returning to the initial question, suppose we have the data set
$(x_i,y_i)$ for $i=1,\ldots,n$ where $x_i\in\RR^d$ and
$y_i\in\{-1,1\}$.
\begin{dfn}[Linearly Separable]
  We say $(x_i,y_i)$ for $i=1,\ldots,n$ is \textit{linearly separable} if there
  is a $w\in\RR^d$ and $a\in\RR$ such that $y_i(w^Tx_i+a)>0$ for all
  $i$. The set $\{v\in\RR^d\mid w^Tv+a=0\}$ is called a \textit{separating hyperplane}.
\end{dfn}
Let's examine what this definition says.  If $y_i=+1$ then we require
that $w^Tx_i>-a$ and if $y_i=-1$ we require that $w^Tx_i<-a$.  Thus
linearly separable means that we can separate all of the $+1$'s from
the $-1$'s using the hyperplane $\{v\mid w^Tv=-a\}$.  For the rest of this section, we
assume our data is linearly separable.
[Linearly separable data]
If we can find the $w,a$ corresponding to a hyperplane that separates
the data, we then have a decision function for classifiying elements
of $\XX$: $f(x) = \sgn(w^Tx+a)$.  Before we look for such a
hyperplane, we must address another issue.  If the data is linearly
separable, then there are infinitely many choices of separating
hyperplanes. [Show many separating planes.]
We will choose the hyperplane that maximizes a quantity called the
\textit{margin}.
\begin{dfn}[Margin]
  Let $H$ be a hyperplane that separates the data $(x_i,y_i)$ for
  $i=1,\ldots,n$.  The margin of this hyperplane is 
  $$\min_i d(x_i,H),$$
  the distance from the hyperplane to the closest data point.
\end{dfn}
Fix $w\in\RR^d$ and $a\in\RR$ with $y_i(w^Tx_i+a)>0$ for all $i$.
Then we saw earlier that
$$d(x_i,H) = \left|\frac{w^Tx_i+a}{\|w\|_2}\right| =
\frac{y_i(w^Tx_i+a)}{\|w\|_2}.$$
This gives us the following optimization problem:
$$\begin{Array}{ll}
  \text{maximize}_{w,a} & \min_i\frac{y_i(w^Tx_i+a)}{\|w\|_2}.
\end{Array}$$
We can rewrite this in a more standard form:
$$\begin{Array}{ll}
  \text{maximize}_{w,a,M} & M\\
  \text{subject to} & \frac{y_i(w^Tx_i+a)}{\|w\|_2} \geq
  M\quad\text{for all $i$.}
\end{Array}$$
[Image of maximum margin hyperplane.]
The expression $y_i(w^Tx_i+a)/\|w\|_2$ allows us to choose any
positive value for $\|w\|_2$ by changing $a$ accordingly.  Thus we can
fix $\|w\|_2=1/M$ and obtain
$$\begin{Array}{ll}
  \text{maximize}_{w,a} & 1/\|w\|_2\\
  \text{subject to} & y_i(w^Tx_i+a) \geq 1\quad\text{for all $i$.}
\end{Array}$$
To find the optimal $w,a$ we can solve the corresponding minimization
problem
$$\begin{Array}{ll}
  \text{minimize}_{w,a} & \|w\|_2^2\\
  \text{subject to} & y_i(w^Tx_i+a) \geq 1\quad\text{for all $i$.}
\end{Array}$$
This is a quadratic program that can be solved quickly on fairly large datasets.
\subsection{Soft Margin SVM}
