\section{Recitation 2: Geometric Derivation of SVMs}
\subsection{Intro Question}
\begin{enumerate}
\item You have been given a data set $(x_i,y_i)$ for $i=1,\ldots,n$ where
  $x_i\in\RR^d$ and $y_i\in\{-1,1\}$.  Assume $w\in\RR^n$ and
  $a\in\RR$.
  \begin{enumerate}
  \item Suppose $y_i(w^Tx_i+a)>0$ for all $i$.  Use a picture to
    explain what this means when $d=2$.
  \item Fix $M>0$.
    Suppose $y_i(w^Tx_i+a)\geq M$ for all $i$.  Use a picture to
    explain what this means when $d=2$.
  \end{enumerate}
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/Data.asy";
\end{asy}
\caption{Data set with $x_i\in\RR^2$ and $y_i\in\{+1,-1\}$}
\end{figure}
\begin{solution}
\item[]\Sol 
  \begin{enumerate}
  \item The data is linearly separable.
  \item The data is separable with margin at least $M/\|w\|_2$.
  \end{enumerate}
  Both of these answers will be fleshed out in the upcoming sections.
\end{solution}
\end{enumerate}
\subsection{Support Vector Machines}
\subsubsection{Review of Geometry}
If $v,w\in\RR^d$ then the component of $v$ in the direction $w$ is
given by $\displaystyle{\frac{w^Tv}{\|w\|_2}}$.  This can also be thought of as the
signed length of $v$ when orthogonally projected onto the line through
the vector $w$.
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/Component.asy";
\end{asy}
\caption{Component of $v_1,v_2$ in the direction $w$.}
\end{figure}
Assuming $w\neq 0$ we can use this to interpret the set
$$S=\{x\in\RR^d\mid w^Tx=b\}.$$
Noting that $w^Tx=b\iff \frac{w^Tx}{\|w\|_2} = \frac{b}{\|w\|_2}$ we
see that $S$ contains all vectors whose component in the direction $w$
is $\frac{b}{\|w\|_2}$.  
Using linear algebra we can see this is the hyperplane orthogonal
to the vector $w$ that passes through the point
$\frac{bw}{\|w\|_2^2}$.  Note also that there are infinitely many
pairs $(w,b)$ that give the same hyperplane.  If $c\neq0$ then
$$\{x\in\RR^d\mid w^Tx=b\}\quad\text{and}\quad\{x\in\RR^d\mid
(cw)^Tx=(cb)\}$$
result in the same hyperplanes.
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/Level.asy";
\end{asy}
\caption{Level Surfaces of $f(v)=w^Tv$ with $\|w\|_2=1$}
\end{figure}
Given a hyperplane $\{v\mid w^Tv=b\}$, we can distinguish points
$x\in\RR^d$ depending on whether $w^Tx-b$ is zero, positive, or
negative, or in other words, whether $x$ is on the hyperplane, on the
side $w$ is pointing at, or on the side $-w$ is pointing at. 
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/Sides.asy";
\end{asy}
\caption{Sides of the Hyperplane $w^Tv=15$}
\end{figure}
If we have a vector $x\in\RR^d$ and a hyperplane $H=\{v\mid w^Tv=b\}$ we
can measure the distance from $x$ to $H$ by
$$d(x,H) = \left|\frac{w^Tx-b}{\|w\|_2}\right|.$$
Without the absolute values we get the \textit{signed distance}: a
positive distance if $w^Tx>b$ and a negative distance if $w^Tx<b$.  To
see why this formula is correct, note that we are computing
$$\frac{w^Tx}{\|w\|_2} - \frac{w^Tv}{\|w\|_2},$$
where $v$ is any vector in the hyperplane $\{v\mid w^Tv=b\}$.
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/Distance.asy";
\end{asy}
\caption{Signed Distance from $x_1,x_2$ to Hyperplane $w^Tv=20$}
\end{figure}
\subsection{Hard Margin SVM}
Returning to the initial question, suppose we have the data set
$(x_i,y_i)$ for $i=1,\ldots,n$ where $x_i\in\RR^d$ and
$y_i\in\{-1,1\}$.
\begin{dfn}[Linearly Separable]
  We say $(x_i,y_i)$ for $i=1,\ldots,n$ is \textit{linearly separable} if there
  is a $w\in\RR^d$ and $a\in\RR$ such that $y_i(w^Tx_i+a)>0$ for all
  $i$. The set $\{v\in\RR^d\mid w^Tv+a=0\}$ is called a \textit{separating hyperplane}.
\end{dfn}
Let's examine what this definition says.  If $y_i=+1$ then we require
that $w^Tx_i>-a$ and if $y_i=-1$ we require that $w^Tx_i<-a$.  Thus
linearly separable means that we can separate all of the $+1$'s from
the $-1$'s using the hyperplane $\{v\mid w^Tv=-a\}$.  For the rest of this section, we
assume our data is linearly separable.
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/LinSep.asy";
\end{asy}
\caption{Linearly Separable Data}
\end{figure}
If we can find the $w,a$ corresponding to a hyperplane that separates
the data, we then have a decision function for classifiying elements
of $\XX$: $f(x) = \sgn(w^Tx+a)$.  Before we look for such a
hyperplane, we must address another issue.  If the data is linearly
separable, then there are infinitely many choices of separating
hyperplanes. 
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/ManyLinSep.asy";
\end{asy}
\caption{Many Separating Hyperplanes Exist}
\end{figure}
We will choose the hyperplane that maximizes a quantity called the
\textit{margin}.
\begin{dfn}[Margin]
  Let $H$ be a hyperplane that separates the data $(x_i,y_i)$ for
  $i=1,\ldots,n$.  The margin of this hyperplane is 
  $$\min_i d(x_i,H),$$
  the distance from the hyperplane to the closest data point.
\end{dfn}
Fix $w\in\RR^d$ and $a\in\RR$ with $y_i(w^Tx_i+a)>0$ for all $i$.
Then we saw earlier that
$$d(x_i,H) = \left|\frac{w^Tx_i+a}{\|w\|_2}\right| =
\frac{y_i(w^Tx_i+a)}{\|w\|_2}.$$
This gives us the following optimization problem:
$$\begin{Array}{ll}
  \text{maximize}_{w,a} & \min_i\frac{y_i(w^Tx_i+a)}{\|w\|_2}.
\end{Array}$$
We can rewrite this in a more standard form:
$$\begin{Array}{ll}
  \text{maximize}_{w,a,M} & M\\
  \text{subject to} & \frac{y_i(w^Tx_i+a)}{\|w\|_2} \geq
  M\quad\text{for all $i$.}
\end{Array}$$
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/MaxMargin.asy";
\end{asy}
\caption{Maximum Margin Separating Hyperplane}
\end{figure}
Note above how the margin is achieved on both sides of the optimal
hyperplane.  This must be the case, as otherwise we could slightly
move the hyperplane and obtain a better solution.
The expression $y_i(w^Tx_i+a)/\|w\|_2$ allows us to choose any
positive value for $\|w\|_2$ by changing $a$ accordingly (e.g., we can
replace $w\to 2w$ and $a\to 2a$ and get the same value for all $(x_i,y_i)$).  Thus we can
fix $\|w\|_2=1/M$ and obtain
$$\begin{array}{ll}
  \text{maximize}_{w,a} & 1/\|w\|_2\\
  \text{subject to} & y_i(w^Tx_i+a) \geq 1\quad\text{for all $i$.}
\end{array}$$
To find the optimal $w,a$ we can instead solve the minimization
problem
$$\begin{array}{ll}
  \text{minimize}_{w,a} & \|w\|_2^2\\
  \text{subject to} & y_i(w^Tx_i+a) \geq 1\quad\text{for all $i$.}
\end{array}$$
This is a quadratic program that can be solved quickly on fairly large datasets.
\subsection{Soft Margin SVM}
The methods developed thus far require linearly separable data.  To
remove this restriction, we will allow vectors to violate the margin
requirements, but at a penalty.  More precisely, we replace our
previous SVM formulation
$$\begin{array}{ll}
  \text{minimize}_{w,a} & \|w\|_2^2\\
  \text{subject to} & y_i(w^Tx_i+a) \geq 1\quad\text{for all $i$}
\end{array}$$
with
$$\begin{array}{ll}
  \text{minimize}_{w,a} & \|w\|_2^2 + \frac{C}{n}\sum_{i=1}^n\xi_i\\
  \text{subject to} & y_i(w^Tx_i+a) \geq 1-\xi_i\quad\text{for all
    $i$}\\
  & \xi_i \geq 0 \quad\text{for all $i$.}
\end{array}$$
This is the standard formulation of a support vector machine, and is
equivalent to the statement from the lecture.
When $\xi_i>0$ the corresponding $x_i$ violates the margin condition.
The constant $C$ controls how much we penalize violations.
Rewriting the condition as
$$\frac{y_i(w^Tx_i+a)}{\|w\|_2} \geq \frac{1-\xi_i}{\|w\|_2}$$
shows that $\xi_i$ measures the size of the violation in multiples of
the margin.  For example, $\xi_i=1$ means $x_i$ lies on the decision hyperplane
$w^Tv+a=0$, and $\xi_i=3$ means $x_i$ lies 2 margin widths past the
decision hyperplane. 
\begin{figure}[H]
\centering
\begin{asy}
  include "3-SVM/SoftMargin.asy";
\end{asy}
\caption{Soft Margin SVM (unlabeled points have $\xi_i=0$)}
\end{figure}
Recall from the treatment in class, that the minimizer $w$ will be a
linear combination of some of the $x_i$, called support vectors.  More
precisely, the support vectors will be some subset of the $x_i$ that
either lie on the margin boundary ($y_i(w^Tx_i+a)=1$) or violate the
margin boundary ($y_i(w^Tx_i+a)<1$, $\xi_i>0$).
