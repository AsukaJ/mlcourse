#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass paper
\use_default_options false
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding iso8859-1
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 5
\tocdepth 5
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset


\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset


\begin_inset FormulaMacro
\newcommand{\loss}{V}
\end_inset


\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset


\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}_{\ell}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset


\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}_{\ell}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1_{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\mbox{argmax}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\mbox{argmin}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vmu}{\boldsymbol{\mu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset


\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\E}{\ex}
\end_inset


\begin_inset FormulaMacro
\newcommand{\err}{\text{err}}
\end_inset


\end_layout

\begin_layout Title
Boosting
\end_layout

\begin_layout Author
David S.
 Rosenberg
\end_layout

\begin_layout Section
AdaBoost (Freund-Schapire '95)
\end_layout

\begin_layout Standard
[slight notational(?) differences from lecture notes]
\end_layout

\begin_layout Enumerate
\begin_inset Formula $D_{1}(1)=\cdots=D_{1}(n)=\frac{1}{n}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $F_{0}(x)\equiv0$
\end_inset


\end_layout

\begin_layout Enumerate
for 
\begin_inset Formula $t=1,\ldots,T$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Enumerate
Choose 
\begin_inset Formula $f_{t}\in G$
\end_inset

 [base classifier choice, approximately minimizes 
\begin_inset Formula $\eps_{t}$
\end_inset

 below]
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\eps_{t}:=\sum_{i=1}^{n}D_{t}(i)\ind{f_{t}(x_{i})\neq y_{i}}$
\end_inset

 [weighted error of 
\begin_inset Formula $f_{t}$
\end_inset

]
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\alpha_{t}=\frac{1}{2}\ln\left(\frac{1-\eps_{t}}{\eps_{t}}\right)$
\end_inset

 [update weight]
\end_layout

\begin_layout Enumerate
\begin_inset Formula $F_{t}=F_{t-1}+\alpha_{t}f_{t}$
\end_inset

 [classifier at 
\begin_inset Formula $t$
\end_inset

th round]
\end_layout

\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula $Z_{t}=2\sqrt{\eps_{t}(1-\eps_{t})}$
\end_inset

 [chosen s.t.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit

\begin_inset Formula $Z_{t}$
\end_inset

 is such that 
\begin_inset Formula $\sum_{i=1}^{n}D_{t+1}(i)=1$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
]
\end_layout

\begin_layout Enumerate
\begin_inset Formula $D_{t+1}(i)=\frac{1}{Z_{t}}D_{t}(i)\times\begin{cases}
e^{\alpha_{t}} & \mbox{if }f_{t}(x_{i})\neq y_{i}\\
e^{-\alpha_{t}} & \mbox{otherwise}
\end{cases}$
\end_inset

, where 
\end_layout

\end_deeper
\begin_layout Standard
Final output: 
\begin_inset Formula $H(x)=\sign\left(\sum_{t=1}^{T}\alpha_{t}f_{t}(s)\right)$
\end_inset


\end_layout

\begin_layout Section
AdaBoost.M1 [Equivalenet version given in Hastie book and lecture notes]
\end_layout

\begin_layout Standard
As given in HTF Algorithm 10.1, AdaBoost is:
\end_layout

\begin_layout Standard
Given training set 
\begin_inset Formula $\cd=\left\{ \left(x_{1},y_{1}\right),\ldots,\left(x_{n},y_{n}\right)\right\} $
\end_inset

.
\end_layout

\begin_layout Enumerate
Initialize observation weights 
\begin_inset Formula $w_{i}=1/n$
\end_inset

, 
\begin_inset Formula $i=1,2,\ldots,n$
\end_inset

.
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $m=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Fit classifier 
\begin_inset Formula $G_{m}(x)$
\end_inset

 to 
\begin_inset Formula $\cd$
\end_inset

 using weights 
\begin_inset Formula $w_{i}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Compute weighted 0-1 empirical risk:
\begin_inset Formula 
\[
\mbox{err}_{m}=\frac{1}{W}\sum_{i=1}^{n}w_{i}\ind{y_{i}\neq G_{m}(x_{i})}\quad\text{where }W=\sum_{i=1}^{n}w_{i}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
Compute 
\begin_inset Formula $\alpha_{m}=\ln\left(\frac{1-\text{err}_{m}}{\text{err}_{m}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Set 
\begin_inset Formula $w_{i}\gets w_{i}\cdot\exp\left[\alpha_{m}\ind{y_{i}\neq G_{m}(x_{i})}\right],\quad i=1,2,\ldots,N$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Ouptut 
\begin_inset Formula $G(x)=\sign\left[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)\right]$
\end_inset

.
\end_layout

\begin_layout Standard
We now show this is equivalent to the traditional formulation we gave above:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\alpha_{t}=\frac{1}{2}\ln\left(\frac{1-\eps_{t}}{\eps_{t}}\right)$
\end_inset

 [update weight]
\end_layout

\begin_layout Enumerate
\begin_inset Formula $F_{t}=F_{t-1}+\alpha_{t}f_{t}$
\end_inset

 [classifier at 
\begin_inset Formula $t$
\end_inset

th round]
\end_layout

\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula $Z_{t}=2\sqrt{\eps_{t}(1-\eps_{t})}$
\end_inset

 [chosen s.t.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit

\begin_inset Formula $Z_{t}$
\end_inset

 is such that 
\begin_inset Formula $\sum_{i=1}^{n}D_{t+1}(i)=1$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
]
\end_layout

\begin_layout Enumerate
\begin_inset Formula $D_{t+1}(i)=\frac{1}{Z_{t}}D_{t}(i)\times\begin{cases}
e^{\alpha_{t}} & \mbox{if }f_{t}(x_{i})\neq y_{i}\\
e^{-\alpha_{t}} & \mbox{otherwise}
\end{cases}$
\end_inset

, where 
\end_layout

\begin_layout Standard
Final output: 
\begin_inset Formula $H(x)=\sign\left(\sum_{t=1}^{T}\alpha_{t}f_{t}(s)\right)$
\end_inset


\end_layout

\begin_layout Standard
So, 
\begin_inset Formula $\alpha$
\end_inset

's are the same, up to a constant factor, which doesn't change the final
 prediction at all.
\end_layout

\begin_layout Standard
HTF updates are:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w_{i}\gets w_{i}\exp\left[\ln\left(\frac{1-\eps_{t}}{\eps_{t}}\right)\ind{y_{i}\neq G_{m}(x_{i})}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
while traditional is (ignoring normalization)
\begin_inset Formula 
\begin{eqnarray*}
D_{t+1}(i) & \gets & D_{t}(i)\times\begin{cases}
e^{\alpha_{t}} & \mbox{if }f_{t}(x_{i})\neq y_{i}\\
e^{-\alpha_{t}} & \mbox{otherwise}
\end{cases}\\
 & = & D_{i}\exp\left[\alpha_{t}\left[2\times\ind{y_{i}\neq G_{m}(x_{i})}-1\right]\right]\\
 & = & D_{i}\exp\left[\ln\left(\frac{1-\eps_{t}}{\eps_{t}}\right)\left[\ind{y_{i}\neq G_{m}(x_{i})}-\frac{1}{2}\right]\right]\\
 & = & D_{i}\exp\left[\ln\left(\frac{1-\eps_{t}}{\eps_{t}}\right)\ind{y_{i}\neq G_{m}(x_{i})}\right]\exp\left[-\frac{1}{2}\ln\left(\frac{1-\eps_{t}}{\eps_{t}}\right)\right]
\end{eqnarray*}

\end_inset

and notice that the last term is independent of 
\begin_inset Formula $i$
\end_inset

, and thus is lost in the normalization.
 So they're the same.
\end_layout

\begin_layout Section
AdaBoost Minimizes Empirical Risk
\end_layout

\begin_layout Standard
[ we should change notation to look more like lecture notes...
 ]
\end_layout

\begin_layout Standard
[From Peter Bartlett's lecture -- I think they're using the original form
 of AdaBoost here]
\end_layout

\begin_layout Theorem
We have 
\begin_inset Formula 
\begin{align}
\hat{P}\left(YF_{T}(x)\leq0\right) & =\frac{1}{n}\left|\left\{ i:y_{i}F_{T}(x_{i})\leq0\right\} \right|\\
 & \leq\prod_{t=1}^{T}2\sqrt{\eps_{t}(1-\eps_{t})}
\end{align}

\end_inset

Furthermore, if we know that 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is slightly less than 
\begin_inset Formula $\frac{1}{2}$
\end_inset

, say 
\begin_inset Formula $\epsilon_{t}\leq\frac{1}{2}-\gamma\,\forall t$
\end_inset

, the product above is no more than 
\begin_inset Formula $(1-4\gamma^{2})^{\frac{T}{2}}$
\end_inset

.
\end_layout

\begin_layout Proof
Instead of the event 
\begin_inset Formula $YF_{T}(X)\leq0$
\end_inset

, look at the equivalent event 
\begin_inset Formula $\exp(-YF_{T}(X))\geq1$
\end_inset

.
 Also, note that 
\begin_inset Formula 
\[
\ind{\exp(-YF_{T}(X))\geq1}\le\exp(-YF_{T}(X))
\]

\end_inset

So
\begin_inset Formula 
\begin{align}
\hat{P}\left(YF_{T}(X)\leq0\right) & =\hat{\ex}\ind{\exp(-YF_{T}(X))\geq1}\\
 & \leq\hat{\ex}\left[\exp(-YF_{T}(X))\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\exp\left(-y_{i}\sum_{t=1}^{T}\alpha_{t}f_{t}(x_{i})\right)\\
 & =\frac{1}{n}\sum_{i}\prod_{t}\exp\left(-y_{i}\alpha_{t}f_{t}(x_{i})\right)
\end{align}

\end_inset

We also know that, since 
\begin_inset Formula $y_{i},f(x_{i})\in\{\pm1\}$
\end_inset

, their product is also in 
\begin_inset Formula $\{\pm1\}$
\end_inset

.
 Applying this to the expression for 
\begin_inset Formula $D_{t+1}$
\end_inset

 in the algorithm, we have
\begin_inset Formula 
\begin{align*}
D_{t+1}(i) & =\frac{1}{Z_{t}}D_{t}(i)\times\begin{cases}
e^{\alpha_{t}} & \mbox{if }f_{t}(x_{i})\neq y_{i}\\
e^{-\alpha_{t}} & \mbox{otherwise}
\end{cases}\\
 & =\frac{1}{Z_{t}}D_{t}(i)\exp\left(-y_{i}\alpha_{t}f_{t}(x_{i})\right)
\end{align*}

\end_inset

Plugging in, we get
\begin_inset Formula 
\begin{align}
\hat{\ex}\left[\exp(-YF_{T}(X))\right] & =\frac{1}{n}\sum_{i}\prod_{t}\left(\frac{D_{t+1}(i)}{D_{t}(i)}Z_{t}\right)\\
 & =\frac{1}{n}\sum_{i}\left(\prod_{t}Z_{t}\right)\frac{D_{T+1}(i)}{D_{1}(i)}\\
 & =\prod_{t}Z_{t}\label{prod}
\end{align}

\end_inset

 where in the final equality we use the fact that 
\begin_inset Formula $D_{T+1}$
\end_inset

 is a distribution and sums over 
\begin_inset Formula $i$
\end_inset

 to one.
 
\end_layout

\begin_layout Proof
Meanwhile, recalling that 
\begin_inset Formula $\eps_{t}=\sum_{i=1}^{n}D_{t}(i)\ind{f_{t}(x_{i})\neq y_{i}}$
\end_inset

, we have
\begin_inset Formula 
\begin{align*}
Z_{t}= & \sum_{i=1}^{n}D_{t}(i)e^{\alpha_{t}}\ind{f_{t}(x_{i})\neq y_{i}}+\sum_{i=1}^{n}D_{t}(i)e^{-\alpha_{t}}\ind{f_{t}(x_{i})=y_{i}}\\
= & e^{\alpha_{t}}\eps_{t}+e^{-\alpha_{t}}(1-\eps_{t})
\end{align*}

\end_inset

 If we choose 
\begin_inset Formula $\alpha_{t}$
\end_inset

 to minimize 
\begin_inset Formula $Z_{t}$
\end_inset

 (by differentiating w.r.t.
 
\begin_inset Formula $\alpha_{t}$
\end_inset

...) we get
\begin_inset Formula 
\[
\alpha_{t}=\frac{1}{2}\ln\left(\frac{1-\eps_{t}}{\eps_{t}}\right)
\]

\end_inset

Which gives
\begin_inset Formula 
\begin{align}
Z_{t} & =(1-\epsilon_{t})\sqrt{\frac{\epsilon_{t}}{1-\epsilon_{t}}}+\epsilon_{t}\sqrt{\frac{1-\epsilon_{t}}{\epsilon_{t}}}\\
 & =2\sqrt{\epsilon_{t}(1-\epsilon_{t})}
\end{align}

\end_inset

We can plug this into 
\begin_inset CommandInset ref
LatexCommand ref
reference "prod"

\end_inset

 to get the desired result.
\end_layout

\begin_layout Standard
We can extend the above theorem to include a margin as well.
\end_layout

\begin_layout Section
Exponential Loss with FSAM is AdaBoost
\end_layout

\begin_layout Subsection
Exponential loss and adaboost (Section 10.4)
\end_layout

\begin_layout Standard
Turns out that the traditional AdaBoost.M1 algorithm (Alg.
 10.1) is equivalent to forward stagewise additive modeling (Alg 10.2) using
 the loss function
\begin_inset Formula 
\[
L(y,f(x))=\exp(-yf(x))
\]

\end_inset

 Then taking 
\begin_inset Formula $G(x)=b(x;\gamma)$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\left(\beta_{m},G_{m}\right) & = & \argmin_{\beta,G}\sum_{i=1}^{N}\exp\left[-y_{i}\left(f_{m-1}(x_{i})+\beta G(x_{i})\right)\right]\\
 & = & \argmin_{\beta,G}\sum_{i=1}^{N}w_{i}^{(m)}\exp\left[-\beta y_{i}G(x_{i})\right]
\end{eqnarray*}

\end_inset

 where 
\begin_inset Formula $w_{i}^{(m)}:=\exp(-y_{i}f_{m-1}(x_{i}))$
\end_inset

.
 Note that 
\begin_inset Formula $w_{i}^{(m)}$
\end_inset

 only depends on the 
\begin_inset Formula $m-1$
\end_inset

'st classifier, thus we can consider 
\begin_inset Formula $w^{(m)}$
\end_inset

 as the weighting of the data for the 
\begin_inset Formula $m$
\end_inset

'th classifier.
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $y_{i},G(x_{i})\in\{-1,1\}$
\end_inset

.
 Thus for any fixed 
\begin_inset Formula $\beta>0$
\end_inset

, we can break up the sum by possible values inside the exponential.
\begin_inset Formula 
\begin{eqnarray}
\sum_{i=1}^{N}w_{i}^{(m)}\exp\left[-\beta y_{i}G(x_{i})\right] & = & e^{-\beta}\sum_{i:y_{i}=G(x_{i})}w_{i}^{(m)}+e^{\beta}\sum_{i:y_{i}\neq G(x_{i})}w_{i}^{(m)}\nonumber \\
 & = & e^{-\beta}\left[\sum_{i=1}^{N}w_{i}^{(m)}-\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}\right]\\
 &  & +e^{\beta}\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}\\
 & = & (e^{\beta}-e^{-\beta})\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}+e^{-\beta}\sum_{i=1}^{N}w_{i}^{(m)}\label{eq:weightedLossNewExpr}
\end{eqnarray}

\end_inset

 Plugging this in, and keeping 
\begin_inset Formula $\beta$
\end_inset

 fixed, we get
\begin_inset Formula 
\begin{eqnarray*}
G_{m} & = & \arg\min_{G}(e^{\beta}-e^{-\beta})\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}+e^{-\beta}\sum_{i=1}^{N}w_{i}^{(m)}\\
 & = & \arg\min_{G}\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G(x_{i})}
\end{eqnarray*}

\end_inset

 Notice that this last expression is independent of 
\begin_inset Formula $\beta$
\end_inset

\SpecialChar \@.
 
\end_layout

\begin_layout Standard
Since the minimizing 
\begin_inset Formula $G_{m}$
\end_inset

 is independent of 
\begin_inset Formula $\beta$
\end_inset

, we can plug it into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weightedLossNewExpr"

\end_inset

 and minimize with respect to 
\begin_inset Formula $\beta$
\end_inset

.
 First, let's introduce 
\begin_inset Formula 
\begin{eqnarray*}
\err_{m} & := & \frac{\sum_{i=1}^{N}w_{i}^{(m)}\ind{y_{i}\neq G_{m}(x_{i})}}{W^{(m)}}\\
W^{(m)} & := & \sum_{i=1}^{N}w_{i}^{(m)}
\end{eqnarray*}

\end_inset

the weighted error rate for 
\begin_inset Formula $G_{m}$
\end_inset

 on the training data.
 Then minimizing 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weightedLossNewExpr"

\end_inset

 with respect to 
\begin_inset Formula $\beta$
\end_inset

 is equivalent to minimizing
\begin_inset Formula 
\[
(e^{\beta}-e^{-\beta})\err_{m}+e^{-\beta}.
\]

\end_inset

Differentiating w.r.t.
 
\begin_inset Formula $\beta$
\end_inset

 and equating to zero, we get
\begin_inset Formula 
\begin{eqnarray*}
\partial_{\beta}\left[(e^{\beta}-e^{-\beta})\err_{m}+e^{-\beta}\right] & = & 0\\
(e^{\beta}+e^{-\beta})\err_{m}-e^{-\beta} & = & 0\\
e^{\beta}\err_{m}+e^{-\beta}\err_{m}-e^{-\beta} & = & 0\\
e^{2\beta}\err_{m}+\err_{m}-1 & = & 0\\
e^{2\beta} & = & \frac{1-\err_{m}}{\err_{m}}\\
\implies\beta_{m} & = & \frac{1}{2}\ln\left(\frac{1-\err_{m}}{\err_{m}}\right)
\end{eqnarray*}

\end_inset

(How do we know this is a local min and not a local max? What if we allow
 negative weight values?)
\end_layout

\begin_layout Standard
Now we've found the next basis function 
\begin_inset Formula $G_{m}(x)$
\end_inset

 and the next coefficient 
\begin_inset Formula $\beta_{m}$
\end_inset

.
 Interesting that 
\begin_inset Formula $\beta_{m}$
\end_inset

 is the best coefficient, no matter what the value of 
\begin_inset Formula $G_{m}(x)$
\end_inset

? Anyway, the next approximation in our forward stagewise additive model
 is
\begin_inset Formula 
\begin{eqnarray*}
f_{m}(x) & = & f_{m-1}(x)+\beta_{m}G_{m}(x).
\end{eqnarray*}

\end_inset

[NoteRecalling that 
\begin_inset Formula $w_{i}^{(m)}:=\exp(-y_{i}f_{m-1}(x_{i}))$
\end_inset

, we find that the weights at the next iteration are
\begin_inset Formula 
\begin{eqnarray*}
w_{i}^{(m+1)} & = & \exp(-y_{i}f_{m}(x_{i}))\\
 & = & \exp(-y_{i}(f_{m-1}(x_{i})+\beta_{m}G_{m}(x_{i})))\\
 & = & w_{i}^{(m)}\exp(-y_{i}\beta_{m}G_{m}(x_{i}))
\end{eqnarray*}

\end_inset

 Noting that 
\begin_inset Formula $y_{i}G_{m}(x_{i})=2\cdot\ind{y_{i}\neq G_{m}(x_{i})}-1$
\end_inset

, we find
\begin_inset Formula 
\begin{eqnarray*}
w_{i}^{(m+1)} & = & w_{i}^{(m)}\exp\left[-\beta_{m}(2\cdot\ind{y_{i}\neq G_{m}(x_{i})}-1)\right]\\
 & = & w_{i}^{(m)}e^{-2\beta_{m}\ind{y_{i}\neq G_{m}(x_{i})}}e^{-\beta_{m}}
\end{eqnarray*}

\end_inset

 Taking 
\begin_inset Formula $\alpha_{m}:=2\beta_{m}=\log\frac{1-\err(m)}{\err(m)}$
\end_inset

, and noting that 
\begin_inset Formula $e^{-\beta_{m}}$
\end_inset

 multiplies all the weights by the same amount, we can equivalently take
 the weight updates to be
\begin_inset Formula 
\begin{eqnarray*}
w_{i}^{(m+1)} & = & w_{i}^{(m)}e^{-\alpha_{m}\ind{y_{i}\neq G_{m}(x_{i})}}
\end{eqnarray*}

\end_inset

 We see that this algorithm is the same as the traditional AdaBoost.M1 on
 p.
 301.
 The ony difference is that, while in AdaBoost, we were loose about the
 requirements for 
\begin_inset Quotes eld
\end_inset

fitting the weighted training data
\begin_inset Quotes erd
\end_inset

, in the forwards stagewise approach, we are explicity looking for 
\begin_inset Formula 
\[
\arg\min_{G}\sum_{i=1}^{N}w_{i}^{(m)}\ind{(y_{i}\neq G(x_{i}))}.
\]

\end_inset


\end_layout

\begin_layout Section
Population Minimizer of Exponential Loss 
\end_layout

\begin_layout Standard
In the previous section we showed that AdaBoost.M1 is equivalent to forward
 stagewise additive modeling with an exponential loss.
 The exponential loss gave of certain computational benefits, but what are
 its statistical properties? 
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula 
\begin{eqnarray*}
f^{*}(x) & = & \arg\min_{f(x)}\ex_{Y|x}e^{-Yf(x)}
\end{eqnarray*}

\end_inset

 Note that we can solve this for each fixed 
\begin_inset Formula $x$
\end_inset

.
 Also note that 
\begin_inset Formula $Y\in\{-1,1\}$
\end_inset

, so we can write
\begin_inset Formula 
\begin{eqnarray*}
\ex_{Y|x}e^{-Yf(x)} & = & e^{-f(x)}\pr(Y=1|x)+e^{f(x)}P(Y=-1|x)
\end{eqnarray*}

\end_inset

Considering 
\begin_inset Formula $x$
\end_inset

 to be fixed, and 
\begin_inset Formula $f(x)\in\reals$
\end_inset

, we can find the population minimum by differentiating with respect to
 
\begin_inset Formula $f(x)$
\end_inset

 (a scalar) and equating to zero.
 Doing this gives us 
\begin_inset Formula 
\begin{eqnarray*}
0=\partial_{f(x)}\ex_{Y|x}e^{-Yf(x)} & = & -f(x)e^{-f(x)}\pr(Y=1|x)+f(x)e^{f(x)}P(Y=-1|x)\\
\implies f(x)e^{f(x)}P(Y=-1|x) & = & -f(x)e^{-f(x)}\pr(Y=1|x)\\
\implies e^{2f(x)} & = & \frac{\pr(Y=1|x)}{\pr(Y=-1|x)}
\end{eqnarray*}

\end_inset

so we get
\begin_inset Formula 
\[
f^{*}(x)=\frac{1}{2}\ln\frac{\pr(Y=1|x)}{\pr(Y=-1|x)}.
\]

\end_inset

Let 
\begin_inset Formula $p=\pr(Y=1|x)$
\end_inset

 and 
\begin_inset Formula $f=f^{*}(x)$
\end_inset

, and let's solve for 
\begin_inset Formula $p$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
f & = & \frac{1}{2}\ln\frac{p}{1-p}\\
e^{2f} & = & \frac{p}{1-p}\\
p & = & \frac{e^{2f}}{1+e^{2f}}
\end{eqnarray*}

\end_inset

Equivalently, 
\begin_inset Formula 
\[
\pr(Y=1|x)=\frac{1}{1+e^{-2f^{*}(x)}}
\]

\end_inset

 Thus the additive expansion produced by AdaBoost is estimating one-half
 the log-odds of 
\begin_inset Formula $P(Y=1|x)$
\end_inset

! This justifies using its sign as the classification rule.
 (So we're estimating Bayes rule?)
\end_layout

\begin_layout Standard
Another loss criterion with the same populating minimizer is the binomial
 negative log-likelihood (or 
\emph on
deviance
\emph default
 or 
\emph on
cross-entropy
\emph default
), where we interpret 
\begin_inset Formula $f$
\end_inset

 as the logit transform.
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\end_body
\end_document
